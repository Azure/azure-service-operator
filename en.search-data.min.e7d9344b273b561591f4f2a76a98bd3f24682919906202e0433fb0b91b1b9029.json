[{"id":0,"href":"/azure-service-operator/contributing/contributing/","title":"Contributing","section":"For Contributors","content":"Contributing to Azure Service Operator v2 #  Code Structure #  Azure Service Operator v2 #  Code Generator #  Adding a new code-generated resource #  See adding a new code-generator resource.\nDeveloper setup (with VS Code) #  This is the recommended setup, especially if you are using Windows as your development platform.\nThis repository contains a devcontainer configuration that can be used in conjunction with VS Code to set up an environment with all the required tools preinstalled.\nIf you want to use this:\n Make sure you have installed the prerequisites to use Docker, including WSL if on Windows.\n  Install VS Code and the Remote Development extension (check installation instructions there).\n  Run the VS Code command (with Ctrl-Shift-P): Remote Containers: Clone Repository in Container Volume...\nNote: in Windows, it is important to clone directly into a container instead of cloning first and then loading that with the Remote Containers extension, as the tooling performs a lot of file I/O, and if this is performed against a volume mounted in WSL then it is unusably slow.\nTo complete the clone:\n Select \u0026ldquo;GitHub\u0026rdquo;. Search for \u0026ldquo;Azure/azure-service-operator\u0026rdquo;. Choose either of the following options about where to create the volume. The window will reload and run the Dockerfile setup. The first time, this will take some minutes to complete as it installs all dependencies. Run git submodule init and git submodule update    To validate everything is working correctly, you can open a terminal in VS Code and run task -l. This will show a list of all task commands. Running task by itself (or task default) will run quick local pre-checkin tests and validation.\n  Without VS Code #  Option 1: Dockerfile #  The same Dockerfile that the VS Code devcontainer extension uses can also be used outside of VS Code; it is stored in the root .devcontainer directory and can be used to create a development container with all the tooling preinstalled:\n$ docker build $(git rev-parse --show-toplevel)/.devcontainer -t asodev:latest … image will be created … $ # After that you can start a terminal in the development container with: $ docker run --env-file ~/work/envs.env -v $(git rev-parse --show-toplevel):/go/src -w /go/src -u $(id -u ${USER}):$(id -g ${USER}) --group-add $(stat -c '%g' /var/run/docker.sock) -v /var/run/docker.sock:/var/run/docker.sock --network=host -it asodev:latest /bin/bash It is not recommended to mount the source like this on Windows (WSL2) as the cross-VM file operations are very slow.\nOption 2: ./dev.sh #  If you are using Linux, instead of using VS Code you can run the dev.sh script in the root of the repository. This will install all required tooling into the hack/tools directory and then start a new shell with the PATH updated to use it.\nRunning integration tests #  Basic use: run task controller:test-integration-envtest.\nRecord/replay #  The task controller:test-integration-envtest runs the tests in a record/replay mode by default, so that it does not touch any live Azure resources. (This uses the go-vcr library.) If you change the controller or other code in such a way that the required requests/responses from ARM change, you will need to update the recordings.\nTo do this, delete the recordings for the failing tests (under {test-dir}/recordings/{test-name}.yml), and re-run controller:test-integration-envtest. If the test passes, a new recording will be saved, which you can commit to include with your change. All authentication and subscription information is removed from the recording.\nTo run the test and produce a new recording you will also need to have set the required authentication environment variables for an Azure Service Principal: AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET. This Service Principal will need access to the subscription to create and delete resources.\nIf you need to create a new Azure Service Principal, run the following commands:\n$ az login … follow the instructions … $ az account set --subscription {the subscription ID you would like to use} Creating a role assignment under the scope of \u0026quot;/subscriptions/{subscription ID you chose}\u0026quot; … $ az ad sp create-for-rbac --role contributor --name {the name you would like to use} { \u0026quot;appId\u0026quot;: \u0026quot;…\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;{name you chose}\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;{name you chose}\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;…\u0026quot;, \u0026quot;tenant\u0026quot;: \u0026quot;…\u0026quot; } The output contains appId (AZURE_CLIENT_ID), password (AZURE_CLIENT_SECRET), and tenant (AZURE_TENANT_ID). Store these somewhere safe as the password cannot be viewed again, only reset. The Service Principal will be created as a “contributor” to your subscription which means it can create and delete resources, so ensure you keep the secrets secure.\nRunning live tests #  If you want to skip all recordings and run all tests directly against live Azure resources, you can use the controller:test-integration-envtest-live task. This will also require you to set the authentication environment variables, as detailed above.\nRunning a single test #  By default task controller:test-integration-envtest and its variants run all tests. This is often undesirable as you may just be working on a single feature or test. In order to run a subset of tests, use:\nTEST_FILTER=test_name_regex task controller:test-integration-envtest Running the operator locally #  If you would like to try something out but do not want to write an integration test, you can run the operation locally in a kind cluster.\nBefore launching kind, make sure that your shell has the AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables set. See above for more details about them.\nOnce you\u0026rsquo;ve set the environment variables above, run one of the following commands to create a kind cluster:\n Service Principal authentication cluster: task controller:kind-create-with-service-principal. AAD Pod Identity authentication enabled cluster (emulates Managed Identity): controller:kind-create-with-podidentity.  You can use kubectl to interact with the local kind cluster.\nWhen you\u0026rsquo;re done with the local cluster, tear it down with task controller:kind-delete.\nSubmitting a pull request #  Pull requests opened from forks of the azure-service-operator repository will initially have a skipped Validate Pull Request / integration-tests check which will prevent merging even if all other checks pass. Once a maintainer has looked at your PR and determined it is ready they will comment /ok-to-test sha=\u0026lt;sha\u0026gt; to kick off an integration test pass. If this check passes along with the other checks the PR can be merged.\nCommon problems and their solutions #  Error loading schema from root #  Full error:\n error loading schema from root \u0026hellip; open /azure-service-operator/v2/specs/azure-resource-manager-schemas/schemas/2019-04-01/deploymentTemplate.json no such file or directory\n This git repo contains submodules. This error occurs when the submodules are missing, possibly because the repo was not cloned with --recurse-submodules.\nTo resolve this problem, run git submodule init and git submodule update and then try building again.\n"},{"id":1,"href":"/azure-service-operator/architecture-decision-records/","title":"Architecture Decision Records","section":"","content":"Architecture Decision Records #  This folder documents some of the architecturally significant decisions we\u0026rsquo;ve made during the Azure Service Operator project.\nFiles are listed chronologically by month, and should contain the following sections:\n Context Decision Status Consequences Experience Report References  For background information, check out this Cognitect blog entry.\nADR documents should be updated over time to keep them relevant.\n"},{"id":2,"href":"/azure-service-operator/architecture-decision-records/adr-2021-02-property-conversions/","title":"2021-02: Property Conversions","section":"Architecture Decision Records","content":"Property Conversions #  Context #  To facilitate the use of dedicated storage variants for persistence of our custom resources, we need to codegen conversion routines that will copy all properties defined on one version of a resource to another version.\nGiven the way resources evolve from version to version, these we need to support a wide range of conversions between types that are similar, but not identical.\nFor example, looking primitive types (such as string, int, and bool):\nA primitive type can become optional in a later version when a suitable default is provided, or when an alternative becomes available.\nAn optional primitive type can become mandatory in a later version when deprecation of other properties leaves only one way to do things.\nA primitive type can be replaced by an enumeration when validation for a property (such as VM SKU) is tightened up to avoid problems.\nA primitive type can be replaced by an alias when additional validation constraints (e.g., limiting bounds or a regular expression) are added to address problems.\nThese transformations (and others) can occur in combination with each other (e.g., a primitive type replaced by an optional enumeration) and with other constructs, such as maps, arrays, resources, and complex object types.\nOther constraints apply, such as the need to clone optional values during copying lest the two objects become coupled with changes to one being visible on the other.\nWhen implementation began, it very quickly became apparent that independently addressing every possible conversion requirement would be difficult to impossible given the combinatorial explosion of possibilities.\nDecision #  Use a recursive algorithm to generate the required conversion by composing simpler conversion steps together.\nFor example, given a need to copy an optional string to an optional enumeration, the process works as follows:\nOriginal problem: *string -\u0026gt; *Sku (where Sku is based on a string)\nThe handler assignFromOptional knows how to handle the optionality of *string, reducing the size of the problem. A recursive call is made to solve the new problem.\nReduced problem: string -\u0026gt; *Sku\nThe handler assignToOptional knows how to handle the optionality of *Sku, reducing the size of the problem further. A recursive call is made to solve the new problem.\nReduced problem #2: string -\u0026gt; Sku\nThe handler assignToAliasedPrimitive recognizes that Sku is an enumeration based on string and reduces the problem. A recursive call is made to solve the new problem.\nReduced problem #3: string -\u0026gt; string\nNow assignPrimitiveFromPrimitive can handle the reduced problem, generating a simple assignment to copy the value across:\ndestination = source Working backwards, the handler assignToAliasedPrimitive injects the required cast to the enumeration\ndestination = Sku(source) Then, assignToOptional injects a local variable and takes its address\nsku := Sku(source) destination = \u0026amp;sku Finally, assignFromOptional injects a check to see if we have a value to assign in the first place, assigning a suitable zero value if we don\u0026rsquo;t:\nif source != nil { sku := Sku(source) destination := \u0026amp;sku } else { destination := \u0026#34;\u0026#34; } Status #  Successfully implemented. First commit in PR# #378\nThe full list of implemented conversions can be found in property_conversions.go.\nConsequences #  It required some finessing of the conversion code to generate high quality conversions; early results were functional but not comparable with handwritten efforts.\nAs we’ve encountered new cases where new transformations are required, the list of conversions has been extended with additional handlers, including:\n•\tSupport for enumerations (PR #392) •\tConversion of complex object types (PR #395) •\tSupport for aliases of otherwise supported types (PR #433) •\tAbility to read values from functions (PR #1545) •\tSupport for JSON properties (PR #1574) •\tProperty Bags for storing/recalling unknown properties (PR# 1682)\nExperience Report #  TBC\nReferences #  TBC\n"},{"id":3,"href":"/azure-service-operator/architecture-decision-records/adr-2020-11-abstract-syntax-trees/","title":"2020-11: AST Library Choice","section":"Architecture Decision Records","content":"Abstract Syntax Tree Library Choice #  Context #  When we first started working on the Azure Service Operator Code Generator, we used the standard go ast library to create the final Go code.\nUnfortunately, we have run into a number of significant limitations with this library.\n  Comments are often emitted in unexpected places.\nThey would often appear at the end of the previous line, which conveys an incorrect meaning to a casual reader of the code. As a workaround, we found that prefacing some comments with newline characters would force them onto the next line. While hacking the library in this function worked, we are concerned that it\u0026rsquo;s a fragile technique that would break in a future version of Go. We also found situations where this technique did not work.\n  Generated code does not always comply with go fmt standards.\nFor some constructs, the generated code would fail the go fmt check included in our continuous integration (CI) builds. We found a workaround that involved using ast to read in a generated file and immediately rewrite it back out again, immitating the results of go fmt.\n  Generated code does not always compile.\nFor some constructs, we found the comments were emitted at the start of the line of code (not on the previous line), resulting in the code being commented out. We found no workaround for this.\n  Poor control over whitespace\nWe were unable to find a reliable technique within ast to introduce blank lines betweens stanza of code, or before comment blocks. As a workaround, we would read in the formatted code, scan it for comments and manually inject blank lines before each comment block.\n  We have come to the conclusions that the ast library is intended as a way to make minor modifications to existing well formatted Go files, not for the creation of entirely new Go files from scratch.\nA potential alternative is the dst (Decorated Syntax Tree) package. This package was specifically created to address the kinds of issues described above.\nDecision #  After some informative trials, we have decided to adopt the dst library.\nStatus #  Adopted.\nInitial change committed in PR#366.\nConsequences #  With a very high level of API compatibility, we were able to introduce dst with a low level of code churn by aliasing the import as ast in most files. Future changes should remove this aliasing as files are modified.\nThe DST library requires that any node be used exactly once, and will panic if a node is reused in mulitiple locations. We\u0026rsquo;ve mitigated this by liberally cloning nodes as we build the desired final syntax tree. (See astbuilder.Expressions(), introduced in PR#1613 and astbuilder.Statements(), introduced in PR#427).\nExperience Report #  TBC\nReferences #  Go AST (Abstract Syntax Tree) library\nDST (Decorated Syntax Tree) library\n"},{"id":4,"href":"/azure-service-operator/architecture-decision-records/adr-2020-07-pipeline-architecture/","title":"2020-07: Pipeline Architecture","section":"Architecture Decision Records","content":"Pipeline architecture #  Context #  As the complexity of code generation grew, and with a partial view of the complexity yet to come, it became clear that we were facing a challenge. Each additional required function was making integration and testing progressively more difficult.\nWe also have the requirement to support a parallel pipeline that generates code appropriate for CrossPlane integration, and have a strong desire for an approach that minimizes the amount of ongoing maintenance.\nIntroducing a formal pipeline architecture, where each function forms a distinct pipeline stage, solves these problems.\n  Each pipeline stage can (in theory, at least) be tested independently, improving our confidence in the functionality\n  Dependencies between stages can be explicitly declared, allowing the validity of the pipeline to be checked\n  A single factory method can create both required pipeline variants\n  Decision #  Adopted.\nStatus #  Pipeline introduced in PR#171.\nStage prerequisites introduced in PR#366 and postrequisites in PR#1541.\nConsequences #  Individual pipeline stages are independently tested, but some require considerable set up to create the expected initial state required for execution. The addition of mini-pipelines in PR#1649 partially mitigates this.\nReferences #  Pipeline (software) on Wikipedia\n"},{"id":5,"href":"/azure-service-operator/architecture-decision-records/adr-2020-04-code-generation/","title":"2020-04: Why Code Generation?","section":"Architecture Decision Records","content":"Why Code Generation? #  Context #  The Azure Service Operator CSE team had successfully developed support for a handful of ARM (Azure Resource Manager) based custom resources over the course of eighteen months or so.\nTheir experience yielded several useful lessons.\n  Working closely with their customers, they only built out resource support for those features required by the current customer base. While this successfully reduced the amount of work, each additional customer brought new requirements and a need to revisit/enhance existing resources.\n  Adding support for a brand-new resource was often needed by new adopters of ASO and required a significant effort. Extending ASO to have broad support across all (or nearly all) Azure ARM resources would require a major investment\n  A separate controller for each resource type introduced inconsistencies in behavior\n  Multiple people concurrently had the idea of using code generation based on ARM JSON Schema to address these problems:\n  By default, a code generator would mirror the the entire ARM specification for a resource in Kubernetes, providing full feature coverage with consistent naming.\n  The overhead of adding a new resource would be much reduced, potentially limited to as little as testing\n  Using a single generic controller for all resources would ensure consistent behavior\n  Decision #  A proof of concept by David Justice proved the concept sufficiently that we had confidence the idea would work.\nStatus #  Adopted, based on the proof of concept code in the Azure/k8s-infra repository.\nConsequences #  We think it\u0026rsquo;s working.\nExperience Report #  The ARM JSON Schema definitions weren’t sufficient in isolation as they don’t contain all of the information required. Use of Azure Swagger specifications to augment our status types was introduced in PR #205.\nMigration of the code generator into the ASO repo occurred in PR #1427.\nReferences #  None.\n"},{"id":6,"href":"/azure-service-operator/architecture-decision-records/adr-2021-06-api-version-recovery/","title":"Adr 2021 06 API Version Recovery","section":"Architecture Decision Records","content":"API Version Recovery #  Context #  In addition to structural changes, there may be behaviour changes between ARM API versions. It is therefore important that we use the correct API version - the version requested by the user - when interacting with ARM, to ensure that we get the expected behaviour.\nRevisting the CRM example from the Versioning specification, consider what happens if we have two available versions of the resource Person, lets call them v1 and v2. In v2 the new properties PostalAddress and ResidentialAddress are mandatory, requiring that everyone have a both a mailing address and a home.\nIf we have a valid v1 Person, trying to submit that through the v2 ARM API will fail because it\u0026rsquo;s missing these addresses.\nWe need to be able to pivot from the version Person we have \u0026ldquo;in hand\u0026rdquo; (usually the Hub version) back to the original version in order to generate the correct ARM payload.\nWe also have a performance issue - our current infrastructure for conversions is oriented to convert an entire resource, including both Spec and Status.\nWhen submitting a request to ARM we\u0026rsquo;re only interested in the Spec; any effort expended on conversion of the current Status is wasted.\nSimilarly, when retrieving Status from ARM, we\u0026rsquo;re only interested in conversion of that; any effort expended on conversion of the Spec is also wasted.\nDecision #  The original API version used to create the custom resource will as the API version for ARM. From the above example, a v1.Person will result in API version v1 being used, and for a v2.Person we\u0026rsquo;ll use API version v2.\nWe\u0026rsquo;ll preserve this as another property on the CRD, but one the user does not need to manually provide, allowing us to know which API version to use when interacting with ARM.\nThe code generator will also inject functions allowing easy access to this information.\nHelper methods in the generic reconciler will use these functions implement the required pivot between the current hub version and the specified API version.\nAPI Preservation #  Into the API version of each resource\u0026rsquo;s Spec, inject function OriginalVersion(), with a hard coded return value:\n// OriginalVersion returns the original API version used to create the resource. func (storageAccountsSpec *StorageAccounts_Spec) OriginalVersion() string { return GroupVersion.Version } var GroupVersion = schema.GroupVersion{Group: \u0026#34;storage.azure.com\u0026#34;, Version: \u0026#34;v1alpha1api20210401\u0026#34;} Into the Storage version of each resource\u0026rsquo;s Spec, inject string property OriginalVersion to persist the value returned by OriginalVersion().\ntype StorageAccounts_Spec struct { // ... elided ... \tOriginalVersion string `json:\u0026#34;originalVersion\u0026#34;` // ... elided ... } The generated AssignProperties*() methods will copy OriginalVersion as expected.\nRetrieving the API Version #  Into each resource, inject function OriginalGVK() to return the required group-version-kind for the original version of the specified resource:\n// OriginalGVK returns a GroupValueKind for the original API version used to create the resource func (storageAccount *StorageAccount) OriginalGVK() *schema.GroupVersionKind { return \u0026amp;schema.GroupVersionKind{ Group: GroupVersion.Group, Version: storageAccount.Spec.OriginalVersion, Kind: \u0026#34;StorageAccount\u0026#34;, } } Implementations will use either the OriginalVersion property (as shown above), or the OriginalVersion() function, depending on the resource variant.\nRetrieving Spec and Status #  Two new interfaces will be added to the genruntime package:\ntype ConvertibleSpec interface { // ConvertSpecTo will populate the passed Spec by copying over all available information from this one \tConvertSpecTo(destination ConvertibleSpec) error // ConvertSpecFrom will populate this spec by copying over all available information from the passed one \tConvertSpecFrom(source ConvertibleSpec) error } type ConvertibleStatus interface { // ConvertStatusTo will populate the passed Status by copying over all available information from this one \tConvertStatusTo(destination ConvertibleStatus) error // ConvertStatusFrom will populate this status by copying over all available information from the passed one \tConvertStatusFrom(source ConvertibleStatus) error } Implementations of these interfaces will be added to all generated Spec and Status types, leveraging the AssignProperties*() methods already being generated.\nThe KubernetesResource interface and all implementations will be modified by adding four new methods:\ntype KubernetesResource interface { // GetSpec returns the specification of the resource \tGetSpec() ConvertibleSpec // GetStatus returns the current status of the resource \tGetStatus() ConvertibleStatus // NewEmptyStatus returns a blank status ready for population \tNewEmptyStatus() ConvertibleStatus // SetStatus updates the status of the resource \tSetStatus(status ConvertibleStatus) error } Spec Conversion #  A helper method for the generic reconciler will be added to return the correct Spec from any resource:\n// GetVersionedSpec returns a versioned spec for the provided resource; // the original API version used when the resource was first created // is used to identify the version to return func GetVersionedSpec(metaObject MetaObject, scheme *runtime.Scheme) (ConvertibleSpec, error) { // ... elided ... } Status Conversion #  Two similar helper methods will be added for support of Status.\n// GetVersionedStatus returns a versioned status for the provided resource; // the original API version used when the resource was first created // is used to identify the version to return func GetVersionedStatus(metaObject MetaObject, scheme *runtime.Scheme) (ConvertibleStatus, error) { // ... elided ... } // NewEmptyVersionedStatus returns a blank versioned status for the provided // resource; the original API version used when the resource was first created // is used to identify the version to return func NewEmptyVersionedStatus(metaObject MetaObject, scheme *runtime.Scheme) (ConvertibleStatus, error) { // ... elided ... } Status #  Successfully implemented across several PRs:\n PR #1773 - Add GetStatus() and GetSpec() to all resources PR #1787 - Implement SetStatus() for all Resources PR #1851 - Integrate support for multiple API versions PR #1875 - Eliminate much use of reflection by using generated code PR #1889 - Introduce API Version helper methods PR #1935 - Integrate API versioning PR #1936 - Activate versioning stages in pipeline and update generated files PR #1973 - Generate patches for Kustomize to enable storage versioning PR #1985 - Copy both ObjectMeta and TypeMeta when doing version conversion  Consequences #  TBC\nExperience Report #  TBC\nReferences #  TBC\n"},{"id":7,"href":"/azure-service-operator/architecture-decision-records/adr-2022-01-reconciler-extensions/","title":"Adr 2022 01 Reconciler Extensions","section":"Architecture Decision Records","content":"Reconciler Extensions #  Context #  We are discovering inconsistencies in the way different Azure Resource Providers behave.\nThese inconsistencies are anticipated. Given the number of disparate teams independently implementing their providers and given the way the ARM guidance has changed over time, differences in behaviour are to be expected.\nFor example, the Azure Redis Resource Provider can return a HTTP conflict (409) with a retry message indicating that it\u0026rsquo;s not a fatal error but a transient one.\nCurrently we handle this with special case behaviour in error_classifier.go (see PR#2008 for details).\nHowever, directly changing our generic reconciler is a problem. The growing complexity and increasing number of resources makes this unsustainable, resulting in a risk that changes made to support a new resource type will break an existing one.\nWe need a way to precisely customize behaviour on a per-resource basis, allowing us to maintain consistent functionality within the operator.\nWhen a new version of a resource is introduced, we don\u0026rsquo;t want to lose any existing customizations. This rules out hosting the extensions directly on the current hub type.\nIt\u0026rsquo;s quite likely that some extension points will need to import our generated resource types to allow per-version manipulation. This rules out referencing extensions from the generated resource types.\nWe don\u0026rsquo;t want extension points to duplicate functionality already present in the generic reconciler as this runs the risk of inconsistencies (especially if we change the generic reconciler itself), and because it introduces a significant overhead to implementation. Instead, extension points will provide an opportunity to modify the current behaviour. An extension point that does nothing should have the same effect as one that hasn\u0026rsquo;t been implemented.\nDecision #  To provide a stable implementation location for extensions, we\u0026rsquo;ll introduce a dedicated customization package alongside the existing versioned packages for each supported service. This package will provide customization for all supported resource versions. For example, alongside the existing batch packages v1alpha1api20210101, and v1alpha1api20210101storage, we will generate customization.\nInto the customization package we will codegen a skeleton framework. For each resource there will be a separate customization host type, suffixed with Extensions. For example, for the compute resources Disk, VirtualMachine, and VirtualMachineScaleSet we will create the extension types DiskExtensions, VirtualMachineExtensions, and VirtualMachineScaleSetExtensions.\nWe\u0026rsquo;ll enhance the existing registration file (controller_resources_gen.go) to also provide registration of extensions. They will be indexed by the GVK of each resource, allowing easy lookup by the generic reconciler.\nFor each supported extension point in our reconciler, we\u0026rsquo;ll define a separate interface containing a single method with exactly the parameters required for that extension point. These interfaces will be found in the genruntime/extensions package.\nImplementers will assert type compatibility with the desired extension interface to ensure at compile time they have the correct method signature. This will also ensure we get compilation errors if we unexpectedly need to modify the signature of an extension point in the future.\nThe generic reconciler will identify available extensions by testing for the presence of the extension interface.\nExtension method signature will follow a common pattern:\n The parameter list will start with any input parameters required. These parameters will be custom for each extension point. A next parameter, a func that the extension should call to invoke the default behaviour.\nThis allows the extension to act both before and after the default behaviour, or even to skip the default behaviour if necessary. An apiVersion parameter that receives the actual ARM API version being used.\nThis enables extensions to do different things based on the ARM API version, such as correcting the behaviour of one version. A log parameter allowing additional information to be logged.\nSome entry/exit information will be automatically logged, so extensions will only need to log any additions Optionally, an extension point may return a value.\nIf multiple return values are needed, these will be wrapped in a custom struct, with a public factory method. There will always be an error return value, and it will always be the last one.  Example #  To allow customization of error handling for Azure Redis, we\u0026rsquo;ll introduce an extension point for classification of errors.\nWe create the required extension interface in the package genruntime/extensions:\n// error_classifier_extension.go  package extensions type ErrorClassifier interface { // Classify the provided error, returning details about the error including whether it is fatal  // or can be retried.  // cloudError is the error returned from ARM.  // next is the function to call for standard classification behaviour.  // apiVersion is the ARM API version used for the request.  // log is a logger than can be used for telemetry.  ClassifyError( cloudError *genericarmclient.CloudError, next func(*genericarmclient.CloudError) (*CloudErrorDetails, error), apiVersion string, log logr.Logger) (*CloudErrorDetails, error) } In the cache/customization package, a host type for the extension point will be generated:\n// redis_extensions.go  package customization type RedisExtensions struct{} This type doesn\u0026rsquo;t contain any members as we require extension points to be pure functions; if they contain mutable state, we run the risk of introducing concurrency and sequence errors that would be hard to diagnose and fix.\nManual implementation of the extension point will be in a different file so that it\u0026rsquo;s not obliterated next time we re-run the generator:\n// redis_extensions.go  package customization var _ extensions.ErrorClassifier = \u0026amp;RedisExtensions{} func (e *RedisExtensions) ClassifyError( cloudError *genericarmclient.CloudError, next func(*genericarmclient.CloudError) *CloudErrorDetails, apiVersion string, log logr.Logger) (*CloudErrorDetails, error) err, result := next(cloudError) if err != nil { return nil, err } if inner := cloudError.InnerError; inner != nil { code := to.String(inner.Code) if code == \u0026#34;Conflict\u0026#34; { // For Azure Redis, Conflict errors may be retried, as they are  // returned when a required dependency is still being created  result.Classification = CloudErrorRetryable } } return result, nil } Passing in the default behaviour as next allows extensions to augment and/or supplant the behaviour with a great deal of flexibility.\nIn the generic reconciler, we make use of the extension:\n// azure_generic_arm_reconciler.go  func (r *AzureDeploymentReconciler) makeReadyConditionFromError( cloudError *genericarmclient.CloudError) conditions.Condition { // ... existing code elided ...  ext := r.FindErrorClassifierExtension() errorDetails, err := ext.ClassifyError(cloudError, apiVersion, r.ClassifyCloudError) if err != nil { // handle error  } // ... existing code elided ... } The helper method FindErrorClassifierExtension() does the lookup for the extension, and always returns a default implementation that can be invoked. The consuming code therefore doesn\u0026rsquo;t need to worry about null checking, only error handling. To ensure consistent telemetry no matter who implements an extension, this default implementation will include logging.\n// error_classifier_extension.go  package extensions //TODO: This needs a better name type ErrorClassifierDefault struct { extension ErrorClassifierExtension } var _ ErrorClassifier = \u0026amp;ErrorClassifierDefault{} // NewErrorClassifierDefault creates a new ErrorClassifier that will invoke the passed host if it implements // the ErrorClassifier interface. func NewErrorClassifier(host interface{}) *LoggingErrorClassifier { result := \u0026amp;ErrorClassifier{} // Hook up our host if appropriate  if extension, ok := host.(ErrorClassifierExtension); ok { result.extension = extension } return result } func (classifier *ErrorClassifier) ClassifyError( cloudError *genericarmclient.CloudError, next func(*genericarmclient.CloudError) (*CloudErrorDetails, error), apiVersion string, log logr.Logger) (*CloudErrorDetails, error) { log.V(Verbose).Info(\u0026#34;Classifying error\u0026#34;, \u0026#34;error\u0026#34;, cloudError) var details *CloudErrorDetails var err error if classifier.extension != null { log.V(Verbose).Info(\u0026#34;Invoking extension\u0026#34;) details, err = classifier.extension.ClassifyError(cloudError, next, apiVersion, log) } else { details, err = next(cloudError, next, apiVersion, log) } if err != nil { log.Error(\u0026#34;Failure classifying error\u0026#34;, \u0026#34;error\u0026#34;, err) } else { log.V(Verbose).Info(\u0026#34;Success classifying error\u0026#34;, \u0026#34;result\u0026#34;, details) } return details, err } Status #  Proposed.\nConsequences #  TBC.\nExperience Report #  TBC.\nReferences #  "},{"id":8,"href":"/azure-service-operator/contributing/add-a-new-code-generated-resource/","title":"Add a New Code Generated Resource","section":"For Contributors","content":"Adding a new code generated resource to ASO v2 #  This document discusses how to add a new resource to the ASO v2 code generation configuration. Check out this PR if you\u0026rsquo;d like to see what the end product looks like.\nWhat resources can be code generated? #  Any ARM resource can be generated. There are a few ways to determine if a resource is an ARM resource:\n If the resource is defined in the ARM template JSON schema, or the auto-generated ARM template JSON schema it is an ARM resource. If the resource is defined in a resource-manager folder in the Azure REST API specs repo, it is an ARM resource.  If the resource is not in either of the above places and cannot be deployed via an ARM template, then it\u0026rsquo;s not an ARM resource and currently cannot be code generated.\nDetermine a resource to add #  There are three key pieces of information required before adding a resource to the code generation configuration file, and each of them can be found in the ARM template JSON schema or the auto-generated ARM template JSON schema. To get started, find the entry for the resource in one of the two templates mentioned. For example, the entry for Azure Network Security Groups looks like this: { \u0026quot;$ref\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2020-11-01/Microsoft.Network.json#/resourceDefinitions/networkSecurityGroups\u0026quot; },\nNote: In many cases there will be multiple entries for the same resource, each with a different api-version. It is strongly recommended that you use the latest available non-preview api-version.\n  The name of the resource.\nYou usually know this going in. In our example above, the name of the resource is networkSecurityGroups.\n  The group the resource is in.\nThis is usually named after the Azure service, for example resources or documentdb. In our example entry from above, this is network.\n  The api-version of the resource.\nThis is usually a date, sometimes with a -preview suffix. In our example entry from above, this is 2020-11-01.\n  Adding the resource to the code generation configuration file #  The code generation configuration file is located here. To add a new resource to this file, find the objectModelConfiguration section of the file.\nFind the configuration for the group you want; if it\u0026rsquo;s not there, create a new one, inserting it into the existing list in alphabetical order. Within the group, find the version you want; again, create a new one if it\u0026rsquo;s not already there.\nAdd your new resource to the list for that version, including the directive $export: true nested beneath. The final result should look like this:\n\u0026lt;group\u0026gt;: \u0026lt;version\u0026gt;: \u0026lt;resource name\u0026gt;: # singular, typically just remove the trailing \u0026#34;s\u0026#34; $export: true For example, taking the Azure Network Security Groups sample from above:\nnetwork: 2020-11-01: NetworkSecurityGroup: $export: true If ASO was already configured to generate resources from this group (or version), you will need to add your new configuration around the existing values.\nRun the code generator #  Follow the steps in the contributing guide to set up your development environment. Once you have a working development environment, run the task command to run the code generator.\nFix any errors raised by the code generator #  \u0026lt;Resource\u0026gt; looks like a resource reference but was not labelled as one #  Example:\n Replace cross-resource references with genruntime.ResourceReference: [\u0026ldquo;github.com/Azure/azure-service-operator/hack/generated/_apis/containerservice/v1alpha1api20210501/PrivateLinkResource.Id\u0026rdquo; looks like a resource reference but was not labelled as one.\n To fix this error, determine whether the property in question is an ARM ID or not, and then update the objectModelConfiguration section in the configuration file.\nFind the section you added earlier, adding your property with an $armReference: declaration nested below.\nIf the property is an ARM ID, use $armReference: true to flag that property as a reference:\nnetwork: 2020-11-01: NetworkSecurityGroup: $export: true PrivateLinkResource: $armReference: true # the property IS an ARM reference If the property is not an ARM ID, use $armReference: false instead:\nnetwork: 2020-11-01: NetworkSecurityGroup: $export: true PrivateLinkResource: $armReference: false # the property IS NOT an ARM reference TODO: expand on other common errors\nExamine the generated resource #  After running the generator, the new resource you added should be in the apis directory.\nHave a look through the files in the directory named after the group and version of the resource that was added. In our NetworkSecurityGroups example, the best place to start is /v2/api/network/v1alpha1api20201101/network_security_group_types_gen.go There may be other resources that already exist in that same directory - that\u0026rsquo;s expected if ASO already supported some resources from that provider and API version.\nStarting with the network_security_group_types_gen.go file, find the struct representing the resource you just added. It should be near the top and look something like this:\n// +kubebuilder:object:root=true // +kubebuilder:subresource:status // +kubebuilder:storageversion // +kubebuilder:printcolumn:name=\u0026#34;Ready\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].status\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;Reason\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].reason\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;Message\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].message\u0026#34; //Generated from: https://schema.management.azure.com/schemas/2020-11-01/Microsoft.Network.json#/resourceDefinitions/networkSecurityGroups type NetworkSecurityGroup struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec NetworkSecurityGroups_Spec `json:\u0026#34;spec,omitempty\u0026#34;` Status NetworkSecurityGroup_Status_NetworkSecurityGroup_SubResourceEmbedded `json:\u0026#34;status,omitempty\u0026#34;` } Look over the Spec and Status types and their properties (and the properties of their properties and so-on). The Azure REST API specs and ARM template JSON schemas which these types were derived from are not perfect. Sometimes they mark a readonly property as mutable or have another error or mistake.\nIf you do identify properties which should be removed or changed, you can make customizations to the resource in the typeTransformers section of the code generation config. The most common issues have their own sections:\n # Deal with readonly properties that were not properly pruned in the JSON schema # Deal with properties that should have been marked readOnly but weren't  Write a CRUD test for the resource #  The best way to do this is to start from an existing test and modify it to work for your resource. It can also be helpful to refer to examples in the ARM templates GitHub repo.\nRun the CRUD test for the resource and commit the recording #  See the code generator README for how to run recording tests.\nAdd a new sample #  The samples are located in the samples directory. There should be at least one sample for each kind of supported resource. These currently need to be added manually. It\u0026rsquo;s possible in the future we will automatically generate samples similar to how we automatically generate CRDs and types, but that doesn\u0026rsquo;t happen today.\nSend a PR #  You\u0026rsquo;re all done!\n"},{"id":9,"href":"/azure-service-operator/contributing/create-a-new-release/","title":"Create a New Release","section":"For Contributors","content":"Creating a new release of ASO v2 #   Go to the releases page and draft a new release. In the tag dropdown, type the name of the new tag you\u0026rsquo;d like to create (it should match the pattern of previous releases tags, for example: v2.0.0-alpha.1). The release target should be main (the default). If publishing an alpha/beta, be sure to mark the release as a pre-release. Write a \u0026ldquo;Release Notes\u0026rdquo; section. This can be generated by looking through the commits between the last release and now: git log v2.0.0-alpha.0..main. In the future it would be nice to automatically generate this. Publish the release. This will automatically trigger a GitHub action to build and publish an updated Docker image with the latest manager changes. Ensure that the action associated with your release finishes successfully.  Testing the new release #   Download the yaml file from the release page Create a kind cluster: task controller:kind-create Install cert-manager: task controller:install-cert-manager Create the namespace for the operator: k create namespace azureserviceoperator-system Source the SP credentials to use for the secret and then run ./scripts/deploy_testing_secret.sh sp Deploy the operator from MCR: k apply --server-side=true -f \u0026lt;path-to-downloaded-yaml\u0026gt; (We need to use server-side apply because the CRD for VirtualMachines is large enough that it can\u0026rsquo;t fit in the last-applied-configuration annotation client-side kubectl apply uses.) Wait for it to start: k get all -n azureserviceoperator-system Create a resource group and a vnet in it (the vnet is to check that conversion webhooks are working, since there aren\u0026rsquo;t any for RGs): k apply -f v2/config/samples/resources/v1alpha1api20200601_resourcegroup.yaml k apply -f v2/config/samples/network/v1alpha1api20201101_virtualnetwork.yaml  Make sure they deploy successfully - check in the portal as well.  Fixing an incorrect release #  If there was an issue publishing a new release, we may want to delete the existing release and try again. Only do this if you\u0026rsquo;ve just published the release and there is something wrong with it. We shouldn\u0026rsquo;t be deleting releases people are actually using.\n Delete the release in the releases page. Delete the tag: git push \u0026lt;origin\u0026gt; --delete \u0026lt;tag\u0026gt;, for example git push origin --delete v2.0.0-alpha.1.  At this point, you can safely publish a \u0026ldquo;new\u0026rdquo; release with the same name.\n"},{"id":10,"href":"/azure-service-operator/contributing/generator-overview/","title":"Generator Overview","section":"For Contributors","content":"Code Generator Overview #  Core to Azure Service Operator (ASO) v2 is our code generator. This consumes ARM JSON Schema and Swagger specifications and generates code for each desired resource that works with our generic operator reconciler.\nCode Structure #  The key packages used to structure the code of the generator are as follows:\n   Package Content     astmodel Short for Abstract Syntax Tree Model, this package contains the core data types for defining the Go functions, data types, and methods we generate.   functions Support for generation of individual functions, based on the astmodel.Functions interface   interfaces Support for generation of interface implementations, based on the astmodel.InterfaceImplementer interface   testcases Support for generation of test cases (used to verify our generated code works as expected), based on the astmodel.TestCase interface   astbuilder Intention revealing utility methods for creating the underlying Go abstract syntax tree we serialize as the last step of generation.   codegen The core processing pipeline of the code generator   codegen/pipeline Individual pipeline stages that are composed to form the code generator itself.   test Support methods to make writing tests easier    Object Model #  At the core, the code generator works with a rich semantic object model that captures the structure of the resources (and related types) we are generating.\nUnderpinning this object model is the astmodel package.\nThe interface astmodel.Type represents a specific data type. The most widely used implementations of Type fall into a few separate groups:\n Simple Go types, including PrimitiveType, ArrayType, and MapType. Complex types with internal structure, most notably ObjectType and ResourceType. Wrapper types that provide additional semantics, including OptionalType, ErroredType, FlaggedType and ValidatedType.  This list is not exhaustive; other implementations of Type are used within limited scopes. For example, AllOfType and OneOfType represent JSON Schema techniques for creating object definitions via composition.\nUsefully, there is also TypeName which is both a type in itself and an indirect reference to a type defined elsewhere.\nWhen a Type is given a TypeName, it becomes a TypeDefinition and can be emitted as the source code for a Go type definition. A set of many TypeDefinition, each with a unique name is a Types.\nBoth ResourceType and ObjectType act as containers, each implementing PropertyContainer, FunctionContainer, and TestCaseContainer. These do pretty much what you\u0026rsquo;d expect from the names, though the implementations differ between ResourceType and ObjectType. For example, where an ObjectType implements PropertyContainer by providing support for an arbitrary set of properties, ResourceType has only Spec and Status.\nMost implementations of astmodel.Function are found in the functions package. New function implementations should go here; existing implementations are slowly being relocated.\nAll implementations of astmodel.TestCase are found in the testcases package. New test case implementations should go here.\nResources, Objects and other types #  Each distinct resource is represented by a ResourceType. The Spec of each resource is an ObjectType containing a collection of PropertyDefinition values, along with implementations of the Function, and TestCase interfaces. The Status of a resource is a different ObjectType.\nSome properties capture primitive values (strings, integers, and so on), while others are themselves complex ObjectType definitions, forming a hierarchy of information. MetaType wrappers (including OptionalType, and ValidatedType) are used to add semantic information to both properties and to type definitions.\nGenerator Pipeline #  The code generator itself, found in the package codegen, is structured as a pipeline, composed of a series of stages that transform our object model incrementally. All the pipeline stages are found in the subpackage codegen/pipeline.\nOne reason for this is to allow the creation of multiple pipelines (currently we have separate definitions targeting Azure, Crossplane, and for testing), each sharing the majority of their implementation. Another reason is to allow individual pipeline stages to be tested in isolation, though not all existing pipeline stages have tests in this form. New stage implementations should have isolated tests where possible.\nPipeline stages are instances of pipeline.Stage. Each has a factory method that returns a pipeline.Stage instance. In operation, each accepts a pipeline.State containing the current object model and transforms it into a new state, that is passed to the next stage in turn. If a stage returns an error, the pipeline run is aborted.\nNew stages should use the MakeStage() function. You\u0026rsquo;ll see some older stages that predate a structural change use the deprecated MakeLegacyStage() factory instead; these older stages are slowly being migrated and MakeLegacyStage() will be deleted when this is complete.\nCode Generation #  Code is generated as a Go abstract syntax tree fragments which are then serialized to files as compilable Go code, forming a part of the operator itself.\nTo make generation easier, our astbuilder package contains a wide variety of helper methods that allow declarative construction of the required tree. We are using the dst package instead of the Go core ast package, as it provides better control of comments and whitespace in the final output.\n"},{"id":11,"href":"/azure-service-operator/design/clarifying-object-structure/","title":"Clarifying Object Structure","section":"Design \u0026 Specifications","content":"Clarifying object structure #  Today we have resources that look like:\napiVersion: microsoft.storage.infra.azure.com/v1alpha1api20190401 kind: StorageAccount metadata: name: samplekubestorage namespace: default spec: azureName: mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS owner: name: k8sinfra-sample-rg accessTier: Hot tags: tag1: tag1 tag2: tag2 The problem #  There\u0026rsquo;s no good way with this object structure to differentiate stuff that is for Azure directly, versus stuff that is for the operator. owner almost falls into this category already, but there are other likely upcoming properties that definitely fall into this category:\n SecretConfiguration: details about where/how we should store secrets created by this entity. Credentials: per object credentials used to support CAPZ-like scenarios.  The problem also manifests in Status where ideally we would distinguish between properties from Azure directly (the result of a GET on the resource) and properties that we are presenting. For example, we may want to have a status field for deploymentId documenting the deployment ID used to create the resource, or the error if there was an error. If we do that there\u0026rsquo;s no easy way for the customer to understand that field is provided by ASO and not by Storage.\nProposal #  We introduce an additional level of hierarchy specifically to clarify what is coming or going directly from or to Azure. This is similar to what Crossplane does (see their MySQLServer). Unlike Crossplane though we will push the operator specific properties down a level and leave the Azure properties at the top level.\nOn the spec we could call this something like forOperator or operator, and on the status fromOperator or operator.\nOur structure would then look like:\napiVersion: microsoft.storage.infra.azure.com/v1alpha1api20190401 kind: StorageAccount metadata: name: samplekubestorage namespace: default spec: azureName: mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS accessTier: Hot tags: tag1: tag1 tag2: tag2 owner: name: k8sinfra-sample-rg operatorSpec: credentials: credentials Similarly, a status might look like:\nstatus: id: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS accessTier: Hot tags: tag1: tag1 tag2: tag2 operatorStatus: deploymentId: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/Microsoft.Deployments/deployments/1234 goalSeekingState: GoalMet FAQ #  Q: What about the owner field? A: It is such an important field and cuts across both Azure (since it\u0026rsquo;s partially about resource relationships in Azure) and Kubernetes (since it\u0026rsquo;s partially about resource relationships in Kubernetes) so it makes sense to leave it at the top level.\nQ: What about the azureName field? A: It is for Azure, so stays at the top level.\nQ: Do we actually have any properties that need to move TODAY because of this change? A: No, because state and deploymentId are currently resource annotations, not fields in the status, and on the spec the only fields which might belong in operatorSpec are owner and azureName which as per the above FAQ are staying at the top level.\nOpen questions #   What should we call the property which we push things down \u0026ldquo;into\u0026rdquo;? Here are some options:   operator (for both) operatorData (for both) forOperator / fromOperator operatorSpec / operatorStatus ??? / operatorState  "},{"id":12,"href":"/azure-service-operator/design/crossplane/","title":"Crossplane","section":"Design \u0026 Specifications","content":"Thoughts on performing provider-azure code generation for Crossplane #  The patterns identified below were extrapolated from the Crossplane VNET spec.\nGeneral notes #  Crossplane has the following static \u0026ldquo;special\u0026rdquo; properties #   ForProvider - this contains the actual \u0026ldquo;spec\u0026rdquo; as Azure would see it. DeletionPolicy ProviderConfigRef WriteConnectionSecretToRef  Since these are statically shaped they should be relatively easy to add with Crossplane specific generator pipeline stages.\nItems #2-4 are included automatically by embedding the runtimev1alpha1.ResourceSpec from runtimev1alpha1 \u0026quot;github.com/crossplane/crossplane-runtime/apis/core/v1alpha1\u0026quot;.\nCross resource references in Crossplane #   X (e.g. ResourceGroupName) - An actual field that exists in the Azure API. Can be set to any string you\u0026rsquo;d post to the Azure API. XRef (e.g. ResourceGroupNameRef) - A reference to a named Kubernetes object of a preordained kind. Used to resolve a value for X. XNameSelector (e.g. ResourceGroupNameSelector) - A selector used to set XRef.  Note that these values cascade down from the selector, to the ref, to the underlying field. If the ref is already set the selector is ignored, and if the field is already set the ref is ignored.\nUsually these are used to specify parentage for a resource (as in the case of the example above with ResourceGroupName) but may also be used for other cross resource references.\nGenerating these might take a bit of work as we need to derive the name(s) for each of these for each of the parent resources. This should be doable in a pipeline stage by walking up the graph of owners that we already have and getting their names, then generating a set of these for each owner.\nStatus #  Our status' include the \u0026ldquo;full\u0026rdquo; shape of the object, whereas Crossplane\u0026rsquo;s include the things which are not in the spec (basically the server-only fields). As pointed out by @negz, it\u0026rsquo;s a bit weird to duplicate data across Spec and Status like we\u0026rsquo;re currently doing so it may be worth us considering their approach. #269 is tracking this discussion.\nMissing features #  These are features the k8s-infra code generator is currently lacking that would be required to do a good job of generating the Crossplane CRDs.\n#266 Support for embedding structs/interfaces into generated types #  We don\u0026rsquo;t currently support embedding in the generated types, and we will need to in order to support embedding of runtimev1alpha1.ResourceSpec and runtimev1alpha1.ResourceStatus.\nSupport for additionalPrinterColumns in the CRD YAML #  Since these are likely custom per resource I am not sure if we want to generate them or just merge them in with hand-maintained Kustomize files.\n#267 Detection and removal of embedded subresources. #  Some of our CRDs aren\u0026rsquo;t as clear as they should be due to the service teams embedding subresources in the parent resource as properties. For example on VNET we have subnets and virtualNetworkPeerings properties that really probably shouldn\u0026rsquo;t be there (since those are their own sub-resources).\nImprovements in JSON schema/Swagger upstream for marking read-only properties #  This is minor as we can bypass properties using exclusions in the configuration file for now, but ideally we\u0026rsquo;d push this stuff up to the actual \u0026ldquo;source of truth\u0026rdquo; documents.\nAn example of this is: our VNET Spec has provisioningState in the spec which doesn\u0026rsquo;t make sense.\nA plan for secrets #  Right now we don\u0026rsquo;t have a plan for fields which are classified as secret. This will matter for some things such as Microsoft SQL Server. We do already have an issue tracking this at #154.\nSupport for resource references other than ownership #  We have a plan for supporting Cross resource references but don\u0026rsquo;t currently have the data to easily detect them. Ideally we could detect these and then turn them into our resource reference shape, or into Crossplanes.\n#268 Promote \u0026ldquo;Properties\u0026rdquo; Spec property #  This shows up on quite a large number of resources and adds an extra level of nesting that doesn\u0026rsquo;t look great.\nSpecific resources #  VNET #  Note: VNET in Crossplane is a v1alpha1 API and as such doesn\u0026rsquo;t follow all of their best practices (v1beta1 APIs do)\n As mentioned above, our VNET today has subnets and virtualNetworkPeerings, which are really subresources. It also has provisioningState which it shouldn\u0026rsquo;t. If we excluded these properties in the configuration file, we could definitely hack towards generating a VNET that looks like Crossplane\u0026rsquo;s now.  Redis #   Our version of Redis has a Properties field in the spec, which holds a bunch of properties. Some properties (Location, Owner, Tags) are peers of Properties. We should consider promoting the contents of Properties up a level into Spec, as mentioned above. There are a few trivial capitalization differences between our generated Redis and the one that is in Crossplane. For example: EnableNonSSLPort vs our EnableNonSslPort. Crossplane is correct here, but the difference only shows up in the generated code and so isn\u0026rsquo;t a big deal. We generate Enums for Redis.Sku fields Name and Family, whereas Crossplane just uses strings. Neither Crossplane nor we support SubnetId as a reference to a Kubernetes resource, but we probably should. (They have a TODO in the code). Crossplane makes use of a +immutable annotation, presumably to annotate that certain fields cannot be changed once they are set. We should look into doing that as well. #180 is tracking this. It\u0026rsquo;s not clear to me that this annotation actually does anything right now though. Crossplane is making use of a newer version of the Redis API than we see, because the 2018+ versions of the Redis API seem to have not made it into the deployment template we\u0026rsquo;re using as the source of truth. I filed #1237 in the schemas repro tracking this.  Microsoft SQL #   The strange thing with SQL is somehow some of their APIs don\u0026rsquo;t have the full API surface area\u0026hellip; For example I think 2019-06-01-preview exists but it\u0026rsquo;s not in the JSON schema we\u0026rsquo;re using. Other bits are split across API versions but then unioned together in their SDK: 2015-05-01-preview has servers, and 2017-03-01-preview has databases. Their Swagger is equally confusing. As mentioned above, the big issue here is going to be secrets/credentials since we don\u0026rsquo;t have an automated solution for that yet. Other than that, I think servers and databases (which are AFAIK the two interesting resources here) should be doable today.  Plugging into Crossplane framework #  One other area we need to investigate is how to write a generic adapter that takes standard ARM deployments and turns them into something that plugs into the Crossplane framework (CRUD). We might need some discussion with the Crossplane folks here because one thing with deployments is that they are actually their own resource, so a Create VNET operation would have to create a deployment that created the VNET. Once the deployment is done (resource in steady state), we\u0026rsquo;d want a way to conditionally delete the deployment (that doesn\u0026rsquo;t delete the VNET).\nFor those curious, we\u0026rsquo;ve got the scaffolding to do that in our generic controller in PR #250, you can see the workflow here.\n"},{"id":13,"href":"/azure-service-operator/design/defaulter-validator/","title":"Defaulter Validator","section":"Design \u0026 Specifications","content":"Custom validation and defaulting for code generated resources #  Reasoning #  controller-runtime defines admission.Defaulter and admission.Validator. These interfaces only give you a single Default or ValidateX method, which means that all validation/defaulting needs to be done in that method. I think we\u0026rsquo;re going to quickly run into situations where we want custom (handcrafted) validations or defaults for a particular resource and we\u0026rsquo;re not going to want to teach the code generator about these.\nSuggestion #  Strucutre our autogenerated webhooks such that there an ability to override the default behavior by implementing an interface.\nFor defaulting, implement this interface to hook into the autogenerated defaulting process:\ntype Defaulter interface { CustomDefault() } For validation, the interface being implemented should allow easy collation of errors, so we expand the controller-runtime Validator interface to return a collection of validation functions whose errors the autogenerated code can aggeregate:\ntype Validator interface { CreateValidations() []func() error UpdateValidations() []func(old runtime.Object) error DeleteValidations() []func() error } Sample Default:\n// +kubebuilder:webhook:path=/mutate-microsoft-storage-infra-azure-com-v1alpha1api20190401-storageaccount,mutating=true,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=microsoft.storage.infra.azure.com,resources=storageaccounts,verbs=create;update,versions=v1alpha1api20190401,name=default.v1alpha1api20190401.storageaccounts.microsoft.storage.infra.azure.com,admissionReviewVersions=v1beta1  var _ admission.Defaulter = \u0026amp;StorageAccount{} // Default defaults the Azure name of the resource to the Kubernetes name func (storageAccount *StorageAccount) Default() { storageAccount.default() var temp interface{} = storageAccount if runtimeDefaulter, ok := temp.(genruntime.Defaulter); ok { runtimeDefaulter.CustomDefault() } } func (storageAccount *StorageAccount) default() []func() { storageAccount.defaultAzureName() } func (storageAccount *StorageAccount) defaultAzureName() { if storageAccount.Spec.AzureName == \u0026#34;\u0026#34; { storageAccount.Spec.AzureName = storageAccount.Name } } Sample Validate:\n// +kubebuilder:webhook:path=/validate-microsoft-storage-infra-azure-com-v1alpha1api20190401-storageaccount,mutating=false,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=microsoft.storage.infra.azure.com,resources=storageaccounts,verbs=create;update,versions=v1alpha1api20190401,name=validate.v1alpha1api20190401.storageaccounts.microsoft.storage.infra.azure.com,admissionReviewVersions=v1beta1  var _ admission.Validator = \u0026amp;StorageAccount{} // ValidateCreate validates the creation of the resource func (storageAccount *StorageAccount) ValidateCreate() error { validations := storageAccount.createValidations() var temp interface{} = storageAccount if runtimeValidator, ok := temp.(genruntime.Validator); ok { validations = append(validations, runtimeValidator.CreateValidations()...) } var errs []error for _, validation := range validations { err := validation() if err != nil { errs = append(errs, err) } } return kerrors.NewAggregate(errs) } func (storageAccount *StorageAccount) createValidations() []func() error { return []func() error{ storageAccount.validateResourceReferences, } } func (storageAccount *StorageAccount) validateResourceReferences() error { refs, err := reflecthelpers.FindResourceReferences(\u0026amp;storageAccount.Spec) if err != nil { return err } return genruntime.ValidateResourceReferences(refs) } // \u0026lt;other Validator methods elided...\u0026gt; Open questions #   How awkward is var temp runtime.Object = storageAccount + if runtimeValidator, ok := temp.(genruntime.Validator); ok - is there a better way to do this?   Other possibilities #   Don\u0026rsquo;t use admission.Defaulter or admission.Validator, instead register N webhooks. This has significant downsides though because the Defaulter and Validator webhooks are automatically registered by our registerWebhook method in generic_controller (ctrl.NewWebhookManagedBy(mgr).For(obj).Complete())  "},{"id":14,"href":"/azure-service-operator/design/resource-states/","title":"Resource States","section":"Design \u0026 Specifications","content":"Proposal for reporting resource Status #  What Status are we talking about? #  There are two types of Status that we\u0026rsquo;re interested in understanding and reporting to the user when running an operator that creates resources in Azure:\n The Status of the operator and its actions on the resource. Has the resource successfully been reconciled? Is the operator in the progress of driving the resource to its goal state (defined in the spec), or has it already done so and is now waiting for more changes before taking further action? The State of the resource in Azure. What fields are set in Azure? There are often defaults and other values which, while you may not have specified them in the spec have been applied on the server side and we want to show to you.  This document is focused on the first type of Status defined above.\nCurrent state of ASO #  ASO v1 (handcrafted) #  The handcrafted ASO resources have a 3-tuple on the Status field which describes the state of the resource:\nProvisioning bool `json:\u0026#34;provisioning,omitempty\u0026#34;` Provisioned bool `json:\u0026#34;provisioned,omitempty\u0026#34;` FailedProvisioning bool `json:\u0026#34;failedProvisioning,omitempty\u0026#34;` There are also additional fields for conveying additional information, including the details of any error as well as information about the Resource ID of the resource\nState string `json:\u0026#34;state,omitempty\u0026#34;` Message string `json:\u0026#34;message,omitempty\u0026#34;` ResourceId string `json:\u0026#34;resourceId,omitempty\u0026#34;` Problems with this approach #   Provisioning, Provisioned and FailedProvisioning are mutually exclusive but the structure of the status doesn\u0026rsquo;t convey that well. Additionally the fact that there are multiple fields means it\u0026rsquo;s easier for bugs to creep in where we set one but didn\u0026rsquo;t clear another. The State field is not widely used by all resources, and seems to conflict with Provisioning/Provisioned/FailedProvisioning. It\u0026rsquo;s not clear when you should look at one or the other.  ASO v2 (auto-generated) #  The auto-generated resources don\u0026rsquo;t currently have anything in their Status which conveys the Status of the operator. For the initial alpha releases, these fields are instead set as annotations on the object:\nDeploymentIDAnnotation = \u0026#34;deployment-id.infra.azure.com\u0026#34; DeploymentNameAnnotation = \u0026#34;deployment-name.infra.azure.com\u0026#34; ResourceStateAnnotation = \u0026#34;resource-state.infra.azure.com\u0026#34; ResourceErrorAnnotation = \u0026#34;resource-error.infra.azure.com\u0026#34; This was done mostly to avoid the additional work to code-generate the correct fields in each resources status and set them. Since it\u0026rsquo;s possible that the whole status can get lost, some of the things may need to stay as annotations (deploymentId possibly), while others are obviously status and can be derived again if lost. We took the temporary approach of putting it all in annotations while we learned more about what the best practices were.\nProblems with this approach #   The ResourceStateAnnotation is reusing the same enum to describe state as the armclient uses for deployments. This is problematic because that only supports Succeeded, Failed, Deleting, and Accepted. Succeeded, Failed, and Deleting make sense in the context of the operator but Accepted doesn\u0026rsquo;t really. There\u0026rsquo;s no state for InProgress or Reconciling. There\u0026rsquo;s an awkward tension between sharing the state of the ARM deployment and the state of the resource. They\u0026rsquo;re often the same but sometimes not (such as in the case the deployment is being deleted but the resource is not). Using annotations is awkward because changing an annotation triggers another reconcile. Using annotations means that it\u0026rsquo;s not clear to the user what the possible valid states are for the fields that are enums. Users expect to look in Status for status related fields. Having it exposed as an annotation was never intended as a long term solution, just a hack so that we could get things up and running.  Examining other projects like ASO #  A quick look across the field of projects similar to ASO suggests that there is a relatively standard approach to solving this problem:\n Crossplane reports status through a Ready condition ACK reports status through a variety of conditions, including ACK.Adopted, ACK.resourceSynced, ACK.Terminal, etc. Cluster API originally used a phase and failureReason/failureMessage pattern, but has since moved to use conditions and is deprecating the old pattern. Pod uses a combination of phase and conditions, including PodScheduled, ContainersReady, Initialized, and Ready.  It seems that most of these (and many core Kubernetes resources) use a Ready condition to specify the status.\nMore on conditions #  These best practices were gathered from conditions API conventions as of July 2021.\n Some resources in the v1 API contain fields called phase, and associated message, reason, and other status fields. The pattern of using phase is deprecated. Newer API types should use conditions instead. Phase was essentially a state-machine enumeration field, that contradicted system-design principles and hampered evolution, since adding new enum values breaks backward compatibility. Rather than encouraging clients to infer implicit properties from phases, we prefer to explicitly expose the individual conditions that clients need to monitor. Conditions also have the benefit that it is possible to create some conditions with uniform meaning across all resource types, while still exposing others that are unique to specific resource types. See #7856 for more details and discussion.\n  In condition types, and everywhere else they appear in the API, Reason is intended to be a one-word, CamelCase representation of the category of cause of the current status, and Message is intended to be a human-readable phrase or sentence, which may contain specific details of the individual occurrence. Reason is intended to be used in concise output, such as one-line kubectl get output, and in summarizing occurrences of causes, whereas Message is intended to be presented to users in detailed status explanations, such as kubectl describe output.\n  In general, condition values may change back and forth, but some condition transitions may be monotonic, depending on the resource and condition type. However, conditions are observations and not, themselves, state machines, nor do we define comprehensive state machines for objects, nor behaviors associated with state transitions. The system is level-based rather than edge-triggered, and should assume an Open World.\n  Controllers should apply their conditions to a resource the first time they visit the resource, even if the status is Unknown. This allows other components in the system to know that the condition exists and the controller is making progress on reconciling that resource.\n These from standardizing conditions:\n reason is required and must not be empty. The actor setting the value should always describe why the condition is the way it is, even if that value is \u0026ldquo;unknown unknowns\u0026rdquo;. No other actor has the information to make a better choice.\n Some other best practices on Kubernetes design principles:\n Object status must be 100% reconstructable by observation. Any history kept must be just an optimization and not required for correct operation.\n  Do not define comprehensive state machines for objects with behaviors associated with state transitions and/or \u0026ldquo;assumed\u0026rdquo; states that cannot be ascertained by observation.\n Proposal #  Goals #   The status of the operator should be reported as a subsection in the Status of the resource itself. See #1504. It should be clear to users where to look to determine what the current status of their resource is. The status should include information in both normal and failure cases. When a failure occurs, it should be clear to the user that a failure has happened and what the cause of the failure was. When a failure occurs, there should be an error or code that is programmatically consumable (for automation, etc). When a failure occurs, there should be an error or text that is human consumable and ideally more informative than the programmatically consumable error code. When a failure occurs, it should be clear to the user whether that failure is transient and the operator can self-recover.  Details #  All resource\u0026rsquo;s Status\u0026rsquo;s will have a top level Conditions field which is a collection of type Condition. This collection supports extension (through conditions with different type names). Initially we will expose a single Ready condition across all resources, representing the high level availability of the resource and its readiness for use.\nThe structure of the Condition is based on the recommended shape of conditions KEP as well as the Cluster API conditions proposal and is as follows:\n// ConditionSeverity expresses the severity of a Condition. type ConditionSeverity string const ( // ConditionSeverityError specifies that a failure of a condition type  // should be viewed as an error. Errors are fatal to reconciliation and  // mean that the user must take some action to resolve  // the problem before reconciliation will be attempted again.  ConditionSeverityError ConditionSeverity = \u0026#34;Error\u0026#34; // ConditionSeverityWarning specifies that a failure of a condition type  // should be viewed as a warning. Warnings are informational. The operator  // may be able to retry through the warning without any action from the user, but  // in some cases user action to resolve the warning will be required.  ConditionSeverityWarning ConditionSeverity = \u0026#34;Warning\u0026#34; // ConditionSeverityInfo specifies that a failure of a condition type  // should be viewed as purely informational. Things are working.  // This is the happy path.  ConditionSeverityInfo ConditionSeverity = \u0026#34;Info\u0026#34; // ConditionSeverityNone specifies that there is no condition severity.  // For conditions which have positive polarity (Status == True is their normal/healthy state), this will set when Status == True  // For conditions which have negative polarity (Status == False is their normal/healthy state), this will be set when Status == False.  // Conditions in Status == Unknown always have a severity of None as well.  // This is the default state for conditions.  ConditionSeverityNone ConditionSeverity = \u0026#34;\u0026#34; ) type ConditionType string const ( ConditionTypeReady = \u0026#34;Ready\u0026#34; ) // Condition defines an extension to status (an observation) of a resource type Condition struct { // Type of condition.  // +kubebuilder:validation:Required  Type ConditionType `json:\u0026#34;type\u0026#34;` // Status of the condition, one of True, False, or Unknown.  // +kubebuilder:validation:Required  Status metav1.ConditionStatus `json:\u0026#34;status\u0026#34;` // Severity with which to treat failures of this type of condition.  // For conditions which have positive polarity (Status == True is their normal/healthy state), this will be omitted when Status == True  // For conditions which have negative polarity (Status == False is their normal/healthy state), this will be omitted when Status == False.  // This is omitted in all cases when Status == Unknown  // +kubebuilder:validation:Optional  Severity ConditionSeverity `json:\u0026#34;severity,omitempty\u0026#34;` // LastTransitionTime is the last time the condition changed.  // +kubebuilder:validation:Required  LastTransitionTime metav1.Time `json:\u0026#34;lastTransitionTime\u0026#34;` // Reason for the condition\u0026#39;s last transition.  // Reasons are upper CamelCase (PascalCase) with no spaces. A reason is always provided, this field will not be empty.  // +kubebuilder:validation:Required  Reason string `json:\u0026#34;reason\u0026#34;` // Message is a human readable message indicating details about the transition. This field may be empty.  // +kubebuilder:validation:Optional  Message string `json:\u0026#34;message,omitempty\u0026#34;` } The Ready condition #  The Condition definition above discusses support for arbitrary conditions. In practice at least for now, ASO will only expose a single Ready condition. The addition of a Severity field inspired by Cluster API allows different combinations of Severity and Reason to combine together and describe complex scenarios.\nThe Reason field #  Inside of the Ready condition we always include a Reason which provides a programmatically consumable reason that the resource is in the given state. Reason is derived from multiple sources depending on where in the resource lifecycle we are. The value in Reason may be set by the operator itself, or be set to the result of an error or response code received from Azure.\nThe operator defines the following reasons:\n   Reason Severity Meaning     Reconciling Info A request has been submitted to Azure. The operator may be waiting for a response from Azure.   WaitingForOwner Warning The owner of this resource cannot be found in Kubernetes. Progress is blocked until the owner is created.   Deleting Info The resource is being deleted.   Succeeded None (\u0026quot;\u0026quot;/empty) The spec has successfully been applied. No additional changes are being attempted at this time.    There are no failure conditions (Severity = Error) specified here because currently all fatal errors come directly from Azure. When an error response is received from Azure, the Code from Azure is set as the Reason, and the Message from Azure is set as the Message.\nSeverity meaning in the context of the Ready condition #   Error means that we were unable to reconcile the resource successfully. A Ready condition with Status=False, Reason=\u0026lt;something\u0026gt;, Severity=Error is no longer attempting to be reconciled. The user must make an update to the resource to fix one or more configuration problems. Warning means that something is wrong, but we haven\u0026rsquo;t given up. The resource will continue to be reconciled until we can progress past the cause of the Warning. Users should examine the Warning as some may be due to the operator waiting for action from them, as in the case of Reason=WaitingForOwner, Severity=Warning. Others may be due to transient unavailability in Azure, as in the case of Reason=InternalServerError, Severity=Warning. Info means that everything is proceeded as expected. This is the \u0026ldquo;happy path\u0026rdquo;.  Reference #   Article on conditions and status reporting in Kubernetes. Note that this article is a bit old as Cluster API uses conditions now, but it gives a great overview of the topic. Recommended shape of Conditions Update condition guidance - and the actual guidance itself is here More condition recommendations  Open questions #   There are some small differences between this proposal and what CAPI is doing - are we ok with these differences?  CAPI says Reason is optional, but we\u0026rsquo;re making it required.    Open questions answered #  Open questions which have since been answered are below.\nQuestion: Will this work with kstatus, which recommends negative polarity conditions? Answer: Yes, kstatus supports the Ready condition as well, with a few caveats. Anecdotally, we feel that positive polarity conditions (like Ready) are more clear. As mentioned above, there are many operators that follow this Ready pattern including Crossplane and CAPI. If need-be, we can work around the major problem with kstatus and Ready by providing a webhook that automatically includes it on all resource creations.\nQuestion: The KEP for Condition says that LastTransitionTime should be updated any time the condition changes. The CAPI proposal says it should change when Status changes, but the actual CAPI implementation changes it any time the Condition changes.Which behavior do we want? Answer: We will follow the KEPs definition (and CAPI\u0026rsquo;s actual implementation) and update LastTransitionTime any time a Condition changes, even if that change is from Status=False to Status=False.\nAn aside on ARM templates #  This isn\u0026rsquo;t related to the core topic, but it might be useful to understand because it has a big impact on where failure is possible when communicating with ARM.\nYou can read some guidelines about how ARM templates work here.\nThe process basically boils down to the following and is documented here:\n ARM: Validate basic template syntax. Return an error for invalid JSON, invalid resource ID structure, etc. ARM: Send the resource to the individual resource provider (RP). RP: Return a response for the resource. This can be:  201 or 200 HTTP status code and a response body. At this point the resource should exist, but with possibly with a non-terminal provisioningState. 202 HTTP status code. At this point the resource should not exist, and will not exist until the long-running operation completes successfully.   ARM: Poll long running operation until it completes (if needed). ARM: Deployment status will be the status of the resource at the end of the long running operation, which should be in one of the 3 terminal states Succeeded, Failed, or Canceled  Possible outcomes:\n   Status of deployment State of resource     4xx HTTP error code when creating deployment Resource does not exist   Deployment accepted, but fails after polling Resource may or may not exist (depends on the RP)   Deployment succeeds Resource has Failed provisioningState - The spec doesn\u0026rsquo;t say this is impossible but it\u0026rsquo;s probably very rare   Deployment succeeds Resource has Succeeded provisioningState    Note: When a deployment is accepted, the underlying resource may or may not have been created. This means we have to handle cases where the deployment failed but the resource was created anyway alongside cases where the deployment failed and no resource was created.\n"},{"id":15,"href":"/azure-service-operator/design/secrets/","title":"Secrets","section":"Design \u0026 Specifications","content":"Azure Service Operator support for managing data plane secrets #  What secrets are we talking about? #  The secrets discussed in this document are associated with accessing the data plane of various services.\nThink: Accessing a StorageAccount via Shared Key, accessing a MySQLServer by admin Username and Password, or accessing a VM by SSHKey.\nSometimes these secrets may be used by other CRDs managed by ASO, as would be the case for a MySQLUser CRD, but often the consumers of these secrets are the users applications.\nGoals #  ASO should not be generating secrets on the users behalf #  ASO v1 generated secrets on the users behalf in some cases. The users had access to the generated secret as ASO wrote it into a Kubernetes (or KeyVault) secret.\nThere are problems with this approach:\n It makes performing secret rollover more difficult. If the user had specified the secret, rollover is as easy as modifying the specified secret, which triggers a reconcile, which triggers a PUT to the Azure Resource updating the secret. If the operator generated the secret then the user must somehow issue an instruction to us to perform an action to roll the secret, which doesn\u0026rsquo;t fit as well into the goal-seeking paradigm Kubernetes prefers. It takes control away from the user. What if the secret the operator generates doesn\u0026rsquo;t comply with a particular organizations complexity requirements? It requires that we do a very good job generating cryptographically secure passwords/secrets. This could easily become a can of worms. It hinders adoption of existing resources. If the operator expects to always generate the secret for a SQL DB, but the user wants to import a SQL DB they\u0026rsquo;ve already created through some other mechanism, then they are at an impasse as there is no (easy) way for them to provide ASO with the secret. It doesn\u0026rsquo;t work well with GitOps. A number of customers have expressed the desire to move resources between namespaces. In theory this is easy - just create the exact same resources in a different namespace (pointing at the same Azure resources), mark the old resources as skip-deletion (so that the backing Azure resources are not deleted), and delete the original namespace. If the definition of the secret isn\u0026rsquo;t part of the users GitOps flow this becomes more difficult as the first namespace has the secret (which was created by ASO) and that secret must be cloned manually to the second namespace, otherwise the creation of the resource in the second namespace will attempt to generate a new secret.  We could investigate allowing both automatically generated secrets (the default) and user specified secrets (opt-in). That avenue results in the operator shouldering all of the complexity burden of both though.\nGiven ASO\u0026rsquo;s approach as a low level toolkit providing the ability to create Azure resources, at least at this time we should avoid the added complexity of generating user secrets.\nPrefer Managed Identity #  Managing secrets is hard. We should prefer AAD Authentication and Managed Identity where possible.\nWhat this means in practice is that secrets should not be retrieved unless the user has specifically asked for them to be.\nThis has a number of advantages:\n If the user is using Managed Identity, they don\u0026rsquo;t have a secret leak waiting to happen created in their namespace which they didn\u0026rsquo;t ask about (and might not even know about). We can make a failing ListKeys (or equivalent) call fatal. Many services support ways lock down or forbid access to the ListKeys (or equivalent) API. This allows users to block access if they would like to require AAD authentication with their resource. As long as the operator is not calling those APIs unless asked (in the Spec of the resource) we can make their failure fatal. This gives users an easy way to drive towards full AAD usage in ASO and elsewhere.  We\u0026rsquo;ll dig more into how we might accomplish this a bit later.\nWork well with GitOps #  Resources should strive to play well with GitOps. This means that when a new resource is deployed it is capable of adopting an already existing ARM resource if one exists. This also must apply to secrets. As mentioned above one of the reasons that we do not want to be generating secrets on the users behalf is that it makes redeploy and resource adoption hard because the user has no configuration representing the generated secret.\nKinds of secrets #  There are two main types of secrets we need to consider, differing primarily by the origin of the secret.\nUser provided secrets: Secrets provided by the user at resource creation time.\nAzure generated secrets: Secrets created by Azure, and returned by a special GetMeTheSecrets API call.\nSample resources that have secrets #  Below is a table containing a sampling of resources with secrets that ASO already supports or has a plan to support in the near future.\n   CRD User provided secrets Azure generated secrets AAD/Managed Identity Support Notes     VirtualMachineScaleSet ✔️ ❌ ❌ Username and Password. Can be modified by subsequent PUT.   VirtualMachine ✔️ ❌ ❌ Username and Password. Can be modified by subsequent PUT.   PostgreSQL FlexibleServer ✔️ ❌ ✔️ AdministratorLogin and AdministratorLoginPassword. Must have even if using AAD. Can be modified by subsequent PUT.   MySQL FlexibleServer ✔️ ❌ ✔️ AdministratorLogin and AdministratorLoginPassword. Must have even if using AAD. Can be modified by subsequent PUT.   StorageAccount ❌ ✔️ ✔️ List Keys API and Regenerate Keys API. AAD+RBAC (blob/table only?) Authorizing Access with Active Directory.   CosmosDB DatabaseAccount ❌ ✔️ ✔️ List Keys API, List Read Only Keys and Regenerate Key API. For AAD+RBAC (supported by SQL only?), see Disabling Local Auth, Create Role Assignment API, Create Role Definition API. Built-in Role Definitions.   EventHubAuthorizationRules ❌ ✔️ ❌ List Keys API. There are default authorization rules created, such as RootManageSharedAccessKey. Supports regeneration.   Redis ❌ ✔️ ❌ List Keys API. Regenerate Key API.     Other kinds of secrets in Azure: #  There are a few other types of secrets in Azure in addition to the two main ones discussed above.\n Get once Azure generated secrets: These are secrets created by Azure and only returned once, usually as the response to a POST. These don\u0026rsquo;t fit cleanly into the table above because they are a POST action on a parent resource these are not in fact resources themselves.  Application Insights Component APIKey: This is a POST to the Component/ApiKey URL. KeyVault Key: Create Key API.   \u0026ldquo;Secrets\u0026rdquo; created by Azure, and returned in the GET: In almost all cases (all that I have seen), a secret in Azure is not returned by a GET. \u0026ldquo;Secrets\u0026rdquo; returned in a GET are not really secrets per-se, but ASOv1 classifies them as secrets.  ApplicationInsights InstrumentationKey or ConnectionString? See here.   Short-lived \u0026ldquo;tokens\u0026rdquo;  StorageAccount SAS CosmosDB ResourceToken.    Other Operators #  A quick look at what other operators are doing with regard to secrets.\nCrossplane #  Azure generated secrets: A Connection secret stores information needed to connect to the resource, including keys generated by Azure (see for example Storage Account). This is a standard pattern used across all of the providers.\nThe destination of the writeConnectionSecretToRef is currently always a Kubernetes secret, but there is an open issue requesting pluggable secret stores.\nUser provided secrets seem to be automatically generated by Crossplane.\nAWS Controller for Kubernetes (ACK) #  User provided secrets are provided via a SecretKeyRef. This allows cross-namespace references to a secret. The specific Key of the secret is selected with a Key string parameter.\nIt looks like there may not currently be support for key updates/rollover.\nAWS generated secrets I can\u0026rsquo;t find any examples of. They do not seem to classify \u0026ldquo;endpoints\u0026rdquo; as a secret, as shown by opensearchservice endpoint and cluster endpoint.\nProposal #  User specified secrets #  User specified secrets will be detected and transformed from string to a SecretReference:\ntype SecretReference struct { // Name is the name of the secret. The secret must be in the same namespace as the resource.  Name string // Key is the key in the secret to use.  Key string } Detection will be done with a combination of:\n Additions to ASO\u0026rsquo;s configuration file to flag particular fields as secret (probably in the ObjectModelConfiguration section). Using the \u0026quot;format\u0026quot;: \u0026quot;password\u0026quot; data from Swagger, such as that used by MySQL Flexible Server. Note that not all specs have this, for example VMSS does not. Using the x-ms-secret annotation, which I assume has the same meaning as \u0026quot;format\u0026quot;: \u0026quot;password\u0026quot; although it\u0026rsquo;s not actually exactly documented anywhere.  This is a place where we can push changes upstream to flag things as passwords if they\u0026rsquo;re not being flagged.\nLifecycle #  Since these secrets are created by the user, the user owns the lifecycle of these secrets. They could be using the same secret across many resources, or intending to use this secret on a resource they have not created yet. As such, the lifecycle of these secrets must be controlled by the user.\nRollover will be supported by triggering events on the associated resource when the secret is modified. Since multiple custom resources might be using the same secret, this could trigger reconciles on multiple resources. An existing pattern has been established for this in mysqlserver_controller.go of ASO v1.\nWhen secrets are rolled over, there is a risk that applications using the secret will fail because the secret they are using is no longer valid. We don\u0026rsquo;t need to worry about coordination or timing though. If the user asks us to update the password (by changing it in the Kubernetes secret), we should just do it. We aren\u0026rsquo;t going to be able to guarantee 100% uptime unless the service in question supports multiple keys/passwords/secrets. If the service does support that, we should be able to do what we\u0026rsquo;ve designed here and we will automatically get those same 100% uptime guarantees for the services that support it.\nOpen questions #  Should we support KeyVault inputs?\nKubernetes has a proposal out for unions. In this proposal they suggest using a discriminator field, but allow for either of the shapes proposed above.\nOn the discriminator, they say:\n The value of the discriminator is going to be set automatically by the apiserver when a new field is changed in the union. It will be set to the value of the fields-to-discriminateBy for that specific field. When the value of the discriminator is explicitly changed by the client, it will be interpreted as an intention to clear all the other fields. See section below.\n See more details about what they see the discriminator doing in normalizing on updates.\nSince their proposal suggests that discriminator is optional, and as far as I know it is not supported by kubebuilder yet, I suggest we don\u0026rsquo;t add one for now. In any case it\u0026rsquo;s not totally clear to me that we really need the value that it is adding, and it seems to add a significant amount of update complexity.\nWe need to decide which of the above shapes we like more, and also if we do or do not want a secretType discriminator.\nConclusion: Not needed for v1 of the feature, but we need to have a shape ready that makes sense for when we do.\n... secret: type: Secret # Or KeyVault name: foo key: bar keyVaultReference: # This is the standard Reference type we use elsewhere armId: ... Is an AccountName a secret?\nIf the service is returning it in the resource GET, then strictly speaking it is not a secret. Since the field isn\u0026rsquo;t secret, we will not transform it to be a SecretReference. This is for two reasons:\n We won\u0026rsquo;t be able to automate it, since as far as the OpenAPI specification is concerned it isn\u0026rsquo;t a secret. The primary reason to classify this as a secret would be so that users could then inject the value from their secret into a pod. There is a workaround for this though since the user could (using Kustomize or similar) do this already.  Conclusion: We will not classify these as a secret for inputs unless there\u0026rsquo;s some requirement forcing us to do so.\nDo we allow reading a secret from a namespace where the resource isn\u0026rsquo;t?\nThere were some requests for this in ASOv1, see this for example.\nThis has security implications, so initially at least the answer should be no. See: https://github.com/kubernetes/community/pull/5455.\nConclusion: No\nWhat\u0026rsquo;s the difference between x-ms-secret and format: password?\nUnclear currently. I have a question out to the Swagger team. Current plan is to just treat them the same.\nConclusion: They are not the same, x-ms-secret implies that the secret is not returned in the GET, whereas format: password is just an annotation that as far as we can tell is not actually consumed by anything. For our case, we can treat them the same.\nAzure generated secrets #  Azure generated secrets will be optionally downloaded to a Kubernetes or KeyVault secret. Users instruct the operator to download the secrets associated with a resource by supplying a SecretDestination in the Spec.\nThe optionality of this step is key for preferring managed identity, as it allows those in AAD/Managed identity cases to avoid secrets they don\u0026rsquo;t want/won\u0026rsquo;t use being retrieved into their namespace.\n// Note: This is the same type that is used for specifying references to user created secrets/ // reproduced here for clarity type SecretReference struct { Name string Key string } // Using a different type for the destination as there are some things that are destination specific (such as adding annotations or labels, // which don\u0026#39;t make sense on ) type SecretDestination struct { SecretReference } Example: Explicit secrets w/ Azure Storage (Simple) #  spec: # Other spec fields elided... operatorSpec: secrets: primaryKey: name: my-secret key: PRIMARY_KEY secondaryKey: name: my-secret key: SECONDARY_KEY endpoint: name: my-secret key: ENDPOINT Example: Explicit secrets w/ CosmosDB and multiple secret destinations (Complex) #  Here is what a more complex resource might look like if we also supported KeyVault as a secret type.\nNote: We are not planning to support KeyVault initially.\nspec: # Other spec fields elided... operatorSpec: secrets: primaryKey: type: KeyVault reference: armId: /subscriptions/.../resourceGroups/.../providers/Microsoft.KeyVault/vaults/asokeyvault name: my-primary-key secondaryKey: type: KeyVault reference: armId: /subscriptions/.../resourceGroups/.../providers/Microsoft.KeyVault/vaults/asokeyvault name: my-secondary-key readOnlyPrimaryKey: type: Secret name: my-readonly-secret key: PRIMARY_KEY readOnlySecondaryKey: type: Secret name: my-readonly-secret key: SECONDARY_KEY endpoint: type: Secret name: my-secret key: ENDPOINT Note that some resources (like CosmosDB DatabaseAccount) have multiple kinds of secrets. There might be a PrimaryKey, SecondaryKey, ReadOnlyPrimaryKey, and ReadOnlySecondaryKey. At the very least we need to support putting the main keys and the readonly keys into two different secrets. To accomplish this, we can define multiple logical secret groupings in the ASO config and a corresponding destination property will be created for each of them. You can see this done above in the sample.\nSome fields such as Endpoint (or AccountName for other databases) are not really secret, but should be included in these logical secret groups anyway so that they\u0026rsquo;re easier for users to inject into pods.\nAzure generated secrets are more difficult to automatically detect. Often there is a ListKeys or GetKey API for the resource in question, but nothing on the resource itself indicates that it has secrets automatically generated by Azure. For resources like these we will add a flag in the ASO configuration to generate the appropriate structures in the resource. In the future we can investigate detecting this from the Swagger (probably after we move to Swagger as the single source of truth) by introspecting \u0026ldquo;other API calls\u0026rdquo; on the resource in question.\nHooks required #  In addition to the configuration required to generate types with the right shape, we will also need a way to hook into the reconcile process and actually make the right ListKeys or GetKeys call. To support rollover we would need a different hook as well.\nThis is a relatively involved topic so not designing it all here. As a starting point, resources manually implementing the following interface would get us what we need. Issue #1978 is tracking this request in more detail.\nSee also the design of reconciler extensions.\ntype ARMDetails struct { Endpoint string SubscriptionID string Creds azcore.TokenCredential HttpClient *http.Client } type ReconcileDetails struct { log logr.Logger recorder record.EventRecorder ARMDetails *ARMDetails KubeClient *kubeclient.Client ResourceResolver *genruntime.Resolver PositiveConditions *conditions.PositiveConditionBuilder } type BeforeReconcileOverride interface { // BeforeCreateOrUpdate runs before sending the resource to Azure  BeforeCreateOrUpdate(ctx context.Context, details *ReconcileDetails) (ctrl.Result, error) } type AfterReconcileOverride interface { // AfterCreateOrUpdate runs after the resource has been sent to Azure and finished creating, either successfully or with a fatal error.  // You can determine the current state by examining the Ready condition on obj.  AfterCreateOrUpdate(ctx context.Context, details *ReconcileDetails) (ctrl.Result, error) } Resources with Azure generated secrets would manually implement AfterCreateOrUpdate, which would then:\n Check that the resource is in a Ready state. Cast the provided obj to the expected type. This should be guaranteed safe because the method won\u0026rsquo;t have been called unless its implemented. Check the forOperator section of the spec to determine if any keys should be written. This includes determining the destination secret name and any annotations or other properties that should be written or updated on that secret. Also includes ensuring that the secret in question (if it exists) is owned by the resource in question. Make the required GetKeys or ListKeys call to Azure. Create or update the secret.  Lifecycle #  These are secrets created by the operator (usually after calling some ListKeys type API). Since the these secrets are by definition specific to the resource that created them, their ownership in Kubernetes will be set to the resource that created them. When the owning resource is deleted, the created secret will also be deleted.\nOpen questions #  Should Endpoint/AccountName type \u0026ldquo;secrets\u0026rdquo; really be put into the secret, or no?\nPutting them into the secret makes injecting them into pods easier, which is what people are going to want to do with these values. ASOv1 classifies all of these sorts of things as secrets.\nI think we should put these into the generated secrets\u0026hellip; although this does somewhat conflict with my stance on user specified accountName, etc where I had said to not classify them as secrets.\nConclusion: Yes, we will put them into the secret.\nHow does key rollover work for these types of secrets?\nWe don\u0026rsquo;t support this at all initially. This is a somewhat advanced scenario that doesn\u0026rsquo;t fit well into the Kubernetes resource model anyway because rollover is more of an action and less of an actual resource. The user can roll their secrets using the az cli (or other tooling) and then either wait for a reconcile to occur naturally or force one to refresh the secrets locally in the cluster.\nWhen we do decide to support this, we should be able to do so as a Job-esque resource that runs to completion (and somehow triggers a re-reconcile on the parent resource type).\nConclusion: This will not be supported in the initial implementation.\nHow do deal with soft-delete and purge protection?\nIf we support KeyVault, we have to deal with the soft-delete + purge awkwardness that deleting and recreating brings. Some of this we dodge by giving control to the user and expecting them to provide us with the name of a secret that\u0026rsquo;s going to work. We may also need a flag per-secret (or global at the operator level?) for if we should purge the secret when we delete. Something like purgeOnDelete.\nThe main downside of a global flag controlling the behavior of purge or delete is that it doesn\u0026rsquo;t give fine-grain control, yet we are proposing to give fine-grained control over what vault to put secrets in. This seems like a discrepancy. This seems to suggest we just put this option on every KeyVault reference:\nspec: # Other spec fields elided... operatorSpec: secrets: # Save the read-write keys (and endpoints) into a kubernetes secret called \u0026#34;my-secret\u0026#34; and a KeyVault secret called \u0026#34;my-secret\u0026#34;. keyDestination: keyVault: reference: armId: /subscriptions/.../resourceGroups/.../providers/Microsoft.KeyVault/vaults/asokeyvault name: my-secret purge: false # Optional: defaults to false delete: false # Optional: defaults to true Conclusion: When we add KeyVault support, we will add the option to avoid deletion/purging of secrets.\nHow do we ensure that we aren\u0026rsquo;t overwriting secrets that we don\u0026rsquo;t own?\nThe operator must support updating the secret as keys can change or be rolled over. On the other hand we should present a clear error if the user has accidentally pointed two resources at the same secret. The first resource should be unaffected and work normally while the second resource should encounter a reconcile error stating that the secret in question already exists.\nThis applies to either Kubernetes or KeyVault secrets.\nProposed solution for Kubernetes secrets is to issue a GET and check the Owner field for Kubernetes secrets before issuing an update. resourceVersion should ensure that we\u0026rsquo;re protected from any races where some external entity deletes the secret and another resource creates it between our GET and PUT.\nProposed solution for KeyVault secrets is to label the secret with a key uniquely identifying the operator and resource (GVK + namespace + name) which it corresponds to. The operator will then issue a GET prior to attempting to update the secret to ensure that it owns the secret. Unfortunately, as far as I can tell KeyVault secrets don\u0026rsquo;t support Etag so there\u0026rsquo;s a possible data race here that we can\u0026rsquo;t avoid\u0026hellip;\nConclusion: We will use the above design to ensure we\u0026rsquo;re not overwriting secrets.\nWhat happens when a resource is updated to remove the secrets entry?\nIf a resource is created and given a forOperator.secrets field that instructs it to create a secret, and then is updated to remove this secrets entry, should we delete the secret we previously created? It feels like we should, but when the operator is presented with the updated object during that second reconcile it doesn\u0026rsquo;t know that it ever had a secret.\nSome possible solutions:\n We just don\u0026rsquo;t delete the secrets when you do this and expect users to do it themselves if they want things cleaned up. For k8s secrets, we could probably look through all of the secrets in the resources namespace and see if any of them are owned by us. If there are some and we don\u0026rsquo;t have any forOperator.secrets we delete them. The downside here is that\u0026rsquo;s a lot of overhead to do for all reconciles. This also assumes that the operator has Secret Read permissions in the namespaces in question - which it\u0026rsquo;s possible that it doesn\u0026rsquo;t (it may never have created any secrets and the users may know that it won\u0026rsquo;t and so have denied it permission). This also obviously doesn\u0026rsquo;t work for KeyVault secrets as we have no idea which KeyVault the secrets might even be in. When we create a secret we store information about it in Status or Annotations and then use that information to clean up after ourselves. It\u0026rsquo;s a bit cleaner to store it in Status but technically that can be lost, whereas Annotations won\u0026rsquo;t be. This should work for both KeyVault and k8s secrets and while it\u0026rsquo;s a bit icky I think gives the best experience.  Conclusion: We will do nothing, the secret will remain. If this becomes a problem for users we can add support in the future for an annotation on the resource that says azureGeneratedSecretBehavior: DeleteIfNotSpecified. We feel it\u0026rsquo;s unlikely people are going to complain though, and it errs on the side of caution for deleting secrets that may be critical to the users application operation. Note that secrets WILL be deleted when you delete the actual resource (ex: CosmosDB).\nShould we drop the Destination suffix in the field names?\nFor example, the proposal for CosmosDB DatabaseAccount was:\nspec: # Other spec fields elided... forOperator: operatorSpec: keyDestination: # stuff elided... readOnlyKeyDestination: # stuff elided... Without the Destination suffix that would be:\nspec: # Other spec fields elided... operatorSpec: secrets: key: # stuff elided... readOnlyKey: # stuff elided... Conclusion: Yes, drop it\nDo we allow writing a secret to a namespace where the resource isn\u0026rsquo;t?\nConclusion: No, see: https://github.com/kubernetes/community/pull/5455\nDo we support writing the same secret to multiple destinations?\nInitially we had decided to not support this, and instead just write each secret to a maximum of 1 destination. This has one unfortunate side effect. It\u0026rsquo;s tricky for customers to break apart different \u0026ldquo;parts\u0026rdquo; of the secret, such as the primary and secondary keys, into different secrets as the endpoint can only be written to one of them.\nThere is a workaround for this though: when users want to split their primary/secondary keys into different secrets, they can write the endpoint to its own secret as well:\nspec: # Other spec fields elided... operatorSpec: primaryKey: type: Secret name: my-secret key: PRIMARY_KEY secondaryKey: type: Secret name: my-secret key: SECONDARY_KEY readOnlyPrimaryKey: type: Secret name: my-readonly-secret key: PRIMARY_KEY readOnlySecondaryKey: type: Secret name: my-readonly-secret key: SECONDARY_KEY endpoint: type: Secret name: my-endpoint key: ENDPOINT Then if they want to use the secondary key for a pod they would mount the endpoint and the primary/secondary keys from my-readonly-secret.\nWhile this isn\u0026rsquo;t quite as clean as it could otherwise be by supporting multiple secret destinations for each secret kind (endpoint, readOnlyPrimaryKey, etc) it\u0026rsquo;s simpler to implement and has fewer failure modes as we don\u0026rsquo;t have to deal with the possibility of the user specifying 10 destinations where we wrote some but failed to write others.\nConclusion: No, we will not support this.\nWhat client should we use for issuing the ListKeys/GetKeys request?\nWe don\u0026rsquo;t automatically generate methods for performing these calls. It seems easiest to just use the corresponding Azure SDK for this. This does mean that we\u0026rsquo;re taking a dependency on the SDK where we didn\u0026rsquo;t have one before, but the overall footprint of usage is pretty small and it\u0026rsquo;s going to be easier to do that than it is to write the methods ourselves.\nConclusion: We will use the Azure SDK for this.\nWhat API version should we use for issuing the ListKeys/GetKeys request?\nThere are a few options here. The custom hooks could be applied to either the customer facing resource version or to the internal storage version. Applying the hooks to the storage version is easier, but means that the same hook will be run regardless of which customer\nOther kinds of secrets #  See other kinds of secrets in Azure for examples of each of these types of secrets.\nGet once Azure generated secrets: These are secrets created by Azure and only returned once, usually as the response to a POST.\nThis pattern doesn\u0026rsquo;t seem to be very common. The current plan is to not support resources with this pattern. If we did need to support resources with this pattern there would be no way to avoid violating the work well with GitOps goal, as when deploying this resource there would be no way to adopt secrets associated with it (they\u0026rsquo;re GET-once). That might be ok for particular kinds of resources provided we document it though.\nIf there are large user requests for this sort of secret management we can support it similar to the ListKeys cases except that the resources won\u0026rsquo;t be movable without also cloning the secret.\n\u0026ldquo;Secrets\u0026rdquo; created by Azure, and returned in the GET: This is things like InstrumentationKey, or server endpoint URLs.\nSince these are returned in a GET they are not secret and are already being shown on Status. We will also manually classify some of them to be included in secrets we write. That will include things like endpoint. This means that endpoint for a SQL Server would show up in two places, in the status and possibly in the secret written by the operator (assuming that the user has instructed us to write the endpoint someplace).\nThe main reason for writing this into two places is:\n We already have it in the Status and it doesn\u0026rsquo;t make sense to remove it as it\u0026rsquo;s part of the resource payload from Azure. We want the ability to put it into a Secret so that if users want to inject it into their pods as an environment variable it\u0026rsquo;s easy to do so. There is no good way to inject environment variables from CRD Status\u0026rsquo;s.  Short-lived \u0026ldquo;tokens\u0026rdquo;: Like Storage SAS.\nWe will not support these sorts of secrets.\nA special note on KeyVault #  We have an open issue asking for support to manage KeyVault secrets via ASO.\nWe have to be very careful about support for KeyVault secrets (or certs, keys, etc), as the whole point of KeyVault is as a secure place to store your secrets. If you\u0026rsquo;re instead creating those secrets via the operator then you have by definition also located the secret in Kubernetes, which somewhat defeats the purpose from a security perspective.\nThere might be cases where this makes sense if there are APIs that require a KeyVault secret to be provided to them via ARM ID and so KeyVault isn\u0026rsquo;t the medium of secure storage so much as it is a medium of secret transfer.\nUntil we run into such scenarios we should avoid implementing any of the KeyVault key/secret/certificate/etc APIs.\nIntegrations #  If/when we support storing secrets in KeyVault, we can create some demos showing integration with the KeyVault secret store csi driver.\nSupported secret stores #  P0: Kubernetes Secrets\nP1: KeyVault - this seems more interesting for the \u0026ldquo;secrets from Azure\u0026rdquo; case, since at least right now you cannot create KeyVault secrets through ASO. That limitation means if you wanted to use KeyVault for input secrets you must have already pre-created the secrets before deploying via the operator.\nImplementation plan #  There are effectively two parallel features here:\n Input secrets (reading secrets from a store and supplying those secrets to Azure) Output secrets (reading secrets from Azure and storing them in a secret store)  Input secrets #   SecretReference implementation. Code generator changes to detect format: password or x-ms-secret and transform properties appropriately. Reflector library to generically crawl resources and find secret refs. SecretReader library to get secrets from a collection of SecretReference\u0026rsquo;s. This should be expandable to support KeyVault secret refs in the future if we decide to add them. Should probably look something like this. Note that this related to the SecretWriter below in the output secrets section: // TODO: these strings may need to be []byte // Note: these interfaces are acting on collections only so that they can be more efficient, as multiple SecretReference\u0026#39;s or secret values may be read from or written to // a single secret.  type SecretReader interface { GetSecrets(ctx context.Context, refs []SecretReference) (map[SecretReference]string, error) }  ARM conversion changes to take a ResolvedSecrets parameter in addition to the ResolvedReferences it takes now. This will probably require a bit of fussing with the types passed to the ARM conversion methods and the conversion methods themselves. Reconciler changes to perform secret reading in addition to reference resolving. Testing: Modify existing tests passing secrets in plain text as part of the spec to use the new mechanism instead. [Stretch goal] Support for secret rollover:  Hook to add field indexers for specific resources (see what we\u0026rsquo;re doing for MySQL in ASOv1 today). Hook to control custom additional watches used for monitoring changes to secrets (see what we\u0026rsquo;re doing for MySQL in ASOv1 today).    Output secrets #   operatorSpec/operatorStatus preparatory work. See #1612. Add hooks to controller allowing handcrafted per-resource customization, see #1978. Update azure-arm configuration to allow for additional AzureGeneratedSecret properties to be defined. These properties will be rendered into the operatorSpec as SecretDestination\u0026rsquo;s and subsequently read by the resource specific hooks in order to determine what (if any) ListKeys APIs to call and where to store the results. Implement the SecretWriter interface described below (or something like it). Note that this is related to the SecretReader above in the input secrets section. // TODO: these strings may need to be []byte // Note: these interfaces are acting on collections only so that they can be more efficient, as multiple SecretReference\u0026#39;s or secret values may be read from or written to // a single secret. type DestinationValuePair struct { Value string Destination SecretDestination } type SecretWriter interface { // This will perform ownership checks and return an error if an attempt is made to update a secret that is not owned by the operator  SetSecrets(ctx context.Context, owner MetaObject, secrets []DestinationValuePair) error }  Use customization hooks to implement GetKeys or ListKeys for each applicable resource, utilizing the SecretWriter to write the secrets to their destination.  Testing #  Unit testing #  All of the library code should have unit tests written:\n Reflector to discover SecretReference. SecretReader/SecretWriter  End to end testing #  The existing EnvTest tests can be expanded to test secret management. We will need to make sure that we redact the keys returned by ListKeys. Tests for resources which take secrets will need the secrets created beforehand and passed to the resource (just as the customer would have to do).\nRelated issues #   Secrets created by ASO should have configurable annotations Path to secrets created by ASO should be in the status - maybe not needed if we\u0026rsquo;re making the user tell us where to put it? Secrets deletion should be required as part of resource deletion Add option to create and manage keyvault secrets through ASO  "},{"id":16,"href":"/azure-service-operator/design/type-references-and-ownership/","title":"Type References and Ownership","section":"Design \u0026 Specifications","content":"Type references and ownership #  Related reading #   Kubernetes garbage collection. Kubernetes RBAC  Related Projects #   ASO: Azure Service Operator k8s-infra: The handcrafted precurser to the code generation tool being designed.  Goals #   Provide a way for customers to express relationships between Azure resources in an idiomatic Kubernetes way. Provide automatic ownership and garbage collection (using Kubernetes garbage collection) where appropriate (e.g. ResourceGroup as an owner of all the resources inside of it)  Ideally ResourceGroup is handled the same as other owners and isn\u0026rsquo;t special cased.   Define how Kubernetes interacts with Azure resources not created/managed by Kubernetes, for example resources which were created prior to the customer onboarding to the Azure Service Operator. References should be extensible to work across multiple Azure subscriptions, although initially we may not support that.  Non-goals #   Managing ownership for resources/resource hierarchies that were not created by the service operator. While this proposal allows references to point to external resources not managed by the service operator, the operator is not watching/monitoring the resource in question and as such cannot propagate deletes. Put another way: for the operator to manage ownership/object lifecycles, the entire resource hierarchy must exist within Kubernetes. If only part of the resource hierarchy is managed by the service operator, only those parts can have their lifecycles managed.  Different kinds of resource relationships in Azure #  Related/linked resources #  Two resources are related to one another (\u0026ldquo;has-a\u0026rdquo; relationship), but there is no ownership. Example: VMSS → Subnet (json schema).\nThis relationship is always one-way (a VMSS refers to a Subnet, but a Subnet does not refer to a VMSS).\nOwner and dependent #  Two resources have a relationship where one is owned by the other.\nExamples:\n a RouteTable owns many Routes (json schema) a BatchAccount owns many Pools (json schema) a ResourceGroup owns any resource  A relationship like those shown here tells us two things:\n Where to create/manage the dependent resource (this Route goes in that particular RouteTable, this RouteTable has that Route) That the dependent resource should be deleted when the parent resource is deleted. There are theoretically two cases here:  The dependent resource must be deleted before the parent can be deleted. Deletion of the parent automatically cascades to all dependent resources. Due to how Azure ARM resources express ownership (via id which is part of the URL, with dependent resources being a subdirectory under the owning resources URL) all ARM resources should fall into this case.    Note that sometimes an owning resource has its dependent resources embedded directly (for example: RouteTable has the property RouteTablePropertiesFormat). Most types do not embed the dependent resource directly in the owning resource. We will need to cater for both the embedded and non-embedded cases.\nWhat do these relationships look like in existing solutions? #  This section examines how other operator solutions have tackled these problems. We look at:\n ARM templates Azure Service Operator (ASO) k8s-infra  Related/Linked resources #  What does ARM do? #  These are just properties (often but not always called id) which refer to the fully qualified ARM ID of another resource. For example see a sample deployment template for a VMSS refering to an existing vnet.\n\u0026#34;properties\u0026#34;: { \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(parameters(\u0026#39;existingVnetResourceGroupName\u0026#39;), \u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, parameters(\u0026#39;existingVnetName\u0026#39;), parameters(\u0026#39;existingSubNetName\u0026#39;))]\u0026#34; }, } What does ASO do? #  Similar to how ARM templates behave, ASO uses the decomposition of fully qualified resource id to reference another resource, as seen here for VMSS → VNet\ntype AzureVMScaleSetSpec struct { ... Location string `json:\u0026#34;location\u0026#34;` ResourceGroup string `json:\u0026#34;resourceGroup\u0026#34;` VirtualNetworkName string `json:\u0026#34;virtualNetworkName\u0026#34;` SubnetName string `json:\u0026#34;subnetName\u0026#34;` ... } These properties are combined into a fully qualified ARM ID like so:\nsubnetIDInput := helpers.MakeResourceID( client.SubscriptionID, resourceGroupName, \u0026#34;Microsoft.Network\u0026#34;, \u0026#34;virtualNetworks\u0026#34;, vnetName, \u0026#34;subnets\u0026#34;, subnetName, ) This produces a resource ID: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}.\nCurrently ASO does not support cross-subscription references (and some of the resources such as VMSS don\u0026rsquo;t allow cross-resource group references), but it in theory could by adding parameters.\nWhat does k8s-infra do? #  k8s-infra is a bit different in that resource references are in Kubernetes style (namespace + name) and not Azure style (resource-group + resource-name). All resource references are done using the special type KnownTypeReference which contains the fully qualified Kubernetes name for the resource.\nDependent Resources #  What does ARM do? #  ARM template deployments support two different ways of deploying dependent resources:\n Deploy the resources in the same ARM template with dependent resources embedded inside owning resources using the resources property.  The dependent resource need not specify which resource is its owner, because it is implied by embedded structure. The owning resource and the dependent resource must be created at the same time.   Deploy the resources separately.  The dependent resource must specify which resource is its owner by including in its name field both the owning resource name and the dependent resource name separated by a /. Each segment of the name corresponds to an owning resource. For example creating a Batch Pool foo in Batch account account would have name = account/foo. The dependent resource can be created after the owning resource has already been created, or can be created at the same time as the owning resource. If created at the same time, the dependsOn field must be used to inform ARM of the order to perform resource creation.    What does ASO do? #  Dependent resources in ASO have properties which map to the name/path to their owner. For example MySQLFirewallRuleSpec looks like this:\ntype MySQLFirewallRuleSpec struct { ResourceGroup string `json:\u0026#34;resourceGroup\u0026#34;` Server string `json:\u0026#34;server\u0026#34;` StartIPAddress string `json:\u0026#34;startIpAddress\u0026#34;` EndIPAddress string `json:\u0026#34;endIpAddress\u0026#34;` } The ResourceGroup and Server are references to the owners of this type.\nWhat does k8s-infra do? #  k8s-infra uses the same KnownTypeReference type mentioned above for ownership references too. There are two patterns for ownership in k8s-infra today.\nOne pattern is used for ResourceGroup, where top level resources have a link to the resource group they are in.\ntype VirtualMachineScaleSetSpec struct { // +kubebuilder:validation:Required  ResourceGroupRef *azcorev1.KnownTypeReference `json:\u0026#34;resourceGroupRef\u0026#34; group:\u0026#34;microsoft.resources.infra.azure.com\u0026#34; kind:\u0026#34;ResourceGroup\u0026#34;` ... } The other pattern is where the owning resource has links to the dependent resources it is expecting to have:\ntype RouteTableSpecProperties struct { DisableBGPRoutePropagation bool `json:\u0026#34;disableBgpRoutePropagation,omitempty\u0026#34;` RouteRefs []azcorev1.KnownTypeReference `json:\u0026#34;routeRefs,omitempty\u0026#34; group:\u0026#34;microsoft.network.infra.azure.com\u0026#34; kind:\u0026#34;Route\u0026#34; owned:\u0026#34;true\u0026#34;` } If the dependent resources aren\u0026rsquo;t there, the status of the owning resource reflects an error explaining that.\nProposal #  A note on names before we get started #  In Kubernetes each resource must have a unique name for its group-kind. For example, if we had a RouteTable CRD, each RouteTable object would need to have a unique name. In ARM, resources do not need to be uniquely named. There can be two RouteTable resources with the same name provided they are in different resource groups. The owner-dependent resource relationship impacts uniqueness in Azure in a way that it doesn\u0026rsquo;t in Kubernetes.\nProposed solution #  All Kubernetes resources will have two fields which are used in combination to build the Azure name: Metadata.Name and Spec.AzureName. When Spec.AzureName is empty, Metadata.Name is used as the resource name. When Spec.AzureName is provided, it takes precedence and is used when interacting with ARM, but the resource in Kubernetes is still called by its Metadata.Name.\nOverview #  We propose the following high-level solution:\n All references will be via Kubernetes group, kind, and name. If a resource not managed by Kubernetes must be referenced, that resource must be imported into Kubernetes as an Unmanaged resource. Dependent resources will refer to their parent via an Owner property. The Owner property will automatically detect group and kind, making specifying an owner as simple as providing the Kubernetes resource name. Dependent resources with the Owner property set will automatically have their ownerReferences configured so that Kubernetes garbage collection will delete the dependent resources when the owner is deleted. References to related resources will be automatically detected by the code generator and transformed into the correct reference type. At serialization time, the controller will transform the Kubernetes types (including related resource references and owner references) into the correct Azure resource definitions (including fully qualified ARM IDs).  More specific details about how this will be achieved are in the following sections.\nHow to represent references #  There are two kinds of references we need to represent: References to a resource whose type we know statically at compile time, and references to a resource whose type we do not know at compile time.\nWe could use the same type for both kinds of references, but that has the downside of allowing a situation where we know the group and version statically at compile time, but the customer has also provided it and it doesn\u0026rsquo;t match. Two types allows us to clearly express what we\u0026rsquo;re expecting for each reference. The resulting YAMLs look basically the same to the customer, and the required-ness of the fields will give push-back when customers need to specify a group or kind and have not.\n// KnownResourceReference is a resource reference to a known type. type KnownResourceReference struct { // This is the name of the Kubernetes resource to reference. \tName string `json:\u0026#34;name\u0026#34;` // References across namespaces are not supported.  // Note that ownership across namespaces in Kubernetes is not allowed, but technically resource \t// references are. There are RBAC considerations here though so probably easier to just start by \t// disallowing cross-namespace references for now } type ResourceReference struct { // The group of the referenced resource.  Group string `json:\u0026#34;group\u0026#34;` // The kind of the referenced resource.  Kind string `json:\u0026#34;kind\u0026#34;` // The name of the referenced resource.  Name string `json:\u0026#34;name\u0026#34;` // Note: Version is not required here because references are all about linking one Kubernetes  // resource to another, and Kubernetes resources are uniquely identified by group, kind, (optionally namespace) and  // name - the versions are just giving a different view on the same resource } How to represent ownership and dependent resources #  We will use the same KnownResourceReference type as an additional Owner field on dependent resource specifications.\nWhen we determine that a resource is a dependent resource of another resource kind, we will code-generate an Owner property in the dependent resource Spec. This will also include an annotation about the expected type of the resource (group and kind) so that the customer doesn\u0026rsquo;t have to specify that in the YAML.\ntype SubnetSpec struct { Owner KnownResourceReference `json:\u0026#34;owner\u0026#34; group:\u0026#34;microsoft.network.infra.azure.com\u0026#34; kind:\u0026#34;VirtualNetwork\u0026#34;` ... } When users submit a dependent object we will validate that the provided owner reference is present. This can be accomplished by making the property required in the CRD.\nA YAML snippet showing how this will look from the customer\u0026rsquo;s perspective:\n... spec: owner: name: my-vnet ... One major advantage of this approach is that the customer cannot really get the owning type wrong, because we\u0026rsquo;ve autogenerated the expected group/kind information all names they supply must point to the right kind of resource.\nHow to represent a resource generically #  In addition to representing references generically, we will need the ability to reference ARM resources generically, so that the generic controller can act on them without needing to cast to their specific type.\n// TODO: There may be more in this interface, or it may get rolled into MetaObject depending on yet to be determined implementation details type ArmResource interface { // Name returns the ARM resource name \tName() string // Owner returns the ResourceReference so that we can extract the Group/Kind for easier lookups  Owner() *ResourceReference } How to identify resource relationships #  For related (not owned) resources we must find each field that represents a resource reference and transform its type to ResourceReference. There is no specific marker which means: \u0026ldquo;This field is a reference\u0026rdquo; - most are called id but that\u0026rsquo;s not a guarantee. For example on the VirtualMachineScaleSetIPConfigurationProperties the subnet field is of custom type ApiEntityReference, which has an id field where you put the ARM ID for a subnet. This may require some manual work. One thing we can investigate doing long term is see if there\u0026rsquo;s a way to get teams to annotate \u0026ldquo;links\u0026rdquo; in their Swagger somehow.\nFor dependent resources we must identify all of the owner to dependent relationships between resources. As discussed in what ARM does, this can be done using the resources property in the ARM deployment templates. These are much easier to automatically detect than related resources as the dependent types are called out in the resources property explicitly.\nHow to choose the right reference type (ResourceReference vs KnownResourceReference) at generation time #  Because we are code-generating all of the Owner fields based on the resources property in the JSON schema, and each ARM resource can be owned by at most 1 other resource, we can always supply the annotations for group and kind automatically for the Owner field. This is not the case for abitrary references (id\u0026rsquo;s) to other resources. We do not actually know programmatically what type those references are. In some cases it may actually be allowed to point to multiple different types (for example: custom image vs shared image gallery).\nIn the KnownResourceReference case, we know the type we\u0026rsquo;re looking for and can fail fast if the customer specifies the wrong type. In the ResourceReference case, we cannot know the type we\u0026rsquo;re looking for, so we must accept what the customer has provided and ensure that we have good error messages if they have provided a link to an invalid resource (usually the error from Azure should suffice).\nResourceLifeCycle and unmanaged resources #  In order to keep references Kubernetes-native, allow a \u0026ldquo;single pane of glass\u0026rdquo; for customers looking at their Azure resources through Kubernetes, and allow references to resources that were created before the customer onboarded to the operator, we introduce a new mode to each resource: ResourceLifeCycle.\nResourceLifeCycle can be either Managed or Unmanaged.\nResourceLifeCycle is not specified by the customer explicitly in the Spec, instead it is inferred based on how the resource was created in Kubernetes. If a resource is created as just a reference (id, name, no spec details) then it is Unmanaged. If a resource is created with a populated spec, then the resource is Managed.\nRouteTable + Routes issue (multiple routes of the same name are allowed) #  Options for this: Note that all of these options share this restriction: Each resource must be imported, e.g. to import a VNET you may need to import the resource group the VNET is in, and then the VNET (with an Owner reference pointing to the imported ResourceGroup).\nOption 1: Users must create a valid resource with the same name as the resource they want to track. If this resource\u0026rsquo;s spec differs from what is in Azure, an error is logged but we never actually apply any state to Azure (i.e. we don\u0026rsquo;t try to sync to the spec). A tag in the metadata must be added to inform the operator not to sync.\nAdvantages\n Swapping from unmanaged to managed is super easy, just remove the tag blocking the reconciliation loop.  Disadvantages\n There is possibly a significant amount of extra effort required to re-specify a resource whose shape we really just want to \u0026ldquo;import\u0026rdquo; from ARM. Worse for large trees of objects or deeply nested objects. If the tag is forgotten (or has a typo) we will try to manage a resource which we shouldn\u0026rsquo;t be managing. This could be very problematic depending on how different the specification is from what exists in Azure. The existence of a spec may suggest we are actually seeking towards it \u0026ndash; which we are not. ASO does have a similar feature though so maybe not that big of a problem.  Option 2: Users create an entity with just the \u0026ldquo;identifying fields\u0026rdquo; set: Metadata.Name, Owner, and optionally Spec.AzureName. When an entity is created like this, the controller knows to treat it specially (optionally may also add a tag automatically?). These entities will only be watched by the controller, no mutating update will be sent to ARM.\nAdvantages\n Relatively easy to import even complex object hierarchies.  Disadvantages\n This screws up the \u0026ldquo;required-ness\u0026rdquo; of non-identifying fields in a spec. For example: a Virtual Network requires a Properties VirtualNetworkProperties field to be set, but since we have to allow that field to be nil when importing a Virtual Network we can\u0026rsquo;t set the Properties field with a required annotation for Kubebuilder.  Option 3: Same as option 2, but use anyOf to specify two valid structures:\nspec: type: object properties: owner: properties: name: type: string azureName: type: string foo: type: integer anyOf: - required: [\u0026#34;owner\u0026#34;] - required: [\u0026#34;owner\u0026#34;, \u0026#34;foo\u0026#34;] Note that it has to be anyOf because oneOf disallows multiple matches, and owner + foo matches both sets in the example above.\nAdvantages\n Represents what we want and maintains better automatic validation.  Disadvantages\n Kubebuilder doesn\u0026rsquo;t support generating this, so we would have to come up with another way to do it, or possibly upstream changes to Kubebuilder to support it.  Option 4: Other ideas\u0026hellip; Do away with Kubebuilder validation entirely and use our own (including our own validating webhooks). Use Kustomize and our own code-generator/parser to generate amendments to Kubebuilder\u0026rsquo;s generated CRDs to get the anyOf shape we want above.\nHow to transform Kubernetes objects to ARM objects (and back) #  In the case of resource ownership, the proposed Owner property exists on dependent resources in the CRD but must not go to Azure as Azure doesn\u0026rsquo;t understand it. In the case of a generic resource reference, the ResourceReference in the CRD must become an id (with fully-qualified ARM ID) when serialized to ARM. In both cases, we need two representations of the entity: one to Kubernetes as the CRD, and one to Azure. These two types are structurally similar but not identical. We cannot just override JSON serialization to solve this problem due to the fact that there are actually two distinct JSON representations we need.\nThe proposed solution is that the code generator intelligently generates 2 types for cases where we know the CRD shape differs from ARM. We will add an interface which types can optionally implement which allows them to transform themselves to another type prior to serialization to/from ARM. This is also a useful hook for any manual customization for serialization we may need.\nThe interface will look something like this:\ntype ARMTransformer interface { ToArm(owningName string) (interface{}, error) FromArm(owner KnownResourceReference, input interface{}) error } Here\u0026rsquo;s an example of how it will be implemented:\nfunc CreateArmResourceNameForDeployment(owningName string, name string) string { result := owningName + \u0026#34;/\u0026#34; + name return result } // +kubebuilder:object:root=true // +kubebuilder:storageversion type VirtualNetworksSubnets struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec VirtualNetworksSubnetsSpec `json:\u0026#34;spec,omitempty\u0026#34;` } var _ ArmResource = \u0026amp;VirtualNetworksSubnets{} func (resource *VirtualNetworksSubnets) Owner() *ResourceReference { r := reflect.TypeOf(resource.Spec) ownerField, found := r.FieldByName(\u0026#34;Owner\u0026#34;) if !found { return nil } group := ownerField.Tag.Get(\u0026#34;group\u0026#34;) kind := ownerField.Tag.Get(\u0026#34;kind\u0026#34;) return \u0026amp;ResourceReference { group: group, kind: kind, name: resource.Spec.Owner.Name } } func (resource *VirtualNetworksSubnets) Name() string { return resource.Spec.Name } type VirtualNetworksSubnetsSpec struct { // +kubebuilder:validation:Required \tApiVersion VirtualNetworksSubnetsSpecApiVersion `json:\u0026#34;apiVersion\u0026#34;` // +kubebuilder:validation:Required \tName string `json:\u0026#34;name\u0026#34;` // +kubebuilder:validation:Required \tOwner genruntime.KnownResourceReference `json:\u0026#34;owner\u0026#34; group:\u0026#34;microsoft.network\u0026#34; kind:\u0026#34;VirtualNetworks\u0026#34;` // +kubebuilder:validation:Required \t//Properties: Properties of the subnet. \tProperties SubnetPropertiesFormat `json:\u0026#34;properties\u0026#34;` // +kubebuilder:validation:Required \tType VirtualNetworksSubnetsSpecType `json:\u0026#34;type\u0026#34;` } // No KubeBuilder comments required here because not ever used to generate CRD type VirtualNetworksSubnetsSpecArm struct { ApiVersion VirtualNetworksSubnetsSpecApiVersion `json:\u0026#34;apiVersion\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` //Properties: Properties of the subnet. \tProperties SubnetPropertiesFormat `json:\u0026#34;properties\u0026#34;` Type VirtualNetworksSubnetsSpecType `json:\u0026#34;type\u0026#34;` } // This interface implementation would be autogenerated for ARM resources with references var _ genruntime.ArmTransformer = \u0026amp;VirtualNetworksSubnetsSpec{} func (transformer *VirtualNetworksSubnetsSpec) ToArm(owningName string) (interface{}, error) { result = VirtualNetworksSubnetsSpecArm{} result.ApiVersion = transformer.ApiVersion result.Name = CreateArmResourceNameForDeployment(owningName, transformer.Name) result.Properties = transformer.Properties result.Type = transformer.Type return result, nil } func (transformer *VirtualNetworksSubnetsSpec) FromArm(owner genruntime.KnownResourceReference, input interface{}) error { typedInput, ok := input.(VirtualNetworksSubnetsSpecArm) if !ok { return fmt.Errorf(\u0026#34;unexepected type supplied for FromArm function. Expected VirtualNetworksSubnetsSpecArm, got %T\u0026#34;, input) } transformer.ApiVersion = typedInput.ApiVersion transformer.Name = ExtractKubernetesResourceNameFromArmName(typedInput.Name) transformer.Owner = owner transformer.Properties = typedInput.Properties transformer.Type = typedInput.Type return nil } Controller example #  Putting it all together, here\u0026rsquo;s what a generic controller reconciliation loop would look like using the interfaces discussed previously.\n// Example usage -- error handling elided for brevity func (gr *GenericReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { scheme := ... gvk := ... client := ... // Load the object from etcd  obj := scheme.New(gvk) resource := client.Get(req.NamespacedName, obj) // Get the owner details  armResource := obj.(ARMResource) ownerRef := armResource.Owner() // Perform a get from Azure to see current resource state  armId := helpers.GetArmId(resource) objFromAzure := scheme.new(gvk) // We need to provide the empty type to deserialize into  // Somehow construct a new object of type etcdObject  if armTransformer, ok := objFromAzure.(ARMTransformer); ok { result := armTransformer.ToArm(\u0026#34;\u0026#34;) // This just converts from an empty kube shape to an empty arm shape  armClient.GetIt(armId, result) armTransformer.FromArm(ownerRef, result) } // Perform a put to update resource state  // Walk the owner hierarchy (assuming owner has no owner here for simplicity) to build owner name  ownerGvk := ownerRef.ToGvk() owner := scheme.New(ownerGvk) ownerArmResource := owner.(ARMResource) ownerId := owner.Name() var toSerialize interface{} toSerialize = resource if armTransformer, ok := toSerialize.(ARMTransformer); ok { toSerialize = armTransformer.ToArm(ownerArmId) } json := json.Marshal(toSerialize) armClient.SendIt(json) } FAQ #  What happens when a dependent resource specifies an Owner that doesn\u0026rsquo;t exist? #  The dependent resource will be stuck in an unprovisioned state with an error stating that the owner doesn\u0026rsquo;t exist. If the owner is created, the dependent resource will then be created by the reconciliation loop automatically.\nWhat happens when a resource contains a link to another resource which doesn\u0026rsquo;t exist? #  The resource with the link will be stuck in an unprovisioned state with an error stating that the linked resource doesn\u0026rsquo;t exist. This behavior is the same as for a dependent resource with a non-existent owner.\nHow are the CRD entities going to be rendered as ARM deployments? #  There are a few different ways to perform ARM deployments as discussed in Dependent Resources. Due to the nature of Kubernetes CRDs, each resource is managed separately and has its own reconcilation loop. It doesn\u0026rsquo;t make sense to try to deploy a single ARM template with the entire resource graph. Each resource will be done in its own deployment (with a dependsOn specified if required).\nAren\u0026rsquo;t there going to be races in resource creation? #  Yes. If you have a complex hierarchy of resources (where resources have involved relationships between one another) and submit all of their YAMLs to the operator at the same time it is likely that some requests when sent to ARM will fail because of missing dependencies. Those resources that failed to deploy initially will be in an unprovisioned state in Kubernetes, and eventually all the resources will be created through multiple iterations of the reconciliation loop.\nAren\u0026rsquo;t there going to be races in resource deletion? #  Yes. Owner as discussed in this specification is informing Kubernetes how Azure behaves. The fact that a ResourceGroup is the owner of a VirtualMachineScaleSet means that when the ResourceGroup is deleted in Azure, the VirtualMachineScaleSet will be too.\nThis means that practically speaking, we don\u0026rsquo;t need Kubernetes garbage collection to perform deletion of resources in Azure. Azure is already going to do that automatically. We need Kubernetes garbage collection to easily maintain sync with Azure.\nAs far as implementation goes this just means that when we are performing deletes in the generic controller and the resource is already deleted in Azure we just swallow that error and allow the Kubernetes object to be deleted.\nWhat exactly happens when a resource with an Owner is created? #  Once the resource has been accepted by the various admissions controllers and has been cofirmed to match the structural schema defined in the CRD, the generic controller will attempt to look up the owning resource in etcd (or in ARM if it\u0026rsquo;s an AzureReference).\nIf the generic controller finds the owning resource, it updates the ownerReference in the object metadata to include the uid of the owning resource and then submits an ARM template to ARM using the name of the owner and the name of the resource to build the name specified in the ARM template. It will include the name of the owner in the dependsOn field.\nWhat happens if an owning resource is deleted and immediately recreated? #  Kubernetes garbage collection is based on object uid\u0026rsquo;s. As discussed above, we bind to that uid on dependent resource creation. If a resource is deleted and then recreated Kubernetes will still understand that the new resource is fundamentally different than the old resource and garbage collection will happen as expected. The result will be that there is a new owning resource but all of its dependent resources were deleted (in Azure and in k8s).\nTODOs #   How can we allow customers to easily find all dependents for a particular owner (i.e. all subnets of a vnet) using kubectl? Cross subscription refs? Note that these are supported by a few Azure resources (VNET for example), but aren\u0026rsquo;t supported in most places.  Questions #  These are questions I am posing to the group - I don\u0026rsquo;t expect to have an answer without input from the group.\n What to do with awkward resources where the owner requires at least 1 dependent to also be created with it? David Justice pointed out this one Do we want to use the same type for ownership relationships and \u0026ldquo;related\u0026rdquo; relationships? Ownership has other angles such as how deletes propagate which in theory don\u0026rsquo;t apply for other kinds of relationships. Do we need to worry about letting customers choose between foreground cascading deletion and background cascading deletion or do we just pick one behavior which is best for our case?  The road not travelled #  Shape of Azure References #  We considered avoiding the complexity of ResourceLifecycle (Managed vs Unmanaged), instead allowing references to Azure resources directly by ARM ID.\nReferences would look like this:\ntype KnownResourceReference struct { Kubernetes KnownKubernetesReference `json:\u0026#34;kubernetes\u0026#34;` Azure string `json:\u0026#34;azure\u0026#34;` } type ResourceReference struct { Kubernetes KubernetesReference `json:\u0026#34;kubernetes\u0026#34;` Azure string `json:\u0026#34;azure\u0026#34;` } type KnownKubernetesReference struct { // This is the name of the Kubernetes resource to reference. \tName string `json:\u0026#34;name\u0026#34;` // References across namespaces are not supported. \t// Note that ownership across namespaces in Kubernetes is not allowed, but technically resource \t// references are. There are RBAC considerations here though so probably easier to just start by \t// disallowing cross-namespace references for now } type KubernetesReference struct { // The group of the referenced resource. \tGroup string `json:\u0026#34;group\u0026#34;` // The kind of the referenced resource. \tKind string `json:\u0026#34;kind\u0026#34;` // The name of the referenced resource. \tName string `json:\u0026#34;name\u0026#34;` // Note: Version is not required here because references are all about linking one Kubernetes \t// resource to another, and Kubernetes resources are uniquely identified by group, kind, (optionally namespace) and \t// name - the versions are just giving a different view on the same resource } Advantages compared to what we chose #   Can track resources which are not tracked by Kubernetes. Doesn\u0026rsquo;t need to introduce ResourceLifeCycle. ResourceLifeCycle complicates the mental model of individual resources as now apply on a resource can fail due to ResourceLifeCycle being Unmanaged. Can support references to resource types which the operator doesn\u0026rsquo;t yet support. It\u0026rsquo;s likely that we can work around this in the chosen architecture if it becomes a big problem though.  Disadvantages compared to what we chose #   References are not always Kubernetes-native looking. The reference structure is a more complex nested type, which makes references (which are common) more complicated. Moving from a resource link being to Azure directly to that same resource being managed/tracked by Kubernetes requires sweeping updates across all types referencing the migrated resource. Doesn\u0026rsquo;t allow for a \u0026ldquo;single pane of glass\u0026rdquo; experience where customers can easily view all of their resources in a Kubernetes native way.  How to represent references #  Use fully qualified ARM ID (a single string) for all references #  Pros #   Super simple to implement, because it\u0026rsquo;s what ARM expects at the end of the day anyway.  Cons #   You can\u0026rsquo;t easily transplant your YAML between subscriptions/resource groups because those IDs are in the YAML - you need templates and variables so that you can easily move between different resource groups or Subscriptions. Customers can\u0026rsquo;t stay in Kubernetes-land, they have to move their mental model to an \u0026ldquo;Azure\u0026rdquo; model.  Use built-in OwnerReference for owner references (customer setting these directly) #  Pros #   Basically none - customers are not supposed to set this directly.  Cons #   OwnerReference requires the object UID, which cannot be known at template authoring time. OwnerReference only works for ownership relationships, not for references.  Where ownership references are specified #  Ownership is from owner to dependent #  Pros #   It makes getting a list of all resources under a particular owner very easy.  Cons #   Adding/deleting a new dependent resource requires an update to the owner. The owner can be in a failed state because dependent resources are missing. It feels like we\u0026rsquo;re repeating our intent here: On the one hand, we told the owner that it should have 3 dependents, while on the other hand we only created 2 of those 3. It feels like the state of the resources in kubernetes (i.e. how many dependents there actually are) is already expressing the intent for how many we want, so having that also on the owner seems duplicate.  "},{"id":17,"href":"/azure-service-operator/introduction/annotations/","title":"Annotations","section":"User’s Guide","content":"Annotations understood by the operator #  Annotations specified by the user #  Note that unless otherwise specified, allowed values are case sensitive and should be provided in lower case.\nserviceoperator.azure.com/reconcile-policy #  Specifies the reconcile policy to use. Allowed values are:\n manage: The operator performs all actions as normal. This is the default if no annotation is specified. skip: All modification actions on the backing Azure resource are skipped. GETs are still issued to ensure that the resource exists. If the resource doesn\u0026rsquo;t exist, the Ready condition will show a Warning state with the details of the missing resource until the resource is created. If the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and DELETE are skipped while GET is allowed. detach-on-delete: Modifications are pushed to the backing Azure resource, but if the resource is deleted in Kubernetes, it is not deleted in Azure. In REST API terminology, PUT and GET are allowed while DELETE is skipped.  Unknown values default to manage.\nAnnotations written by the operator #  These annotations are written by the operator for its own internal use. Their existence and usage may change in the future. We recommend users avoid depending upon these annotations:\n serviceoperator.azure.com/resource-id: The ARM resource ID. serviceoperator.azure.com/poller-resume-token: JSON encoded token for polling long running operation. serviceoperator.azure.com/poller-resume-id: ID describing the poller to use.  "},{"id":18,"href":"/azure-service-operator/introduction/authentication/","title":"Authentication","section":"User’s Guide","content":"Authentication in Azure Service Operator v2 #  Azure Service Operator supports two different styles of authentication today.\n Service Principal aad-pod-identity authentication (managed identity)  Service Principal #  Prerequisites #   An existing Azure Service Principal.  To use Service Principal authentication, specify an aso-controller-settings secret with AZURE_CLIENT_ID and AZURE_CLIENT_SECRET set.\n AZURE_CLIENT_ID must be set to the Service Principal client ID. This will be a GUID. AZURE_CLIENT_SECRET must be set to the Service Principal client secret.  For more information about Service Principals, see creating an Azure Service Principal using the Azure CLI. The AZURE_CLIENT_ID is sometimes also called the App ID. The AZURE_CLIENT_SECRET is the \u0026ldquo;password\u0026rdquo; returned by the command in the previously linked documentation.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aso-controller-settings namespace: azureserviceoperator-system stringData: AZURE_SUBSCRIPTION_ID: \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$AZURE_CLIENT_ID\u0026#34; AZURE_CLIENT_SECRET: \u0026#34;$AZURE_CLIENT_SECRET\u0026#34; EOF Managed Identity (aad-pod-identity) #  Prerequisites #   An existing Azure Managed Identity. aad-pod-identity installed into your cluster. If you are running ASO on an Azure Kubernetes Service (AKS) cluster, you can instead use the integrated aad-pod-identity.  First, set the following environment variables:\nexport IDENTITY_RESOURCE_GROUP=\u0026#34;myrg\u0026#34; # The resource group containing the managed identity. export IDENTITY_NAME=\u0026#34;myidentity\u0026#34; # The name of the identity. export AZURE_SUBSCRIPTION_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure Subscription ID the identity is in. export AZURE_TENANT_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure AAD Tenant the identity/subscription is associated with. Use the az cli to get some more details about the identity to use:\nexport IDENTITY_CLIENT_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query clientId -otsv)\u0026#34; export IDENTITY_RESOURCE_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query id -otsv)\u0026#34; Deploy an AzureIdentity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentity metadata: name: aso-identity namespace: azureserviceoperator-system spec: type: 0 resourceID: ${IDENTITY_RESOURCE_ID} clientID: ${IDENTITY_CLIENT_ID} EOF Deploy an AzureIdentityBinding to bind this identity to the Azure Service Operator manager pod:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentityBinding metadata: name: aso-identity-binding namespace: azureserviceoperator-system spec: azureIdentity: aso-identity selector: aso-manager-binding EOF Deploy the aso-controller-settings secret, configured to use the identity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aso-controller-settings namespace: azureserviceoperator-system stringData: AZURE_SUBSCRIPTION_ID: \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$IDENTITY_CLIENT_ID\u0026#34; EOF "},{"id":19,"href":"/azure-service-operator/introduction/multitenant-deployment/","title":"Multitenant Deployment","section":"User’s Guide","content":"Deploying Azure Service Operator v2 in multi-tenant mode #  The current release version of the Azure Service Operator (ASO) deployment YAML installs the operator in single-tenant mode: deployed in the azureserviceoperator-system namespace with a single set of Azure credentials, and manages resources in any namespace in the cluster. That single operator deployment handles webhooks fired when Azure resources are changed.\nASO may also be deployed in a multi-tenant configuration, enabling the use of separate credentials for managing resources in different Kubernetes namespaces.\nRunning the operator in multi-tenant mode requires one deployment to handle webhooks (required because webhook configurations are cluster-level resources) and then a separate deployment for each tenant, each with its own credentials and set of namespaces that it watches for Azure resources.\nTo deploy the operator in multi-tenant mode the release YAML has been split into two parts:\n  Cluster-wide resources:\n  Custom resource definitions for the Azure resources.\n  Cluster roles for managing those resources.\n  The azureserviceoperator-system namespace containing the deployment and service to handle ASO webhooks. The webhook service is a deployment of the ASO image, but configured to run in webhook-only mode. It won\u0026rsquo;t try to reconcile Azure resources with ARM, and so doesn\u0026rsquo;t need any Azure credentials.\n  Webhook configuration referring to that service.\n    Per-tenant resources:\n  A namespace containing the deployment to run the tenant operator, configured for watchers-only mode.\n  The aso-controller-settings secret defining the Azure credentials that should be used, and the set of namespaces that this operator will watch for Azure resources.\n  A cluster role binding enabling the per-tenant operator\u0026rsquo;s service account to manage the Azure resources.\n    Example files #  Examples of the deployment YAML files are available on the release page for ASO v2 releases from v2.0.0-alpha.5. The cluster-wide file multitenant-cluster_v2.0.0-alpha.5.yaml can be used as-is (the webhook deployment namespace is fixed as azureserviceoperator-system), but the namespaces and cluster role binding in the per-tenant file multitenant-tenant_v2.0.0-alpha.5.yaml will need to be customised in each tenant\u0026rsquo;s YAML file from tenant1 to the desired name for that tenant.\nPer-tenant configuration #  Create the aso-controller-settings secret as described in the authentication docs, but create the secret in the tenant namespace and add an extra target namespaces key to it:\nexport TENANT_NAMESPACE=\u0026quot;\u0026lt;tenant namespace\u0026gt;\u0026quot; export AZURE_SUBSCRIPTION_ID=\u0026quot;\u0026lt;subscription id\u0026gt;\u0026quot; export AZURE_TENANT_ID=\u0026quot;\u0026lt;tenant id\u0026gt;\u0026quot; export AZURE_CLIENT_ID=\u0026quot;\u0026lt;client id\u0026gt;\u0026quot; export AZURE_CLIENT_SECRET=\u0026quot;\u0026lt;client secret\u0026gt;\u0026quot; export AZURE_TARGET_NAMESPACES=\u0026quot;\u0026lt;comma-separated namespace names\u0026gt;\u0026quot; kubectl create namespace \u0026quot;$TENANT_NAMESPACE\u0026quot; cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aso-controller-settings namespace: $TENANT_NAMESPACE stringData: AZURE_SUBSCRIPTION_ID: \u0026quot;$AZURE_SUBSCRIPTION_ID\u0026quot; AZURE_TENANT_ID: \u0026quot;$AZURE_TENANT_ID\u0026quot; AZURE_CLIENT_ID: \u0026quot;$AZURE_CLIENT_ID\u0026quot; AZURE_CLIENT_SECRET: \u0026quot;$AZURE_CLIENT_SECRET\u0026quot; AZURE_TARGET_NAMESPACES: \u0026quot;$AZURE_TARGET_NAMESPACES\u0026quot; EOF Once the tenant operator is deployed and configured the contents of the tenant namespace will look something like the following:\n$ kubectl get pods,replicasets,deployments,serviceaccounts,secrets -n tenant1-system NAME READY STATUS RESTARTS AGE pod/azureserviceoperator-controller-manager-657948696b-dzfmw 1/1 Running 3 3d NAME DESIRED CURRENT READY AGE replicaset.apps/azureserviceoperator-controller-manager-657948696b 1 1 1 3d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/azureserviceoperator-controller-manager 1/1 1 1 3d NAME SECRETS AGE serviceaccount/default 1 3d NAME TYPE DATA AGE secret/aso-controller-settings Opaque 5 3d secret/default-token-mqmpb kubernetes.io/service-account-token 3 3d Role handling #  The multi-tenant deployment example files have a single ClusterRole that grants access to the Azure resource types, and then a binding to that ClusterRole for the service account in each tenant-operator namespace. Each ClusterRoleBinding is named for the specific tenant so they don\u0026rsquo;t collide and can be managed separately:\nThis is convenient since there\u0026rsquo;s no need to permit access for Azure resources in each of the target namespaces individually, but it means that the only thing preventing one tenant operator from reading another\u0026rsquo;s resources is the AZURE_TARGET_NAMESPACES setting for each operator.\nFor some usage scenarios that might be too permissive.\nIn those cases the azureserviceoperator-manager-role should be changed from a ClusterRole into Roles in each of the target namespaces (where the Azure resources will be created, rather than where the tenant-operator pods run), and a RoleBinding should be created in that namespace linking the Role to the service account for the tenant operator that will be managing Azure resources in this target namespace:\nUpgrading #  When upgrading to a newer version of ASO the cluster-wide resources (CRDs, cluster roles) and the webhook deployment must be upgraded before upgrading the tenant operators.\nApplying the new version of the multitenant-cluster YAML file will add new and updated CRDs, then update the webhook configuration and cluster roles. After that the new version of the multitenant-tenant YAML files (customised for the specific tenant names) can be applied.\n"},{"id":20,"href":"/azure-service-operator/introduction/resources/","title":"Resources","section":"User’s Guide","content":"Supported Resources #  These are the resources with Azure Service Operator support committed to our main branch, grouped by the originating ARM service. (Newly supported resources will appear in this list prior to inclusion in any ASO release.)\nauthorization #  ARM version 2020-08-01-preview #   RoleAssignment (sample)  Use CRD version v1alpha1api20200801preview\nbatch #  ARM version 2021-01-01 #   BatchAccount (sample)  Use CRD version v1alpha1api20210101\ncache #  ARM version 2020-12-01 #   Redis (sample) RedisFirewallRule (sample) RedisLinkedServer (sample) RedisPatchSchedule (sample)  Use CRD version v1alpha1api20201201\nARM version 2021-03-01 #   RedisEnterprise (sample) RedisEnterpriseDatabase (sample)  Use CRD version v1alpha1api20210301\ncompute #  ARM version 2020-09-30 #   Disk (sample)  Use CRD version v1alpha1api20200930\nARM version 2020-12-01 #   VirtualMachine (sample) VirtualMachineScaleSet (sample)  Use CRD version v1alpha1api20201201\ncontainerservice #  ARM version 2021-05-01 #   ManagedCluster (sample) ManagedClustersAgentPool (sample)  Use CRD version v1alpha1api20210501\ndbformysql #  ARM version 2021-05-01 #   FlexibleServer (sample) FlexibleServersDatabase (sample) FlexibleServersFirewallRule (sample)  Use CRD version v1alpha1api20210501\ndbforpostgresql #  ARM version 2021-06-01 #   FlexibleServer (sample) FlexibleServersConfiguration (sample) FlexibleServersDatabase (sample) FlexibleServersFirewallRule (sample)  Use CRD version v1alpha1api20210601\ndocumentdb #  ARM version 2021-05-15 #   DatabaseAccount (sample) MongodbDatabase (sample) MongodbDatabaseCollection (sample) MongodbDatabaseCollectionThroughputSetting (sample) MongodbDatabaseThroughputSetting (sample) SqlDatabase (sample) SqlDatabaseContainer (sample) SqlDatabaseContainerStoredProcedure (sample) SqlDatabaseContainerThroughputSetting (sample) SqlDatabaseContainerTrigger (sample) SqlDatabaseContainerUserDefinedFunction (sample) SqlDatabaseThroughputSetting (sample)  Use CRD version v1alpha1api20210515\neventgrid #  ARM version 2020-06-01 #   Domain (sample) DomainsTopic (sample) EventSubscription (sample) Topic (sample)  Use CRD version v1alpha1api20200601\neventhub #  ARM version 2021-11-01 #   Namespace (sample) NamespacesAuthorizationRule (sample) NamespacesEventhub (sample) NamespacesEventhubsAuthorizationRule (sample) NamespacesEventhubsConsumerGroup (sample)  Use CRD version v1alpha1api20211101\ninsights #  ARM version 2018-05-01-preview #   Webtest (sample)  Use CRD version v1alpha1api20180501preview\nARM version 2020-02-02 #   Component (sample)  Use CRD version v1alpha1api20200202\nmanagedidentity #  ARM version 2018-11-30 #   UserAssignedIdentity (sample)  Use CRD version v1alpha1api20181130\nnetwork #  ARM version 2020-11-01 #   LoadBalancer (sample) NetworkInterface (sample) NetworkSecurityGroup (sample) NetworkSecurityGroupsSecurityRule (sample) PublicIPAddress (sample) VirtualNetwork (sample) VirtualNetworkGateway (sample) VirtualNetworksSubnet (sample) VirtualNetworksVirtualNetworkPeering (sample)  Use CRD version v1alpha1api20201101\noperationalinsights #  ARM version 2021-06-01 #   Workspace (sample)  Use CRD version v1alpha1api20210601\nservicebus #  ARM version 2021-01-01-preview #   Namespace (sample) NamespacesQueue (sample) NamespacesTopic (sample)  Use CRD version v1alpha1api20210101preview\nsignalrservice #  ARM version 2021-10-01 #   SignalR (sample)  Use CRD version v1alpha1api20211001\nstorage #  ARM version 2021-04-01 #   StorageAccount (sample) StorageAccountsBlobService (sample) StorageAccountsBlobServicesContainer (sample) StorageAccountsQueueService (sample) StorageAccountsQueueServicesQueue (sample)  Use CRD version v1alpha1api20210401\n"},{"id":21,"href":"/azure-service-operator/design/versioning/case-studies/chained-storage-versions/","title":"Chained Storage Versions","section":"Case Studies","content":"Case Study - Chained Storage Versions #  This case study explores the alternative solution of using a chained storage versions. We update the storage schema of each resource each release of the service operator. We\u0026rsquo;ll keep the storage version up to date with the latest GA release of each resource. Older storage versions are retained, both as intermediate steps in the hub-and-spoke conversions, and to allow upgrades.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v20110101storage type Person struct { PropertyBag FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Every property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we currently have only single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nThe PropertyBag type provides storage for other properties, plus helper methods. It is always included in storage versions, but in this case will be unused. The method Hub() marks this version as the storage schema.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import storage \u0026#34;v20110101storage\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest storage.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } Conversion is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nThese methods will be automatically generated in order to handle the majority of the required conversions. Since they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map is simple and straightforward.\nVersion 2012-02-02 - No Change #  In this release of the CRM service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  The existing conversion between the v20110101 API version and v20110101storage version is retained, preserving in-place a conversion that\u0026rsquo;s already known to be reliable.\nThe new API version 20120202 has a matching storage version v20120202storage which becomes the authoratative storage version for the CRD. This conversion is identical to the earlier version.\nAn additional bidirectional conversion between v20110101storage and v20120202storage is also generated. Since both versions have the same structure, this is also trivial.\nVersion Map #  Our version map diagram is becoming useful for seeing the relationship between versions:\nObserve that the prior storage version is still shown, with a bidirectional conversion with the current storage version. Existing users who upgrade their service operator will have their storage upgraded using this conversion. The conversion between storage versions will be generated with the same approach, and with the same structure, as all our other conversions.\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The new storage version, based on this version, is what you\u0026rsquo;d expect:\npackage v20130303storage type Person struct { PropertyBag Id *Guid FirstName *string MiddleName *string // *** New storage ***  LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order. This is to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are unchanged, as they still convert to the same storage versions.\nA new bidirectional conversion between v20120202storage and v20130303storage versions is introduced. When down-converting to v20120202storage, the MiddleName property is stashed in the property bag; when up-converting to v20130303storage, the PropertyBag is checked to see if it contains MiddleName:\npackage v20120202storage import vnext \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the storage Hub version. func (person *Person) ConvertToStorage(dest vnext.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName if middleName, ok := PropertyBag.ReadString(\u0026#34;MiddleName\u0026#34;); ok { dest.MiddleName = middleName // *** New property copied too ***  } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(vnext storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.WriteString(\u0026#34;MiddleName\u0026#34;, source.MiddleName) return nil } Version Map #  A graph of our conversions now starts to show the chaining between storage versions that gives the name to this approach. Bidirectional conversions to and from earlier versions of storage allow conversion between any pairs of API versions.\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // ** Only Id is unchanged ***  FullName string FamilyName string KnownAs string } This is a preview version, but it still gets a dedicated storage version, v20140404previewStorage. The official hub version is left unchanged as v20130303storage.\nStorage Conversion #  The new properties don\u0026rsquo;t exist on prior storage versions, so the generated ConvertToStorage() and ConvertFromStorage() methods used to convert between v20130303storage and v20140404previewStorage must use the PropertyBag to carry the properties:\npackage v20140404previewStorage import vprior \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vprior.Person) error { dest.Id = person.Id // *** Store in the property bag ***  dest.WriteString(\u0026#34;FamilyName\u0026#34;, person.FamilyName) dest.WriteString(\u0026#34;FullName\u0026#34;, person.FullName) dest.WriteString(\u0026#34;KnownAs\u0026#34;, person.KnownAs) return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vprior.Person) error { person.Id = source.Id // *** Read from the property bag ***  if familyName, ok := source.ReadString(\u0026#34;FamilyName\u0026#34;);ok { person.FamilyName = familyName } if fullName, ok := source.ReadString(\u0026#34;FullName\u0026#34;); ok { person.FullName = fullName } if knownAs, ok := source.ReadString(\u0026#34;KnownAs\u0026#34;); ok { person.KnownAs = knownAs } return nil } In the example above, we show first copying all the directly supported properties, then using the property bag. We may not separate these steps in the generated code.\nThese methods are always generated on the storage versions furthest from the hub version, converting towards that version. In the usual case we\u0026rsquo;ll use the import name vnext (or equivalent) but in this case, given we have a preview version, we\u0026rsquo;ll use vprior to emphasize the direction of conversion.\nThis provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of a Person written by the preview release will have no values for FirstName, LastName, and MiddleName. Similarly, an older version won\u0026rsquo;t have FamilyName, FullName nor KnownAs.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding the semantic changes between versions.\nTo allow injection of manual conversion steps, interfaces will be generated as follows:\npackage v20130303storage // AssignableToPersonV20130303 provides methods to augment conversion to storage type AssignableToPersonV20130303 interface { AssignToV20130303(person Person) error } // AssignableFromPersonV20130303 provides methods to augment conversion from storage type AssignableFromPersonV20130303 interface { AssignFromV20130303(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment the generated conversion.\n Outstanding Issue: The interfaces and methods shown above include the version number of the target in order to disambiguate between versions. This is necessitated by having multiple storage versions in flight at the same time, and needing to avoid name collisions. Contrast this with the rolling storage version case study where there\u0026rsquo;s only one active storage version at a time.\nIs there a way we could structure this approach to avoid the need for version numbers in method names?\n The generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPersonV20130303); ok { assignable.AssignToV20130303(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPersonV20130303); ok { assignable.AssignFromV20130303(source) } return nil } Version Map #  Preview releases, by definition, include unstable changes that may differ once the feature reaches general availability.\nWe don\u0026rsquo;t want to make changes to our storage versions based on these speculative changes, so we handle persistence of the preview release with the existing storage version, by way of a down-conversion to v20130303storage:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } As usual, a custom storage version is generated:\npackage v20140404storage type Person struct { PropertyBag AlphaKey *string FamilyName *string LegalName *string Id *Guid KnownAs *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods between the API version v20140404 and the storage version v20140404storage are trivial and not shown.\nFor conversions between storage versions, the preview storage version is not considered - it\u0026rsquo;s out of the main line of processing. Instead, we have a bidirectional conversion between v20130303storage and v20140404storage. As usual, the conversion is implemented further away from the (new) hub version, on v20130303storage.\nWith a large difference in structure between the two versions, the PropertyBag gets a workout:\npackage v20130303storage import vnext \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vnext.Person) error { dest.Id = person.Id dest.WriteString(\u0026#34;FirstName\u0026#34;, person.FirstName) dest.WriteString(\u0026#34;LastName\u0026#34;, person.LastName) dest.WriteString(\u0026#34;MiddleName\u0026#34;, person.MiddleName) if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vnext.Person) error { person.Id = source.Id if firstName, ok := source.ReadString(\u0026#34;FirstName\u0026#34;); ok { person.FirstName = firstName } if middleName, ok := source.ReadString(\u0026#34;MiddleName\u0026#34;); ok { person.MiddleName = middleName } if lastName, ok := source.ReadString(\u0026#34;LastName\u0026#34;); ok { person.LastName = lastName } // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPersonV20140404); ok { assignable.AssignFromV20140404(source) } return nil } To interoperate between different versions of Person, we need to add manual conversions between the storage versions where the schema change occcurs.\nWhen we are converting from v20130303storage up to v20140404storage, we need to use FirstName, LastName and MiddleName to populate AlphaKey, FamilyName, KnownAs and LegalName.\nConversely, When we are converting from v20140404storage down to v20130303storage, we need to use AlphaKey, FamilyName, KnownAs and LegalName to populate FirstName, LastName and MiddleName.\nThese conversions occur in addition to use of the PropertyBag to store those same properties.\npackage v20130303storage import vnext \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignToV20140404(dest vnext.Person) error { if dest.KnownAs == \u0026#34;\u0026#34; { dest.KnownAs = person.FirstName } if dest.FamilyName == \u0026#34;\u0026#34; { dest.FamilyName = person.LastName } if dest.LegalName == \u0026#34;\u0026#34; { dest.LegalName = person.FirstName +\u0026#34; \u0026#34;+ person.MiddleName + \u0026#34; \u0026#34; + person.LastName } if dest.AlphaKey == \u0026#34;\u0026#34; { dest.AlphaKey = person.lastName } } func (person *Person) AssignFrom(source vNext.Person) error { if person.FirstName == \u0026#34;\u0026#34; { person.FirstName = source.KnownAs } if person.LastName == \u0026#34;\u0026#34; { person.LastName = source.FamilyName } if person.MiddleName == \u0026#34;\u0026#34; { person.MiddleName = // ... elided ...  } } For each property we need to consider that it might have already been populated with a more accurate value from the PropertyBag, so we only synthesize values when needed.\nVersion Map #  We can see in our version map that the preview release is still supported, but the associated storage version is not in the main chain of interconvertible versions.\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected, a matching storage version is also generated:\npackage v20150505storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string SortKey *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods that are generated between the v20140404storage and v20150505storage versions:\npackage v20140404 import vNext \u0026#34;v20150505storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vNext.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vNext.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } While SortKey appears at the end of the list of assignments in the first method, the mirror assignment of AlphaKey appears at the start of the list in the second method. In both cases the properties are shown in alphabetical order.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nFor users staying up to date with releases of the service operator, this will likely have no effect - but users still using the original release (storage version v2011-01-01storage) will need to update to an intermediate release before adopting this version.\nAn alternative approach would be to always support conversion from every storage version, even if the related API version has been dropped:\nThis would allow users to upgrade from almost any older version of the service operator. (\u0026ldquo;Almost\u0026rdquo; because we would still have older versions drop off when they are retired by ARM.)\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions; 17 of these involved changes to letter case alone. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs that make up our storage version:\npackage v20160606storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string MailingAddress *Address // *** New ***  SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type of Person as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods between the API version v20160606 and the storage version v201606061 get generated in the expected way:\npackage v20160606 import storage \u0026#34;v20160606storage\u0026#34; // ConvertTo converts this Person to the Storage version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest storage.Address) error { dest.City = address.City dest.Street = address.Street if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(storage.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source storage.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version drop out, allowing users of the three prior versions of the service operator to upgrade cleanly:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage versions are identical to those used previously and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import storage \u0026#34;v20170707storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional field that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  How often does optionality change? #  At the time of writing, there are 100 version-to-version changes where fields became optional in the later version of the API, and 99 version-to-version changes where fields became required.\nVersion 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version is generated to match, with prior conversions using the property bag to store additional properties:\npackage v20180808storage type Address struct { PropertyBag City *string Country *string FullAddress *string PostCode *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nUsers still running earlier releases of the service operator that are using 2014-04-04 or earlier will need to install an intermediate release in order to upgrade to this one.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Latitude double Longitude double } The storage version gets generated in a straightforward way:\npackage v20190909storage type Location struct { PropertyBag City *string Country *string FullAddress *string Latitude *double Longitude *double PostCode *string } Storage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename of the type (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20180808storage // *** Updated storage version *** import vNext \u0026#34;v20190909storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vNext.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;vNext.Location{} // ** New Type ***  err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest vNext.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":22,"href":"/azure-service-operator/design/versioning/case-studies/fixed-storage-version/","title":"Fixed Storage Version","section":"Case Studies","content":"Case Study - Fixed Storage Version #  This case study explores the alternative solution of using a fixed storage version where the schema of the storage version is modified to handle each additional release.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus on specific details, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v1 type Person struct { FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} By convention, a fixed storage version is noted as v1\nEvery property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we are only concerned with a single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import \u0026#34;v1\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*v1.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest v1.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*v1.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } These four methods will be automatically generated in order to handle much of the boilerplate required for conversion.\nConversion in each direction is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nSince they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map doesn\u0026rsquo;t look much like the traditional hub and spoke model, but this will change as we work through this case study:\nVersion 2012-02-02 - No Change #  In this release of the CRM service, despite changes elsewhere in the service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  Conversions between version v20120202 and the v1 storage version will be identical to those generated for the earlier v20110101 version.\nVersion Map #  Our hub and spoke diagram is becoming useful for seeing the relationship between versions:\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The storage version is updated with the addition of the new property:\npackage v1 type Person struct { FirstName *string Id *Guid LastName *string MiddleName *string // *** New *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import \u0026#34;v1\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are essentially unchanged.\nVersion Map #  A graph of our conversions now starts to show the expected hub and spoke structure:\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // *** Only Id is unchanged from the prior version ***  FullName string FamilyName string KnownAs string } The storage version gets modified to add these new properties:\npackage v1 type Person struct { FamilyName *string // *** New ***  FirstName *string FullName *string // *** New ***  Id *Guid KnownAs *string // *** New ***  LastName *string MiddleName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The new properties are copied to and from the storage version. Ensuring that all properties are optional makes it possible to leave the unused properties empty.\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.FullName = person.FullName dest.Id = person.Id dest.KnownAs = person.KnownAs return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FamilyName = source.FamilyName person.FullName = source.FullName person.Id = source.Id person.KnownAs = source.KnownAs return nil } This provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of Person written by the preview release will have no values for FirstName, LastName, and MiddleName.\nSimilarly, the storage version of Person written by an earlier release will have no values for KnownAs, FamilyName, or FullName.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding of the semantic changes between versions.\nTo allow injection of manual conversion steps, two interfaces will be generated as follows:\npackage v1 // AssignableToPerson is implemented on an API version // of `Person` to update the storage version type AssignableToPerson interface { AssignTo(person Person) error } // AssignableFromPerson is implemented on an API version // of `Person` to populate it from the storage version type AssignableFromPerson interface { AssignFrom(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment (not replace) the generated conversion.\nThe generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { // … elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { // … elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } Implementations of these interfaces are called after the generated boilerplate conversion has completed. This gives developers freedom to make any changes required as they won\u0026rsquo;t be overwritted by the effects of generated code.\nVersion Map #  The preview version just appears as another version in our hub and spoke diagram:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } The two new properties need to be added to the storage version to allow this to be stored:\npackage v1 type Person struct { Id *Guid AlphaKey *string // *** New ***  FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string // *** New ***  MiddleName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Issue: Property Bloat #  As our API evolves over time, our storage version is accumulating all the properties that have ever existed, bloating the storage version with obsolete properties that are seldom (if ever) used.\nFor example, only preview users would ever have used FullName as it became LegalName in the general release; most users will never use FullName.\nWe have a problem, however. We can\u0026rsquo;t remove the FullName property as that is a breaking change that will negatively impact users who have used the preview version. Both properties will need to be retained permanently.\nThis violates the pay for play principle - even users who adopt the operator after the 2014-04-04 release will have to deal with the complexity even though they\u0026rsquo;ve never used the preview version.\nStorage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods for the new version of Person are generated as expected, copying across values and invoking the AssignableToPerson and AssignableFromPerson interfaces if present:\npackage v20140404 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.AlphaKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } For older versions of Person, the conversion methods are essentially unchanged as the properties they require are still present on the storage version.\nTo interoperate between different versions of Person, we need to add some manual conversions.\nWhen a newer version of Person is written to storage, we need to also populate FirstName, LastName and MiddleName to allow older versions to be requested. Similarly, when an older version of Person is written, we need to populate AlphaKey, FamilyName, KnownAs and LegalName so that newer versions can be requested.\nTo avoid repetition of code across multiple implementations of AssignTo() and AssignFrom(), we write some helper methods on the storage version:\npackage v1 func (person *Person) PopulateFromFirstMiddleLastName(firstName string, middleName string, lastName string) { person.KnownAs = firstName person.FamilyName = lastName person.LegalName = firstName +\u0026#34; \u0026#34;+ middleName + \u0026#34; \u0026#34; + lastName person.AlphaKey = lastName } func (person *Person) PopulateLegacyProperties() { person.FirstName = person.KnownAs person.FamilyName = person.FamilyName person.MiddleName = // ... elided ... } These methods are manually authored, so the names are arbitary. However, we will provide some guidance for implementers to encouarage consistency as the operator is updated and improved.\nWith these methods available, implementing the interface AssignableToPerson becomes straightforward. For the 2011-01-01 release of Person:\npackage v20110101 import v1 func (person *Person) AssignTo(dest v1.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, \u0026#34;\u0026#34;, person.LastName) return nil } For the 2013-03-3 release that introduced MiddleName the code is very similar:\npackage v20130303 import v1 func (person *Person) AssignTo(dest v1.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, person.MiddleName, person.LastName) return nil } Version Map #  We can see in our version map that the preview release is still supported:\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected the storage version is also extended:\npackage v1 type Person struct { Id *Guid AlphaKey *string // ** Debris ***  FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string MiddleName *string SortKey *string // ** New *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Again, we see the issue of property bloat where the storage type needs to have both AlphaKey and SortKey for backward compatibility.\nStorage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods, as shown here for the 2014-04-04 version of Person:\npackage v20140404 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.AlphaKey // *** Store it the old way ***  dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } For forward compatibility, the ConvertToStorage() method for version 2014-04-04 populates both AlphaKey and SortKey.\nFor the 2015-05-05 release of Person, the methods are similar:\npackage v20150505 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.SortKey // *** Back compatibility ***  dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.SortKey // *** New ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName person.SortKey = source.SortKey // *** New ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } Here we can see the 2015-05-05 version of ConvertToStorage() populates AlphaKey for backwards compatiblity.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nOf these 17 of these involved changes to letter case alone.\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with a mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs used in storage:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string MailingAddress *Address // *** New ***  MiddleName *string SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods get generated in the expected way:\npackage v20160606 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest v1.Address) error { dest.Street = address.Street dest.City = address.City if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.SortKey // *** Rename still is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(source.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source v1.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version (2012-02-02) drop out:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage version is identical to that generated used previously (because all properties are marked as optional anyway) and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional property that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  Note that the 2013-03-03 version has now dropped out:\nHow often do optionality changes happen? #  At the time of writing, there are 100 version-to-version changes where properties became optional in the later version of the API, and 99 version-to-version changes where properties became required.\nIssue: Property Amnesia #  Our code generator only knows about properties defined in current versions of the API. Once an API version has been excluded (or if the JSON schema definition is no longer available), the generator completely forgets about older properties.\nThis means that we now have a significant issue - with the versions 2011-01-01, 2012-02-02 and 2013-03-03 no longer included, we can no longer fully generate the expected storage version automatically.\nThe properties FirstName, MiddleName and LastName no longer exist - they are only defined on the earliest versions of the API.\nAs a consequence, the storage version generated for this release will be this:\npackage v1 type Person struct { AlphaKey *string FamilyName *string FullName *string Id *Guid KnownAs *string LegalName *string MailingAddress *Address SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} References to FirstName, MiddleName and LastName will disappear across all the generated ConvertFromStorage() and ConvertToStorage() methods as well.\nThis is a breaking change. Existing long time users of the service operator, with CRDs serialized containing any of the properties FirstName, MiddleName and LastName will find that their clusters break when this version of the service operator is deployed.\nWithout explicit intervention in the operator, their only mitigation would be to revert back to the earlier version of the service operator and never upgrade it ever again.\nThe root cause of the problem is that the CRD has never been modified (upgraded) - once first deployed the resource is never modified, only loaded and reconciled.\nTo retain backward compatibility we would need to manually merge the generated code with prior versions to retain both the property definitions and the conversion support previously generated.\nThere are many problems with this approach:\n Violates the goal of avoiding changes to generated files. Merging these changes would be tedious and error prone. Changes would need to be re-merged for every subsequent release of the service operator. Manual conversions would need to be written for newer API versions. Over time, the number of classes requiring manual attention would grow.  Version 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version is updated with the additional properties:\npackage v1 type Address struct { City *string Country *string // *** New ***  FullAddress *string // *** New ***  PostCode *string // *** New ***  Street *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nDropping those releases triggers a reccurrance of the Property Amnesia issue discussed above - the FullName property (only included in the 2014-04-04preview release) has been forgotten.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Lattitude double Longitide double } The storage version for Location gets generated in a straightforward way:\npackage v1 type Location struct { City *string Country *string FullAddress *string PostCode *string } We also need to retain the Address struct - if we drop support for it, we will be breaking existing users as we will be unable to deserialize their resources.\ntype Address struct { City *string Country *string FullAddress *string PostCode *string Street *string } Issue: Type collision #  We run into a problem with the storage version of the Person type:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Address // *** Existing property ***  MailingAddress *Location // *** New property with the same name ***  SortKey *string } We can\u0026rsquo;t have two properties with the same name, either we create a single property that handles both types, or we must change the name of one of them.\nWe can\u0026rsquo;t change the name of the existing property, because that would break existing users who already have serialised resources. For similar reasons, we can\u0026rsquo;t change the type of the existing property.\nSo we need to change the name of the new property - and need to do so in a deterministic way to ensure that we generate the same code each time. One way would be to suffic the type name with the version:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Address MailingAddress_v20190909 *Location SortKey *string } Needing to do this is a wart, but one with a nasty sting in the tail:\nWhen the original MailingAddress property ages out of the system (see property amnesia, above), we\u0026rsquo;ll no longer have a collision, and the storage struct will be generated with this structure:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Location SortKey *string } This is a major breaking change.\nNot only will this break older users who have serialized resources using Address, but it will also break newer users who have serialized resources using Location.\nStorage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20170707 // *** Updated storage version *** import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;storage.Location{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest v1.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":23,"href":"/azure-service-operator/design/versioning/case-studies/rolling-storage-versions/","title":"Rolling Storage Versions","section":"Case Studies","content":"Case Study - Rolling Storage Versions #  This case study explores the recommended solution of using a rolling storage version where we update the storage schema of each resource each release of the service operator. We\u0026rsquo;ll keep the storage version up to date with the latest GA release of each resource.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v20110101storage type Person struct { PropertyBag FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Every property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we currently have only single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nThe PropertyBag type provides storage for other properties, plus helper methods. It is always included in storage versions, but in this case will be unused. The method Hub() marks this version as the storage schema.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import storage \u0026#34;v20110101storage\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest storage.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } Conversion is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nThese methods will be automatically generated in order to handle the majority of the required conversions. Since they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map doesn\u0026rsquo;t look much like the traditional hub and spoke model, but this will change as we work through this case study:\nVersion 2012-02-02 - No Change #  In this release of the CRM service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  Conversions with the upgraded storage version will need to be trivially modified by changing the import statements for the referenced types.\nVersion Map #  Our hub and spoke diagram is becoming useful for seeing the relationship between versions:\nObserve that the prior storage version is still shown, with a one way conversion to the current storage version. Existing users who upgrade their service operator will have their storage upgraded using this conversion. The conversion between storage versions will be generated with the same approach, and with the same structure, as all our other conversions.\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The new storage version, based on this version, updates accordingly:\npackage v20130303storage type Person struct { PropertyBag Id *Guid FirstName *string MiddleName *string // *** New storage ***  LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order. This is to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are essentially unchanged. The import statement at the top of the file will be updated to the new storage version; no other changes are necessary.\nVersion Map #  A graph of our conversions now starts to show the expected hub and spoke structure, with conversions from earlier versions of storage allowing easy upgrades for existing users of the service operator.\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // ** Only Id is unchanged ***  FullName string FamilyName string KnownAs string } This is a preview version, so the storage version is left unchanged (see below).\nStorage Conversion #  The new properties don\u0026rsquo;t exist on the storage version of Person, so the generated ConvertToStorage() and ConvertFromStorage() methods use the PropertyBag to carry the properties:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.Id = person.Id // *** Store in the property bag ***  dest.WriteString(\u0026#34;FamilyName\u0026#34;, person.FamilyName) dest.WriteString(\u0026#34;FullName\u0026#34;, person.FullName) dest.WriteString(\u0026#34;KnownAs\u0026#34;, person.KnownAs) return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.Id = source.Id // *** Read from the property bag ***  person.FamilyName = source.ReadString(\u0026#34;FamilyName\u0026#34;) person.FullName = source.ReadString(\u0026#34;FullName\u0026#34;) person.KnownAs = source.ReadString(\u0026#34;KnownAs\u0026#34;) return nil } In the example above, we show first copying all the directly supported properties, then using the property bag. We may not separate these steps in the generated code.\nThis provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of Person written by the preview release will have no values for FirstName, LastName, and MiddleName.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding of the semantic changes between versions.\nTo allow injection of manual conversion steps, interfaces will be generated as follows:\npackage v20130303storage // AssignableToPerson provides methods to augment conversion to storage type AssignableToPerson interface { AssignTo(person Person) error } // AssignableFromPerson provides methods to augment conversion from storage type AssignableFromPerson interface { AssignFrom(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment the generated conversion.\nThe generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableTo); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFrom); ok { assignable.AssignFrom(source) } return nil } Version Map #  Preview releases, by definition, include unstable changes that may differ once the feature reaches general availability.\nWe don\u0026rsquo;t want to make changes to our storage versions based on these speculative changes, so we handle persistence of the preview release with the existing storage version:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } No longer being a preview release, the storage version is also regenerated:\npackage v20140404storage type Person struct { PropertyBag AlphaKey *string FamilyName *string LegalName *string Id *Guid KnownAs *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods for the new version of Person are generated as expected, copying across values and invoking the AssignableToPerson and AssignableFromPerson interfaces if present:\npackage v20140404 import storage \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.AlphaKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.AlphaKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } The changes to the structure of Person mean that our prior conversion methods need to change too. For properties that are no longer present on the storage version, they now need to use the PropertyBag to stash the required values.\nFor example, the 2011-01-01 version of Person now has these conversion methods:\npackage v20110101 import storage \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.Id = person.Id dest.WriteString(\u0026#34;FirstName\u0026#34;, person.FirstName) dest.WriteString(\u0026#34;LastName\u0026#34;, person.LastName) if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.Id = source.Id person.FirstName = source.ReadString(\u0026#34;FirstName\u0026#34;) person.LastName = source.ReadString(\u0026#34;LastName\u0026#34;) if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } To interoperate between different versions of Person, we need to add manual conversions.\nWhen a newer version of Person is written to storage, we need to also populate FirstName, LastName and MiddleName within the PropertyBag to allow older versions to be requested.\nWhen an older version of Person is written, we need to populate AlphaKey, FamilyName, KnownAs and LegalName so that newer versions can be requested.\nTo avoid repetition of code across multiple implementations of AssignTo() and AssignFrom(), we might write some helper methods on the storage version:\npackage v20140404storage func (person *Person) PopulateFromFirstMiddleLastName(firstName string, middleName string, lastName string) { person.KnownAs = firstName person.FamilyName = lastName person.LegalName = firstName +\u0026#34; \u0026#34;+ middleName + \u0026#34; \u0026#34; + lastName person.AlphaKey = lastName } func (person *Person) PopulateLegacyFields() { person.WriteString(\u0026#34;FirstName\u0026#34;, person.KnownAs) person.WriteString(\u0026#34;LastName\u0026#34;, person.FamilyName) person.WriteString(\u0026#34;MiddleName\u0026#34;, ... elided ...) } With these methods available, implementing the interface AssignableToPerson becomes straightforward. For the 2011-01-01 release of Person:\npackage v20110101 import storage \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignTo(dest storage.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, \u0026#34;\u0026#34;, person.LastName) } For the 2013-03-03 release that introduced MiddleName the code is very similar:\npackage v20130303 import storage \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignTo(dest storage.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, person.MiddleName, person.LastName) return nil } Version Map #  We can see in our version map that the preview release is still supported, but is now backed by the GA release of the version:\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected the storage version is also regenerated:\npackage v20150505storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string SortKey *string // *** Used to be AlphaKey *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods, as shown here for the 2014-04-04 version of Person:\npackage v20140404 import storage \u0026#34;v20150505storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } While SortKey appears at the end of the list of assignments in the first method, the mirror assignment of AlphaKey appears at the start of the list in the second method.\nIssue: Instability of manual conversions #  The earlier manually authored conversions for AlphaKey will also need to be modified. While this change looks simple, it\u0026rsquo;s a symptom of an underlying problem: with each release, the map of required conversions is completely new (no reuse of older conversions.\nThis both requires the introduction of additional conversions to support older versions (as has happened here) and the modification of existing conversions.\nTo illustrate, consider the manual code (AssignTo() and AssignFrom()) that was written to augment conversion between v20110101.Person and v20140404storage.Person.\nNow that we\u0026rsquo;ve moved to a new release, there is no direct conversion between those two versions (see the version map below) - so the manual conversion just drops off and is ignored. If this is not detected, we may end up corrupting resource definitions as they are converted.\nIn many cases, updating manual conversion code will only require changing imported package references, but this does introduce risk as it involves modifying the code, even if trivially.\nThere will certainly also be cases where the conversion is much harder to convert.\nWe also have the issue seen above where introduction of a change requires additional conversions to be written for older versions.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nFor users staying up to date with releases of the service operator, this will likely have no effect - but users still using the original release (storage version v2011-01-01storage) will need to update to an intermediate release before adopting this version.\nAn alternative approach would be to always support conversion from every storage version, even if the related API version has been dropped. This would allow users to upgrade from any older version of the service operator.\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions; 17 of these involved changes to letter case alone. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs that make up our storage version:\npackage v20160606storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string MailingAddress *Address // *** New ***  SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type of Person as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods get generated in the expected way:\npackage v20160606 import storage \u0026#34;v20160606storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest storage.Address) error { dest.City = address.City dest.Street = address.Street if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(storage.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source storage.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version drop out, allowing users of the three prior versions of the service operator to upgrade cleanly:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage versions are identical to those used previously and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import storage \u0026#34;v20170707storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional field that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  How often does optionality change? #  At the time of writing, there are 100 version-to-version changes where fields became optional in the later version of the API, and 99 version-to-version changes where fields became required.\nVersion 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version changes to match, with prior conversions using the property bag to store additional properties:\npackage v20180808storage type Address struct { PropertyBag City *string Country *string FullAddress *string PostCode *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nUsers still running earlier releases of the service operator that are using 2014-04-04 or earlier will need to install an intermediate release in order to upgrade to this one.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Latitude double Longitude double } The storage version gets changed in a straightforward way:\npackage v20190909storage type Location struct { PropertyBag City *string Country *string FullAddress *string Latitude *double Longitude *double PostCode *string } Storage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20170707 // *** Updated storage version *** import storage \u0026#34;v20190909storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;storage.Location{} // ** New Type ***  err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest storage.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":24,"href":"/azure-service-operator/introduction/tutorial-cosmosdb/","title":"Tutorial: CosmosDB to-do List","section":"User’s Guide","content":"Tutorial: CosmosDB to-do List #  Follow the guided example to create a to-do list application backed by CosmosDB. The CosmosDB is hosted in Azure but created easily via kubectl and Azure Service Operator!\n"},{"id":25,"href":"/azure-service-operator/introduction/tutorial-postgresql/","title":"Tutorial: PostgreSQL Votes","section":"User’s Guide","content":"Tutorial: PostgreSQL Votes #  Follow the guided example to create a simple voting application backed by PostgreSQL. The PostgreSQL Server and Database are hosted in Azure but created easily via kubectl and Azure Service Operator!\n"},{"id":26,"href":"/azure-service-operator/design/versioning/","title":"Versioning","section":"Design \u0026 Specifications","content":"Versioning #  Specification for how storage versioning will operate for code generated CRD definitions.\nWe\u0026rsquo;re generating a large number of CRD definitions based on the JSON schema definitions published for Azure Resource Manager use.\nGoals #  Principle of Least Surprise: The goal of the service operator is to allow users to consume Azure resources without having to leave the tooling they are familiar with. We therefore want to do things in the idiomatic Kubernetes fashion, so that they don\u0026rsquo;t experience any nasty surprises.\nAuto-generated conversions: As far as practical, we want to autogenerate the schema for storage use along with the conversions to and from the actual ARM API versions. Hand coding all the required conversions doesn\u0026rsquo;t scale across all the different Azure sevices, especially with the ongoing rate of change.\nAllow for hand coded conversions: While we expect to use code generation to handle the vast majority of needed conversions, we anticipate that some breaking API changes will require part of the conversion to be hand coded. We need to make it simple for these conversions to be introduced while still autogenerating the majority of the conversion. We also want to minimize the need for these conversions to be revisited and maintained over time.\nNo modification of generated files. Manual modification of generated files is a known antipattern that greatly increases the complexity and burden of updates. If some files have been manually changed, every difference showing after code generation needs to be manually reviewed before being committed. This is tedious and error prone because the vast majority of auto generated changes will be perfectly fine. Worse, this process would need to be repeated every time we want to update the operator.\nCompliance with Kubernetes versioning. To quote Kubebuilder\u0026rsquo;s documentation:\n In Kubernetes, all versions must be safely round-tripable through each other. This means that if we convert from version 1 to version 2, and then back to version 1, we must not lose information. Thus, any change we make to our API must be compatible with whatever we supported in v1, and also need to make sure anything we add in v2 is supported in v1.\n Consistency of experience: Early adopters should have a similar experience with the latest release of the service operator as new users who are adopting it for the first time. We don\u0026rsquo;t want early adopters to be penalized for their enthusiasm.\nNon-Goals #  Coverage of every case by code generation: While it\u0026rsquo;s likely that very high coverage will be achievable with code generation, we don\u0026rsquo;t believe that it will be practical to handle every possible situation automatically. It\u0026rsquo;s therefore necessary for the solution to have some form of extensibility allowing for the injection of hand written code.\nOther Constraints #  Unlike the typical situation with a hand written service operator, we don\u0026rsquo;t have complete control over the schema we are publishing for custom resources - we\u0026rsquo;re deriving the CRD schema from the ARM JSON schema published online. This somewhat paints us into a corner where some issues that would be easily avoided with a hand-coded schema have to be faced head on.\nCase Studies #  There are three case studies that accompany this specification, each one walking through one possible solution and showing how it will perform as a synthetic ARM style API evolves over time.\nThe Chained Versions case study shows how the preferred solution adapts to changes as the API is modified.\nThe Rolling Versions case study shows an alternative that works well but falls down when hand coded conversions are introduced between versions.\nThe Fixed Version case study shows how a popular alternative would fare, calling out some specific problems that will occur.\nTL;DR: Using a fixed storage version appears simpler at first, and works well as long as the changes from version to version are simple. However, when the changes become complex (as they are bound to do over time), this approach starts to break down. While there is up front complexity to address with chained storage versions, the approach doesn\u0026rsquo;t break down over time and we can generate useful automated tests for verification. The rolling storage version approach is viable, but requires additional ongoing maintenance when manual conversions are introduced between versions.\nExamples shown in this document are drawn from the case studies.\nProposed Solution #  In summary:\n  For each supported version of an Azure Resource Type, we will define a synthetic storage type type that will be used for serialization of that versions of the API. The latest non-preview version will be tagged as the canonical storage type for Kubernetes.\n  Automatically generated conversions will allow for lossless conversions between the externally exposed API versions of resources and the related storage versions. Additional conversions will be generated to allow upgrade or downgrade between adjacent storage versions.\n  External metadata that we bundle with the code generator will document common changes that occur over time (including property and type name changes), extending the coverage of our automatically generated conversions.\n  For cases where automatically generated conversion is not sufficient, standard extension points for each resource type will allow hand-coded conversion steps to be injected into the process at key points.\n  Each of these four points is expanded upon in detail below.\nDefining Storage Versions #  We\u0026rsquo;ll base the schema of the storage versions on the corresponding API version, with the following modifications:\nAll properties will be defined as optional allowing for back compatibility with prior versions of the API that might not have included specific properties.\nInclusion of a property bag to provide for storage for properties present in other versions of the API that are not present in this version.\nIf a resource type is dropped from later releases of the ARM API, we will still generate a storage type based on the latest available release of that type. We need to do this in order to maintain backward compatibility with existing installations of the service operator.\nUsing a purpose designed types for storage avoids a number of version-to-version compatibility issues that can arise if the API version itself is used directly for storage.\nTo illustrate, if the API version defined the following Person type:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } Then the generated storage (hub) version will be:\npackage v20110101storage type Person struct { PropertyBag Id *Guid FirstName *string LastName *string } Using the latest version of the API as the basis for our storage version gives us maximum compatibility for the usual case, where a user defines their custom resource using the latest available version.\nIf a type has been dropped from the ARM API, we will still generate a storage schema for it based on the last ARM API version where it existed; this helps to ensure backward compatibility with existing service operator deployments. For example, if Person was dropped in favour of Party type (that can capture companies and organizations as well), we will still continue to generate a storage version of Person to allow deserialization by existing service operator installations as a part of their upgrade process.\nSequestering additional properties away within a property bag in the storage schema is more robust than using separate annotations as they are less subject to arbitrary modification by users. This allows us to largely avoid situations where well meaning (but uninformed) consumers of the service operator innocently make changes that result in the operator becoming failing. We particularly want to avoid this failure mode because recovery will be difficult - restoration of the modified/deleted information may be impractical or impossible.\nThe latest non-preview storage version will be selected as the definitive storage version (or hub version) for use by the controller.\nGenerated conversion methods #  Each of the structs generated for ARM API will have the normal ConvertTo() and ConvertFrom() methods generated automatically, implementing the required Convertible interface:\n// ConvertTo converts this Person to the matching storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertFrom converts from the matching storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } As shown, these methods will delegate to two strongly typed helper methods (ConvertToStorage() and ConvertFromStorage()) that are generated to handle the process of copying information across between instances.\nThe ConvertToStorage() method is responsible for copying all of the properties from the API type onto the storage type. The ConvertFromStorage() method is its mirror, responsible for populating all of the properties on the API type from the storage type.\nEach property defined in the API type is considered in turn, and will require different handling based on its type and whether a suitable match is found on the storage type:\nFor properties with a primitive type a matching property must have the same name and the identical type. If found, a simple assignment will copy the value over. If not found, the value will be stashed-in/recalled-from the property bag present on the storage type.\n Primitive types are string, int, float64, and bool Name comparisons are case-insensitive  For properties with an enumeration type a matching property must have the same property name and a type matching the underlying type of the enumeration. If found, a simple assignment will copy the value over with a suitable type cast. If not found, the value will be stashed-in/recalled-from the property bag present on the storage type using the underlying type of the enumeration.\n Name comparisons are case-insensitive for both property names and enumeration type names Enumeration types are generated independently for each version, so they will never be identical types  For properties with a custom type a matching property must have the same name and a custom type with same type name. If found, a new instance will be created and the appropriate ConvertToStorage() or ConvertFromStorage() method for the custom type will be used. If not found, JSON serialization will be used with the property bag for storage.\n Name comparisons are case-insensitive for both property names and custom type names Custom types are generated independently for each version, so they will never be identical types   TODO: Show an example that includes all the cases\n External Metadata for common changes #  We\u0026rsquo;ll capture common changes between versions in metadata (likely a YAML file) that we bundle with the code generator, allowing it to handle a wider range of scenarios.\nIf a property is renamed in a particular API version, conversion from the prior API version to that point of change will instead match based on the new name of the property on the storage type.\nThere are more than 40 cases of properties being renamed across versions of the ARM API.\n TODO: Show an example\n If a type has been renamed in a particular API version, conversion from the API version prior to that point of change will instead match based on the new type of the property on the storage type.\nThere are 160 cases of properties changing type across versions of the ARM API. Many of these can be handled automatically by capturing type renames in metadata.\n TODO: Show an example\n  Outstanding Issue: Are there other kinds of common change we want to support?\nAre there other cases of changes between versions that we may be able to handle automatically. Can we find examples? Do we want to support these cases?\n Standard extension points #  Code generation will include interfaces to allow easy injection of manual conversion steps.\nFor each storage type, two interfaces will be generated, one for each direction of conversion.\nAn AssignableTo* interface for conversion to the storage type will be available for conversions that write to an instance of the storage type.\ntype AssignableToPerson interface { AssignToPerson(person Person) error } Similarly, an AssignableFrom* interface for conversion from the storage type will be available for conversions that read from an instance of the storage type:\ntype AssignableFromPerson interface { AssignFromPerson(person Person) error } If a type (whether API or storage) implements one (or both) of these interfaces, they will be automatically invoked after the standard conversion code has completed, creating an opportunity to augment the standard conversion process.\nTesting #  It\u0026rsquo;s vital that we are able to correctly convert between versions. We will therefore generate a set of unit tests to help ensure that the conversions work correctly. Coverage won\u0026rsquo;t be perfect (as there are conversion steps we can\u0026rsquo;t automatically verify) but these tests will help ensure correctness.\nRound Trip Testing #  We will generate a unit test to ensure that every spoke version can round trip to the hub version and back again to the same version with no loss of information. This will help to ensure a base level of compliance, that information is not lost through serialization.\nThis test targets the following failure modes:\n Edge cases not correctly handled by the generated conversion code. Manually implemented conversions that don\u0026rsquo;t handle some cases correctly.  Each test will work as follows:\n Create an instance of the required type and API version  This will likely be done by using one of the available fuzzing libraries for Go testing   Convert this to the current storage version Convert back from the storage version to a new instance of the original type and API version Verify that all properties are equal  string, int, bool much match exactly Float64 match within tolerance Complex types are recursively matched using the same rules    Relibility Testing #  We will generate unit tests to ensure that every spoke version can be converted to every other spoke version via the hub version without crashing. We lack the semantic context to verify that the conversion is correct, but we can at least verify that it doesn\u0026rsquo;t crash.\nThis test targets the following failure modes:\n Conversions that fail when information is missing (as may happen when converting from earlier versions)  Golden Tests #  For API (spoke) types where the optional interfaces AssignableTo...() and AssignableFrom...() have been implemented, we\u0026rsquo;ll generate golden tests to verify that they are generating the expected results.\nThis test targets the following failure modes:\n Manually implemented conversions that don\u0026rsquo;t handle all the expected edge cases. Manually implemented conversions that fail when given newer (or older) starting versions than expected.  These tests will be particularly useful when a new version of the ARM API is released for a given service as they will help to catch any new changes that now require support.\nWe\u0026rsquo;ll generate two golden tests for each type in each API type, one to test verify conversion to the latest version, and one to test conversion from the latest version.\nTesting conversion to the latest version will check that an instance of a older version of the API can be up-converted to the latest version:\nThe test will involve these steps:\n Create an exemplar instance of the older API type Convert it to the storage type using ConvertToStorage() Convert it to the latest API type using ConvertFromStorage() Check that it matches the golden file from a previous run  Testing will only occur if one (or both) types implements one of the optional interfaces. That is, one or both of the following must be true:\n The older API type implements AssignableTo...() The latest API type implements AssignableFrom...()  If neither rule is satisfied, the test will silently null out.\nTesting conversion from the latest version will check that an instance of the latest version of the API can be down-converted to an older version.\n Create an exemplar instance of the latest API type Convert it to the storage type using ConvertToStorage() Convert it to the older API type using ConvertFromStorage() Check that it matches the golden file from a previous run  Testing will only occur if one (or both) types implements one of the optional interfaces. That is, one or both of the following must be true:\n The older API type implements AssignableFrom...() The latest API type implements AssignableTo...()  If neither rule is satisfied, the test will silently null out.\nConversion Flow #  To illustrate the operation of conversions, consider the following graph of related versions of Person:\nAPI versions are shown across the top, with the associated storage versions directly below. The arrows show the direction of references between the packages, with a package at the start of the arrow importing the package at the end. For example, package v3 imports v3storage and can access the types within.\nThe highlighted storage version v4storage is the currently nominated hub version - all conversions are to or from this type.\nDirect conversion to storage type #  The simplest case is a conversion directly between v4 and v4storage, which simply involves copying properties across:\nTwo step conversion to storage type #  There\u0026rsquo;s no direct conversion between a v3.Person and a v4storage.Person, so an intermediate step is required: we convert first to a v3storage.Person, and then to the final type:\nMultiple step conversion to storage type #  The approach generalizes - at each stage, an intermediate instance is created, one step closer to the current hub type, and the properties are copied across:\nTwo step conversion from storage type #  When converting in the other direction, the process is similar - we show here just the two step case to illustrate.\nAlternative Solutions #  AKA the road not travelled\nAlternative: Fixed storage version #  The \u0026ldquo;v1\u0026rdquo; storage version of each supported resource type will be created by merging all of the fields of all the distinct versions of the resource type, creating a superset type that includes every property declared across every version of the API.\nTo maintain backward compatibility as Azure APIs evolve over time, we will include properties across all versions of the API, even for versions we are not currently generating as output. This ensures that properties in use by older APIs are still present and available for forward conversion to newer APIs, even as those older APIs age out of use.\nThis approach has a number of issues that are called out in detail in the fixed storage version case study.\nProperty Bloat: As our API evolves over time, our storage version is accumulating all the properties that have ever existed, bloating the storage version with obsolete properties that are seldom (if ever) used. Even properties that only ever existed on a single preview release of an ARM API need to be correctly managed for the lifetime of the service operator.\nProperty Amnesia: Our code generator only knows about properties defined in current versions of the API. Once an API version has been excluded (or if the JSON schema definition is no longer available), the generator completely forgets about older properties. This would cause compatibility issues for established users who would find upgrading the service operator breaks their cluster.\nType Collision: Identically named properties with different types can\u0026rsquo;t be stored in the same property; mitigation is possible for a limited time, though eventually property amnesia will cause a breaking change.\nAlternative: Use the latest API version #  The supported storage version of each resource type will simply be the latest version of the API supported by ARM. Any additional information not supported on that version will be persisted via annotations on the resource.\nThis is a known antipattern that we should avoid.\nAnnotations are publicly visible on the cluster and can easily modified. This makes it spectacularly easy for a user to make an innocent change that would break the functionality of the operator.\nMetadata Design #  To support property and type renaming, we will include metadata describing the known changes. This will be added to the existing configuration file we already have that includes filtering information on the types we exclude/include in the output.\nAs identification of renames requires manual inspection, any case of a property not appearing in a later version of the API needs to be checked. We\u0026rsquo;ll therefore also capture metadata for property removal so that we know which properties have been assessed and which have not.\nOutstanding Issues #  Service Operator Upgrades #  There are a number of issues outstanding around upgrades of the service operator.\nTiming - when are upgrades triggered? Does this happen immediately after installation of a new version of the service operator, or does it happen at a later point? If so, what\u0026rsquo;s the trigger?\nAtomicity - are all the CRDs upgraded in one atomic operation that either succeeds or fails, or are they upgraded one at a time? Are CRDs upgraded serially, or in parallel?\nPerformance - for users who have a large number of CRDs (hundreds to thousands), what sort of upgrade performance will they see?\nRecovery - if an upgrade aborts part way through, or if a new version of the service operator proves to be unreliable, is it possible for users to roll back to a previous version, or must they roll forward to a fixed version?\nSee Also #    Hubs, spokes, and other wheel metaphors\n  Falsehoods programmers believe about addresses\n  https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/\n"}]