[{"id":0,"href":"/azure-service-operator/contributing/contributing/","title":"Contributing","section":"For Contributors","content":"Contributing to Azure Service Operator v2 #  Code Structure #  Azure Service Operator v2 #  Code Generator #  Adding a new code-generated resource #  See adding a new code-generator resource.\nDeveloper setup (with VS Code) #  This is the recommended setup, especially if you are using Windows as your development platform.\nThis repository contains a devcontainer configuration that can be used in conjunction with VS Code to set up an environment with all the required tools preinstalled.\nIf you want to use this:\n Make sure you have installed the prerequisites to use Docker, including WSL if on Windows.\n  Install VS Code and the Remote Development extension (check installation instructions there).\n  Run the VS Code command (with Ctrl-Shift-P): Remote Containers: Clone Repository in Container Volume...\nNote: in Windows, it is important to clone directly into a container instead of cloning first and then loading that with the Remote Containers extension, as the tooling performs a lot of file I/O, and if this is performed against a volume mounted in WSL then it is unusably slow.\nTo complete the clone:\n Select \u0026ldquo;GitHub\u0026rdquo;. Search for \u0026ldquo;Azure/azure-service-operator\u0026rdquo;. Choose either of the following options about where to create the volume. The window will reload and run the Dockerfile setup. The first time, this will take some minutes to complete as it installs all dependencies. Run git submodule init and git submodule update    To validate everything is working correctly, you can open a terminal in VS Code and run task -l. This will show a list of all task commands. Running task by itself (or task default) will run quick local pre-checkin tests and validation.\n  Without VS Code #  Option 1: Dockerfile #  The same Dockerfile that the VS Code devcontainer extension uses can also be used outside of VS Code; it is stored in the root .devcontainer directory and can be used to create a development container with all the tooling preinstalled:\n$ docker build $(git rev-parse --show-toplevel)/.devcontainer -t asodev:latest … image will be created … $ # After that you can start a terminal in the development container with: $ docker run --env-file ~/work/envs.env -v $(git rev-parse --show-toplevel):/go/src -w /go/src -u $(id -u ${USER}):$(id -g ${USER}) --group-add $(stat -c '%g' /var/run/docker.sock) -v /var/run/docker.sock:/var/run/docker.sock --network=host -it asodev:latest /bin/bash It is not recommended to mount the source like this on Windows (WSL2) as the cross-VM file operations are very slow.\nOption 2: ./dev.sh #  If you are using Linux, instead of using VS Code you can run the dev.sh script in the root of the repository. This will install all required tooling into the hack/tools directory and then start a new shell with the PATH updated to use it.\nRunning integration tests #  Basic use: run task controller:test-integration-envtest.\nRecord/replay #  The task controller:test-integration-envtest runs the tests in a record/replay mode by default, so that it does not touch any live Azure resources. (This uses the go-vcr library.) If you change the controller or other code in such a way that the required requests/responses from ARM change, you will need to update the recordings.\nTo do this, delete the recordings for the failing tests (under {test-dir}/recordings/{test-name}.yml), and re-run controller:test-integration-envtest. If the test passes, a new recording will be saved, which you can commit to include with your change. All authentication and subscription information is removed from the recording.\nTo run the test and produce a new recording you will also need to have set the required authentication environment variables for an Azure Service Principal: AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET. This Service Principal will need access to the subscription to create and delete resources.\nIf you need to create a new Azure Service Principal, run the following commands:\n$ az login … follow the instructions … $ az account set --subscription {the subscription ID you would like to use} Creating a role assignment under the scope of \u0026quot;/subscriptions/{subscription ID you chose}\u0026quot; … $ az ad sp create-for-rbac --role contributor --name {the name you would like to use} { \u0026quot;appId\u0026quot;: \u0026quot;…\u0026quot;, \u0026quot;displayName\u0026quot;: \u0026quot;{name you chose}\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;{name you chose}\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;…\u0026quot;, \u0026quot;tenant\u0026quot;: \u0026quot;…\u0026quot; } The output contains appId (AZURE_CLIENT_ID), password (AZURE_CLIENT_SECRET), and tenant (AZURE_TENANT_ID). Store these somewhere safe as the password cannot be viewed again, only reset. The Service Principal will be created as a “contributor” to your subscription which means it can create and delete resources, so ensure you keep the secrets secure.\nRunning live tests #  If you want to skip all recordings and run all tests directly against live Azure resources, you can use the controller:test-integration-envtest-live task. This will also require you to set the authentication environment variables, as detailed above.\nRunning a single test #  By default task controller:test-integration-envtest and its variants run all tests. This is often undesirable as you may just be working on a single feature or test. In order to run a subset of tests, use:\nTEST_FILTER=test_name_regex task controller:test-integration-envtest Running the operator locally #  If you would like to try something out but do not want to write an integration test, you can run the operation locally in a kind cluster.\nBefore launching kind, make sure that your shell has the AZURE_SUBSCRIPTION_ID, AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET environment variables set. See above for more details about them.\nRun the following commands to create a kind cluster and deploy the operator, CRDs, and webhooks into it.\n task controller:kind-create - Creates a kind cluster and local Docker registry. task controller:install-cert-manager - Installs cert manager into the kind cluster. task controller:docker-push-local - Builds a Docker image from the local controller code and pushes it to the local Docker registry. task controller:install - Installs the CRDs, webhooks, and operator pod (running the image from the local registry) into the cluster. Create the secret for use by the operator. task controller:make-sp-secret for Service Principal secret, or task controller:install-aad-pod-identity-local and task controller:make-aadpodidentity-secret for an AAD Pod Identity secret.  Once these steps have completed successfully you can use kubectl to interact with the local kind cluster.\nWhen you\u0026rsquo;re done with the local cluster, tear it down with task controller:kind-delete.\nCommon problems and their solutions #  Error loading schema from root #  Full error:\n error loading schema from root \u0026hellip; open /azure-service-operator/v2/specs/azure-resource-manager-schemas/schemas/2019-04-01/deploymentTemplate.json no such file or directory\n This git repo contains submodules. This error occurs when the submodules are missing, possibly because the repo was not cloned with --recurse-submodules.\nTo resolve this problem, run git submodule init and git submodule update and then try building again.\n"},{"id":1,"href":"/azure-service-operator/architecture-decision-records/","title":"Architecture Decision Records","section":"","content":"Architecture Decision Records #  This folder documents some of the architecturally significant decisions we\u0026rsquo;ve made during the Azure Service Operator project.\nFiles are listed chronologically by month, and should contain the following sections:\n Context Decision Status Consequences Experience Report References  For background information, check out this Cognitect blog entry.\nADR documents should be updated over time to keep them relevant.\n"},{"id":2,"href":"/azure-service-operator/architecture-decision-records/adr-2021-02-property-conversions/","title":"2021-02: Property Conversions","section":"Architecture Decision Records","content":"Property Conversions #  Context #  To facilitate the use of dedicated storage variants for persistence of our custom resources, we need to codegen conversion routines that will copy all properties defined on one version of a resource to another version.\nGiven the way resources evolve from version to version, these we need to support a wide range of conversions between types that are similar, but not identical.\nFor example, looking primitive types (such as string, int, and bool):\nA primitive type can become optional in a later version when a suitable default is provided, or when an alternative becomes available.\nAn optional primitive type can become mandatory in a later version when deprecation of other properties leaves only one way to do things.\nA primitive type can be replaced by an enumeration when validation for a property (such as VM SKU) is tightened up to avoid problems.\nA primitive type can be replaced by an alias when additional validation constraints (e.g., limiting bounds or a regular expression) are added to address problems.\nThese transformations (and others) can occur in combination with each other (e.g., a primitive type replaced by an optional enumeration) and with other constructs, such as maps, arrays, resources, and complex object types.\nOther constraints apply, such as the need to clone optional values during copying lest the two objects become coupled with changes to one being visible on the other.\nWhen implementation began, it very quickly became apparent that independently addressing every possible conversion requirement would be difficult to impossible given the combinatorial explosion of possibilities.\nDecision #  Use a recursive algorithm to generate the required conversion by composing simpler conversion steps together.\nFor example, given a need to copy an optional string to an optional enumeration, the process works as follows:\nOriginal problem: *string -\u0026gt; *Sku (where Sku is based on a string)\nThe handler assignFromOptional knows how to handle the optionality of *string, reducing the size of the problem. A recursive call is made to solve the new problem.\nReduced problem: string -\u0026gt; *Sku\nThe handler assignToOptional knows how to handle the optionality of *Sku, reducing the size of the problem further. A recursive call is made to solve the new problem.\nReduced problem #2: string -\u0026gt; Sku\nThe handler assignToAliasedPrimitive recognizes that Sku is an enumeration based on string and reduces the problem. A recursive call is made to solve the new problem.\nReduced problem #3: string -\u0026gt; string\nNow assignPrimitiveFromPrimitive can handle the reduced problem, generating a simple assignment to copy the value across:\ndestination = source Working backwards, the handler assignToAliasedPrimitive injects the required cast to the enumeration\ndestination = Sku(source) Then, assignToOptional injects a local variable and takes its address\nsku := Sku(source) destination = \u0026amp;sku Finally, assignFromOptional injects a check to see if we have a value to assign in the first place, assigning a suitable zero value if we don\u0026rsquo;t:\nif source != nil { sku := Sku(source) destination := \u0026amp;sku } else { destination := \u0026#34;\u0026#34; } Status #  Successfully implemented. First commit in PR# #378\nThe full list of implemented conversions can be found in property_conversions.go.\nConsequences #  It required some finessing of the conversion code to generate high quality conversions; early results were functional but not comparable with handwritten efforts.\nAs we’ve encountered new cases where new transformations are required, the list of conversions has been extended with additional handlers, including:\n•\tSupport for enumerations (PR #392) •\tConversion of complex object types (PR #395) •\tSupport for aliases of otherwise supported types (PR #433) •\tAbility to read values from functions (PR #1545) •\tSupport for JSON properties (PR #1574) •\tProperty Bags for storing/recalling unknown properties (PR# 1682)\nExperience Report #  TBC\nReferences #  TBC\n"},{"id":3,"href":"/azure-service-operator/architecture-decision-records/adr-2020-11-abstract-syntax-trees/","title":"2020-11: AST Library Choice","section":"Architecture Decision Records","content":"Abstract Syntax Tree Library Choice #  Context #  When we first started working on the Azure Service Operator Code Generator, we used the standard go ast library to create the final Go code.\nUnfortunately, we have run into a number of significant limitations with this library.\n  Comments are often emitted in unexpected places.\nThey would often appear at the end of the previous line, which conveys an incorrect meaning to a casual reader of the code. As a workaround, we found that prefacing some comments with newline characters would force them onto the next line. While hacking the library in this function worked, we are concerned that it\u0026rsquo;s a fragile technique that would break in a future version of Go. We also found situations where this technique did not work.\n  Generated code does not always comply with go fmt standards.\nFor some constructs, the generated code would fail the go fmt check included in our continuous integration (CI) builds. We found a workaround that involved using ast to read in a generated file and immediately rewrite it back out again, immitating the results of go fmt.\n  Generated code does not always compile.\nFor some constructs, we found the comments were emitted at the start of the line of code (not on the previous line), resulting in the code being commented out. We found no workaround for this.\n  Poor control over whitespace\nWe were unable to find a reliable technique within ast to introduce blank lines betweens stanza of code, or before comment blocks. As a workaround, we would read in the formatted code, scan it for comments and manually inject blank lines before each comment block.\n  We have come to the conclusions that the ast library is intended as a way to make minor modifications to existing well formatted Go files, not for the creation of entirely new Go files from scratch.\nA potential alternative is the dst (Decorated Syntax Tree) package. This package was specifically created to address the kinds of issues described above.\nDecision #  After some informative trials, we have decided to adopt the dst library.\nStatus #  Adopted.\nInitial change committed in PR#366.\nConsequences #  With a very high level of API compatibility, we were able to introduce dst with a low level of code churn by aliasing the import as ast in most files. Future changes should remove this aliasing as files are modified.\nThe DST library requires that any node be used exactly once, and will panic if a node is reused in mulitiple locations. We\u0026rsquo;ve mitigated this by liberally cloning nodes as we build the desired final syntax tree. (See astbuilder.Expressions(), introduced in PR#1613 and astbuilder.Statements(), introduced in PR#427).\nExperience Report #  TBC\nReferences #  Go AST (Abstract Syntax Tree) library\nDST (Decorated Syntax Tree) library\n"},{"id":4,"href":"/azure-service-operator/architecture-decision-records/adr-2020-07-pipeline-architecture/","title":"2020-07: Pipeline Architecture","section":"Architecture Decision Records","content":"Pipeline architecture #  Context #  As the complexity of code generation grew, and with a partial view of the complexity yet to come, it became clear that we were facing a challenge. Each additional required function was making integration and testing progressively more difficult.\nWe also have the requirement to support a parallel pipeline that generates code appropriate for CrossPlane integration, and have a strong desire for an approach that minimizes the amount of ongoing maintenance.\nIntroducing a formal pipeline architecture, where each function forms a distinct pipeline stage, solves these problems.\n  Each pipeline stage can (in theory, at least) be tested independently, improving our confidence in the functionality\n  Dependencies between stages can be explicitly declared, allowing the validity of the pipeline to be checked\n  A single factory method can create both required pipeline variants\n  Decision #  Adopted.\nStatus #  Pipeline introduced in PR#171.\nStage prerequisites introduced in PR#366 and postrequisites in PR#1541.\nConsequences #  Individual pipeline stages are independently tested, but some require considerable set up to create the expected initial state required for execution. The addition of mini-pipelines in PR#1649 partially mitigates this.\nReferences #  Pipeline (software) on Wikipedia\n"},{"id":5,"href":"/azure-service-operator/architecture-decision-records/adr-2020-04-code-generation/","title":"2020-04: Why Code Generation?","section":"Architecture Decision Records","content":"Why Code Generation? #  Context #  The Azure Service Operator CSE team had successfully developed support for a handful of ARM (Azure Resource Manager) based custom resources over the course of eighteen months or so.\nTheir experience yielded several useful lessons.\n  Working closely with their customers, they only built out resource support for those features required by the current customer base. While this successfully reduced the amount of work, each additional customer brought new requirements and a need to revisit/enhance existing resources.\n  Adding support for a brand-new resource was often needed by new adopters of ASO and required a significant effort. Extending ASO to have broad support across all (or nearly all) Azure ARM resources would require a major investment\n  A separate controller for each resource type introduced inconsistencies in behavior\n  Multiple people concurrently had the idea of using code generation based on ARM JSON Schema to address these problems:\n  By default, a code generator would mirror the the entire ARM specification for a resource in Kubernetes, providing full feature coverage with consistent naming.\n  The overhead of adding a new resource would be much reduced, potentially limited to as little as testing\n  Using a single generic controller for all resources would ensure consistent behavior\n  Decision #  A proof of concept by David Justice proved the concept sufficiently that we had confidence the idea would work.\nStatus #  Adopted, based on the proof of concept code in the Azure/k8s-infra repository.\nConsequences #  We think it\u0026rsquo;s working.\nExperience Report #  The ARM JSON Schema definitions weren’t sufficient in isolation as they don’t contain all of the information required. Use of Azure Swagger specifications to augment our status types was introduced in PR #205.\nMigration of the code generator into the ASO repo occurred in PR #1427.\nReferences #  None.\n"},{"id":6,"href":"/azure-service-operator/contributing/add-a-new-code-generated-resource/","title":"Add a New Code Generated Resource","section":"For Contributors","content":"Adding a new code generated resource to ASO v2 #  This document discusses how to add a new resource to the ASO v2 code generation configuration. Check out this PR if you\u0026rsquo;d like to see what the end product looks like.\nWhat resources can be code generated? #  Any ARM resource can be generated. There are a few ways to determine if a resource is an ARM resource:\n If the resource is defined in the ARM template JSON schema, or the auto-generated ARM template JSON schema it is an ARM resource. If the resource is defined in a resource-manager folder in the Azure REST API specs repo, it is an ARM resource.  If the resource is not in either of the above places and cannot be deployed via an ARM template, then it\u0026rsquo;s not an ARM resource and currently cannot be code generated.\nDetermine a resource to add #  There are three key pieces of information required before adding a resource to the code generation configuration file, and each of them can be found in the ARM template JSON schema or the auto-generated ARM template JSON schema. To get started, find the entry for the resource in one of the two templates mentioned. For example, the entry for Azure Network Security Groups looks like this: { \u0026quot;$ref\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2020-11-01/Microsoft.Network.json#/resourceDefinitions/networkSecurityGroups\u0026quot; },\nNote: In many cases there will be multiple entries for the same resource, each with a different api-version. It is strongly recommended that you use the latest available non-preview api-version.\n  The name of the resource.\nYou usually know this going in. In our example above, the name of the resource is networkSecurityGroups.\n  The group the resource is in.\nThis is usually named after the Azure service, for example microsoft.resources or microsoft.documentdb. In our example entry from above, this is microsoft.network.\n  The api-version of the resource.\nThis is usually a date, sometimes with a -preview suffix. In our example entry from above, this is 2020-11-01.\n  Adding the resource to the code generation configuration file #  The code generation configuration file is located here. To add a new resource to this file, find the exportFilters section of the file and scroll down until you get to a block of exportFilters for individual resources.\nAdd a new exportFilter of kind include-transitive at the end of that block, right above this section:\n# Exclude everything else as we are operating on an opt-in basis at the moment: - action: exclude because: We don\u0026#39;t want to generate anything else, at the moment. Your export filter should look like this:\n- action: include-transitive group: \u0026lt;group\u0026gt; version: v*api\u0026lt;api-version with dashes removed\u0026gt; name: \u0026lt;resource name, typically just remove the trailing \u0026#34;s\u0026#34;\u0026gt; because: \u0026#34;including \u0026lt;resource\u0026gt;\u0026#34; In the case of our example above, that ends up being:\n- action: include-transitive group: microsoft.network version: v*api20201101 name: NetworkSecurityGroup because: \u0026#34;including NSG\u0026#34; Run the code generator #  Follow the steps in the contributing guide to set up your development environment. Once you have a working development environment, run the task command to run the code generator.\nFix any errors raised by the code generator #  \u0026lt;Resource\u0026gt; looks like a resource reference but was not labelled as one #  Example:\n Replace cross-resource references with genruntime.ResourceReference: [\u0026ldquo;github.com/Azure/azure-service-operator/hack/generated/_apis/microsoft.containerservice/v1alpha1api20210501/PrivateLinkResource.Id\u0026rdquo; looks like a resource reference but was not labelled as one. It might need to be manually added to newKnownReferencesMap,\n To fix this error, determine whether the property in question is an ARM ID or not, and then update the newKnownReferencesMap function in add_cross_resource_references.go.\nIf the property is an ARM ID, update newKnownReferencesMap to flag that property as a reference:\n{ typeName: astmodel.MakeTypeName(configuration.MakeLocalPackageReference(\u0026#34;microsoft.containerservice\u0026#34;, \u0026#34;v1alpha1api20210501\u0026#34;), \u0026#34;PrivateLinkResource\u0026#34;), propName: \u0026#34;Id\u0026#34;, }: true, If the property is not an ARM ID, update newKnownReferencesMap to indicate that property is not a reference by providing the value false instead:\n{ typeName: astmodel.MakeTypeName(configuration.MakeLocalPackageReference(\u0026#34;microsoft.containerservice\u0026#34;, \u0026#34;v1alpha1api20210501\u0026#34;), \u0026#34;PrivateLinkResource\u0026#34;), propName: \u0026#34;Id\u0026#34;, }: false, TODO: expand on other common errors\nExamine the generated resource #  After running the generator, the new resource you added should be in the apis directory.\nHave a look through the files in the directory named after the group and version of the resource that was added. In our NetworkSecurityGroups example, the best place to start is /v2/api/microsoft.network/v1alpha1api20201101/network_security_group_types_gen.go There may be other resources that already exist in that same directory - that\u0026rsquo;s expected if ASO already supported some resources from that provider and API version.\nStarting with the network_security_group_types_gen.go file, find the struct representing the resource you just added. It should be near the top and look something like this:\n// +kubebuilder:object:root=true // +kubebuilder:subresource:status // +kubebuilder:storageversion // +kubebuilder:printcolumn:name=\u0026#34;Ready\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].status\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;Reason\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].reason\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;Message\u0026#34;,type=\u0026#34;string\u0026#34;,JSONPath=\u0026#34;.status.conditions[?(@.type==\u0026#39;Ready\u0026#39;)].message\u0026#34; //Generated from: https://schema.management.azure.com/schemas/2020-11-01/Microsoft.Network.json#/resourceDefinitions/networkSecurityGroups type NetworkSecurityGroup struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec NetworkSecurityGroups_Spec `json:\u0026#34;spec,omitempty\u0026#34;` Status NetworkSecurityGroup_Status_NetworkSecurityGroup_SubResourceEmbedded `json:\u0026#34;status,omitempty\u0026#34;` } Look over the Spec and Status types and their properties (and the properties of their properties and so-on). The Azure REST API specs and ARM template JSON schemas which these types were derived from are not perfect. Sometimes they mark a readonly property as mutable or have another error or mistake.\nIf you do identify properties which should be removed or changed, you can make customizations to the resource in the typeTransformers section of the code generation config. The most common issues have their own sections:\n # Deal with readonly properties that were not properly pruned in the JSON schema # Deal with properties that should have been marked readOnly but weren't  Write a CRUD test for the resource #  The best way to do this is to start from an existing test and modify it to work for your resource. It can also be helpful to refer to examples in the ARM templates GitHub repo.\nRun the CRUD test for the resource and commit the recording #  See the code generator README for how to run recording tests.\nAdd a new sample #  The samples are located in the samples directory. There should be at least one sample for each kind of supported resource. These currently need to be added manually. It\u0026rsquo;s possible in the future we will automatically generate samples similar to how we automatically generate CRDs and types, but that doesn\u0026rsquo;t happen today.\nSend a PR #  You\u0026rsquo;re all done!\n"},{"id":7,"href":"/azure-service-operator/contributing/create-a-new-release/","title":"Create a New Release","section":"For Contributors","content":"Creating a new release of ASO v2 #   Go to the releases page and draft a new release. In the tag dropdown, type the name of the new tag you\u0026rsquo;d like to create (it should match the pattern of previous releases tags, for example: v2.0.0-alpha.1). The release target should be main (the default). If publishing an alpha/beta, be sure to mark the release as a pre-release. Write a \u0026ldquo;Release Notes\u0026rdquo; section. This can be generated by looking through the commits between the last release and now: git log v2.0.0-alpha.0..main. In the future it would be nice to automatically generate this. Publish the release. This will automatically trigger a GitHub action to build and publish an updated Docker image with the latest manager changes. Ensure that the action associated with your release finishes successfully.  Testing the new release #   Download the yaml file from the release page Create a kind cluster: task controller:kind-create Install cert-manager: task controller:install-cert-manager Create the namespace for the operator: k create namespace azureserviceoperator-system Source the SP credentials to use for the secret and then run ./scripts/deploy_testing_secret.sh sp Deploy the operator from MCR: k apply --server-side=true -f \u0026lt;path-to-downloaded-yaml\u0026gt; (We need to use server-side apply because the CRD for VirtualMachines is large enough that it can\u0026rsquo;t fit in the last-applied-configuration annotation client-side kubectl apply uses.) Wait for it to start: k get all -n azureserviceoperator-system Create a resource group: k apply -f v2/config/samples/microsoft.resources/v1alpha1api20200601_resourcegroup.yaml Make sure it deploys successfully, and check in the portal.  Fixing an incorrect release #  If there was an issue publishing a new release, we may want to delete the existing release and try again. Only do this if you\u0026rsquo;ve just published the release and there is something wrong with it. We shouldn\u0026rsquo;t be deleting releases people are actually using.\n Delete the release in the releases page. Delete the tag: git push \u0026lt;origin\u0026gt; --delete \u0026lt;tag\u0026gt;, for example git push origin --delete v2.0.0-alpha.1.  At this point, you can safely publish a \u0026ldquo;new\u0026rdquo; release with the same name.\n"},{"id":8,"href":"/azure-service-operator/design/clarifying-object-structure/","title":"Clarifying Object Structure","section":"Design \u0026 Specifications","content":"Clarifying object structure #  Today we have resources that look like:\napiVersion: microsoft.storage.infra.azure.com/v1alpha1api20190401 kind: StorageAccount metadata: name: samplekubestorage namespace: default spec: azureName: mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS owner: name: k8sinfra-sample-rg accessTier: Hot tags: tag1: tag1 tag2: tag2 The problem #  There\u0026rsquo;s no good way with this object structure to differentiate stuff that is for Azure directly, versus stuff that is for the operator. owner almost falls into this category already, but there are other likely upcoming properties that definitely fall into this category:\n SecretConfiguration: details about where/how we should store secrets created by this entity. Credentials: per object credentials used to support CAPZ-like scenarios.  The problem also manifests in Status where ideally we would distinguish between properties from Azure directly (the result of a GET on the resource) and properties that we are presenting. For example, we may want to have a status field for deploymentId documenting the deployment ID used to create the resource, or the error if there was an error. If we do that there\u0026rsquo;s no easy way for the customer to understand that field is provided by ASO and not by Storage.\nProposal #  We introduce an additional level of hierarchy specifically to clarify what is coming or going directly from or to Azure. This is similar to what Crossplane does (see their MySQLServer). Unlike Crossplane though we will push the operator specific properties down a level and leave the Azure properties at the top level.\nOn the spec we could call this something like forOperator or operator, and on the status fromOperator or operator.\nOur structure would then look like:\napiVersion: microsoft.storage.infra.azure.com/v1alpha1api20190401 kind: StorageAccount metadata: name: samplekubestorage namespace: default spec: azureName: mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS accessTier: Hot tags: tag1: tag1 tag2: tag2 owner: name: k8sinfra-sample-rg operatorSpec: credentials: credentials Similarly, a status might look like:\nstatus: id: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/mykubestorage location: westcentralus kind: BlobStorage sku: name: Standard_LRS accessTier: Hot tags: tag1: tag1 tag2: tag2 operatorStatus: deploymentId: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/Microsoft.Deployments/deployments/1234 goalSeekingState: GoalMet FAQ #  Q: What about the owner field? A: It is such an important field and cuts across both Azure (since it\u0026rsquo;s partially about resource relationships in Azure) and Kubernetes (since it\u0026rsquo;s partially about resource relationships in Kubernetes) so it makes sense to leave it at the top level.\nQ: What about the azureName field? A: It is for Azure, so stays at the top level.\nQ: Do we actually have any properties that need to move TODAY because of this change? A: No, because state and deploymentId are currently resource annotations, not fields in the status, and on the spec the only fields which might belong in operatorSpec are owner and azureName which as per the above FAQ are staying at the top level.\nOpen questions #   What should we call the property which we push things down \u0026ldquo;into\u0026rdquo;? Here are some options:   operator (for both) operatorData (for both) forOperator / fromOperator operatorSpec / operatorStatus ??? / operatorState  "},{"id":9,"href":"/azure-service-operator/design/crossplane/","title":"Crossplane","section":"Design \u0026 Specifications","content":"Thoughts on performing provider-azure code generation for Crossplane #  The patterns identified below were extrapolated from the Crossplane VNET spec.\nGeneral notes #  Crossplane has the following static \u0026ldquo;special\u0026rdquo; properties #   ForProvider - this contains the actual \u0026ldquo;spec\u0026rdquo; as Azure would see it. DeletionPolicy ProviderConfigRef WriteConnectionSecretToRef  Since these are statically shaped they should be relatively easy to add with Crossplane specific generator pipeline stages.\nItems #2-4 are included automatically by embedding the runtimev1alpha1.ResourceSpec from runtimev1alpha1 \u0026quot;github.com/crossplane/crossplane-runtime/apis/core/v1alpha1\u0026quot;.\nCross resource references in Crossplane #   X (e.g. ResourceGroupName) - An actual field that exists in the Azure API. Can be set to any string you\u0026rsquo;d post to the Azure API. XRef (e.g. ResourceGroupNameRef) - A reference to a named Kubernetes object of a preordained kind. Used to resolve a value for X. XNameSelector (e.g. ResourceGroupNameSelector) - A selector used to set XRef.  Note that these values cascade down from the selector, to the ref, to the underlying field. If the ref is already set the selector is ignored, and if the field is already set the ref is ignored.\nUsually these are used to specify parentage for a resource (as in the case of the example above with ResourceGroupName) but may also be used for other cross resource references.\nGenerating these might take a bit of work as we need to derive the name(s) for each of these for each of the parent resources. This should be doable in a pipeline stage by walking up the graph of owners that we already have and getting their names, then generating a set of these for each owner.\nStatus #  Our status' include the \u0026ldquo;full\u0026rdquo; shape of the object, whereas Crossplane\u0026rsquo;s include the things which are not in the spec (basically the server-only fields). As pointed out by @negz, it\u0026rsquo;s a bit weird to duplicate data across Spec and Status like we\u0026rsquo;re currently doing so it may be worth us considering their approach. #269 is tracking this discussion.\nMissing features #  These are features the k8s-infra code generator is currently lacking that would be required to do a good job of generating the Crossplane CRDs.\n#266 Support for embedding structs/interfaces into generated types #  We don\u0026rsquo;t currently support embedding in the generated types, and we will need to in order to support embedding of runtimev1alpha1.ResourceSpec and runtimev1alpha1.ResourceStatus.\nSupport for additionalPrinterColumns in the CRD YAML #  Since these are likely custom per resource I am not sure if we want to generate them or just merge them in with hand-maintained Kustomize files.\n#267 Detection and removal of embedded subresources. #  Some of our CRDs aren\u0026rsquo;t as clear as they should be due to the service teams embedding subresources in the parent resource as properties. For example on VNET we have subnets and virtualNetworkPeerings properties that really probably shouldn\u0026rsquo;t be there (since those are their own sub-resources).\nImprovements in JSON schema/Swagger upstream for marking read-only properties #  This is minor as we can bypass properties using exclusions in the configuration file for now, but ideally we\u0026rsquo;d push this stuff up to the actual \u0026ldquo;source of truth\u0026rdquo; documents.\nAn example of this is: our VNET Spec has provisioningState in the spec which doesn\u0026rsquo;t make sense.\nA plan for secrets #  Right now we don\u0026rsquo;t have a plan for fields which are classified as secret. This will matter for some things such as Microsoft SQL Server. We do already have an issue tracking this at #154.\nSupport for resource references other than ownership #  We have a plan for supporting Cross resource references but don\u0026rsquo;t currently have the data to easily detect them. Ideally we could detect these and then turn them into our resource reference shape, or into Crossplanes.\n#268 Promote \u0026ldquo;Properties\u0026rdquo; Spec property #  This shows up on quite a large number of resources and adds an extra level of nesting that doesn\u0026rsquo;t look great.\nSpecific resources #  VNET #  Note: VNET in Crossplane is a v1alpha1 API and as such doesn\u0026rsquo;t follow all of their best practices (v1beta1 APIs do)\n As mentioned above, our VNET today has subnets and virtualNetworkPeerings, which are really subresources. It also has provisioningState which it shouldn\u0026rsquo;t. If we excluded these properties in the configuration file, we could definitely hack towards generating a VNET that looks like Crossplane\u0026rsquo;s now.  Redis #   Our version of Redis has a Properties field in the spec, which holds a bunch of properties. Some properties (Location, Owner, Tags) are peers of Properties. We should consider promoting the contents of Properties up a level into Spec, as mentioned above. There are a few trivial capitalization differences between our generated Redis and the one that is in Crossplane. For example: EnableNonSSLPort vs our EnableNonSslPort. Crossplane is correct here, but the difference only shows up in the generated code and so isn\u0026rsquo;t a big deal. We generate Enums for Redis.Sku fields Name and Family, whereas Crossplane just uses strings. Neither Crossplane nor we support SubnetId as a reference to a Kubernetes resource, but we probably should. (They have a TODO in the code). Crossplane makes use of a +immutable annotation, presumably to annotate that certain fields cannot be changed once they are set. We should look into doing that as well. #180 is tracking this. It\u0026rsquo;s not clear to me that this annotation actually does anything right now though. Crossplane is making use of a newer version of the Redis API than we see, because the 2018+ versions of the Redis API seem to have not made it into the deployment template we\u0026rsquo;re using as the source of truth. I filed #1237 in the schemas repro tracking this.  Microsoft SQL #   The strange thing with SQL is somehow some of their APIs don\u0026rsquo;t have the full API surface area\u0026hellip; For example I think 2019-06-01-preview exists but it\u0026rsquo;s not in the JSON schema we\u0026rsquo;re using. Other bits are split across API versions but then unioned together in their SDK: 2015-05-01-preview has servers, and 2017-03-01-preview has databases. Their Swagger is equally confusing. As mentioned above, the big issue here is going to be secrets/credentials since we don\u0026rsquo;t have an automated solution for that yet. Other than that, I think servers and databases (which are AFAIK the two interesting resources here) should be doable today.  Plugging into Crossplane framework #  One other area we need to investigate is how to write a generic adapter that takes standard ARM deployments and turns them into something that plugs into the Crossplane framework (CRUD). We might need some discussion with the Crossplane folks here because one thing with deployments is that they are actually their own resource, so a Create VNET operation would have to create a deployment that created the VNET. Once the deployment is done (resource in steady state), we\u0026rsquo;d want a way to conditionally delete the deployment (that doesn\u0026rsquo;t delete the VNET).\nFor those curious, we\u0026rsquo;ve got the scaffolding to do that in our generic controller in PR #250, you can see the workflow here.\n"},{"id":10,"href":"/azure-service-operator/design/defaulter-validator/","title":"Defaulter Validator","section":"Design \u0026 Specifications","content":"Custom validation and defaulting for code generated resources #  Reasoning #  controller-runtime defines admission.Defaulter and admission.Validator. These interfaces only give you a single Default or ValidateX method, which means that all validation/defaulting needs to be done in that method. I think we\u0026rsquo;re going to quickly run into situations where we want custom (handcrafted) validations or defaults for a particular resource and we\u0026rsquo;re not going to want to teach the code generator about these.\nSuggestion #  Strucutre our autogenerated webhooks such that there an ability to override the default behavior by implementing an interface.\nFor defaulting, implement this interface to hook into the autogenerated defaulting process:\ntype Defaulter interface { CustomDefault() } For validation, the interface being implemented should allow easy collation of errors, so we expand the controller-runtime Validator interface to return a collection of validation functions whose errors the autogenerated code can aggeregate:\ntype Validator interface { CreateValidations() []func() error UpdateValidations() []func(old runtime.Object) error DeleteValidations() []func() error } Sample Default:\n// +kubebuilder:webhook:path=/mutate-microsoft-storage-infra-azure-com-v1alpha1api20190401-storageaccount,mutating=true,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=microsoft.storage.infra.azure.com,resources=storageaccounts,verbs=create;update,versions=v1alpha1api20190401,name=default.v1alpha1api20190401.storageaccounts.microsoft.storage.infra.azure.com,admissionReviewVersions=v1beta1  var _ admission.Defaulter = \u0026amp;StorageAccount{} // Default defaults the Azure name of the resource to the Kubernetes name func (storageAccount *StorageAccount) Default() { storageAccount.default() var temp interface{} = storageAccount if runtimeDefaulter, ok := temp.(genruntime.Defaulter); ok { runtimeDefaulter.CustomDefault() } } func (storageAccount *StorageAccount) default() []func() { storageAccount.defaultAzureName() } func (storageAccount *StorageAccount) defaultAzureName() { if storageAccount.Spec.AzureName == \u0026#34;\u0026#34; { storageAccount.Spec.AzureName = storageAccount.Name } } Sample Validate:\n// +kubebuilder:webhook:path=/validate-microsoft-storage-infra-azure-com-v1alpha1api20190401-storageaccount,mutating=false,sideEffects=None,matchPolicy=Exact,failurePolicy=fail,groups=microsoft.storage.infra.azure.com,resources=storageaccounts,verbs=create;update,versions=v1alpha1api20190401,name=validate.v1alpha1api20190401.storageaccounts.microsoft.storage.infra.azure.com,admissionReviewVersions=v1beta1  var _ admission.Validator = \u0026amp;StorageAccount{} // ValidateCreate validates the creation of the resource func (storageAccount *StorageAccount) ValidateCreate() error { validations := storageAccount.createValidations() var temp interface{} = storageAccount if runtimeValidator, ok := temp.(genruntime.Validator); ok { validations = append(validations, runtimeValidator.CreateValidations()...) } var errs []error for _, validation := range validations { err := validation() if err != nil { errs = append(errs, err) } } return kerrors.NewAggregate(errs) } func (storageAccount *StorageAccount) createValidations() []func() error { return []func() error{ storageAccount.validateResourceReferences, } } func (storageAccount *StorageAccount) validateResourceReferences() error { refs, err := reflecthelpers.FindResourceReferences(\u0026amp;storageAccount.Spec) if err != nil { return err } return genruntime.ValidateResourceReferences(refs) } // \u0026lt;other Validator methods elided...\u0026gt; Open questions #   How awkward is var temp runtime.Object = storageAccount + if runtimeValidator, ok := temp.(genruntime.Validator); ok - is there a better way to do this?   Other possibilities #   Don\u0026rsquo;t use admission.Defaulter or admission.Validator, instead register N webhooks. This has significant downsides though because the Defaulter and Validator webhooks are automatically registered by our registerWebhook method in generic_controller (ctrl.NewWebhookManagedBy(mgr).For(obj).Complete())  "},{"id":11,"href":"/azure-service-operator/design/resource-states/","title":"Resource States","section":"Design \u0026 Specifications","content":"Proposal for reporting resource Status #  What Status are we talking about? #  There are two types of Status that we\u0026rsquo;re interested in understanding and reporting to the user when running an operator that creates resources in Azure:\n The Status of the operator and its actions on the resource. Has the resource successfully been reconciled? Is the operator in the progress of driving the resource to its goal state (defined in the spec), or has it already done so and is now waiting for more changes before taking further action? The State of the resource in Azure. What fields are set in Azure? There are often defaults and other values which, while you may not have specified them in the spec have been applied on the server side and we want to show to you.  This document is focused on the first type of Status defined above.\nCurrent state of ASO #  ASO v1 (handcrafted) #  The handcrafted ASO resources have a 3-tuple on the Status field which describes the state of the resource:\nProvisioning bool `json:\u0026#34;provisioning,omitempty\u0026#34;` Provisioned bool `json:\u0026#34;provisioned,omitempty\u0026#34;` FailedProvisioning bool `json:\u0026#34;failedProvisioning,omitempty\u0026#34;` There are also additional fields for conveying additional information, including the details of any error as well as information about the Resource ID of the resource\nState string `json:\u0026#34;state,omitempty\u0026#34;` Message string `json:\u0026#34;message,omitempty\u0026#34;` ResourceId string `json:\u0026#34;resourceId,omitempty\u0026#34;` Problems with this approach #   Provisioning, Provisioned and FailedProvisioning are mutually exclusive but the structure of the status doesn\u0026rsquo;t convey that well. Additionally the fact that there are multiple fields means it\u0026rsquo;s easier for bugs to creep in where we set one but didn\u0026rsquo;t clear another. The State field is not widely used by all resources, and seems to conflict with Provisioning/Provisioned/FailedProvisioning. It\u0026rsquo;s not clear when you should look at one or the other.  ASO v2 (auto-generated) #  The auto-generated resources don\u0026rsquo;t currently have anything in their Status which conveys the Status of the operator. For the initial alpha releases, these fields are instead set as annotations on the object:\nDeploymentIDAnnotation = \u0026#34;deployment-id.infra.azure.com\u0026#34; DeploymentNameAnnotation = \u0026#34;deployment-name.infra.azure.com\u0026#34; ResourceStateAnnotation = \u0026#34;resource-state.infra.azure.com\u0026#34; ResourceErrorAnnotation = \u0026#34;resource-error.infra.azure.com\u0026#34; This was done mostly to avoid the additional work to code-generate the correct fields in each resources status and set them. Since it\u0026rsquo;s possible that the whole status can get lost, some of the things may need to stay as annotations (deploymentId possibly), while others are obviously status and can be derived again if lost. We took the temporary approach of putting it all in annotations while we learned more about what the best practices were.\nProblems with this approach #   The ResourceStateAnnotation is reusing the same enum to describe state as the armclient uses for deployments. This is problematic because that only supports Succeeded, Failed, Deleting, and Accepted. Succeeded, Failed, and Deleting make sense in the context of the operator but Accepted doesn\u0026rsquo;t really. There\u0026rsquo;s no state for InProgress or Reconciling. There\u0026rsquo;s an awkward tension between sharing the state of the ARM deployment and the state of the resource. They\u0026rsquo;re often the same but sometimes not (such as in the case the deployment is being deleted but the resource is not). Using annotations is awkward because changing an annotation triggers another reconcile. Using annotations means that it\u0026rsquo;s not clear to the user what the possible valid states are for the fields that are enums. Users expect to look in Status for status related fields. Having it exposed as an annotation was never intended as a long term solution, just a hack so that we could get things up and running.  Examining other projects like ASO #  A quick look across the field of projects similar to ASO suggests that there is a relatively standard approach to solving this problem:\n Crossplane reports status through a Ready condition ACK reports status through a variety of conditions, including ACK.Adopted, ACK.resourceSynced, ACK.Terminal, etc. Cluster API originally used a phase and failureReason/failureMessage pattern, but has since moved to use conditions and is deprecating the old pattern. Pod uses a combination of phase and conditions, including PodScheduled, ContainersReady, Initialized, and Ready.  It seems that most of these (and many core Kubernetes resources) use a Ready condition to specify the status.\nMore on conditions #  These best practices were gathered from conditions API conventions as of July 2021.\n Some resources in the v1 API contain fields called phase, and associated message, reason, and other status fields. The pattern of using phase is deprecated. Newer API types should use conditions instead. Phase was essentially a state-machine enumeration field, that contradicted system-design principles and hampered evolution, since adding new enum values breaks backward compatibility. Rather than encouraging clients to infer implicit properties from phases, we prefer to explicitly expose the individual conditions that clients need to monitor. Conditions also have the benefit that it is possible to create some conditions with uniform meaning across all resource types, while still exposing others that are unique to specific resource types. See #7856 for more details and discussion.\n  In condition types, and everywhere else they appear in the API, Reason is intended to be a one-word, CamelCase representation of the category of cause of the current status, and Message is intended to be a human-readable phrase or sentence, which may contain specific details of the individual occurrence. Reason is intended to be used in concise output, such as one-line kubectl get output, and in summarizing occurrences of causes, whereas Message is intended to be presented to users in detailed status explanations, such as kubectl describe output.\n  In general, condition values may change back and forth, but some condition transitions may be monotonic, depending on the resource and condition type. However, conditions are observations and not, themselves, state machines, nor do we define comprehensive state machines for objects, nor behaviors associated with state transitions. The system is level-based rather than edge-triggered, and should assume an Open World.\n  Controllers should apply their conditions to a resource the first time they visit the resource, even if the status is Unknown. This allows other components in the system to know that the condition exists and the controller is making progress on reconciling that resource.\n These from standardizing conditions:\n reason is required and must not be empty. The actor setting the value should always describe why the condition is the way it is, even if that value is \u0026ldquo;unknown unknowns\u0026rdquo;. No other actor has the information to make a better choice.\n Some other best practices on Kubernetes design principles:\n Object status must be 100% reconstructable by observation. Any history kept must be just an optimization and not required for correct operation.\n  Do not define comprehensive state machines for objects with behaviors associated with state transitions and/or \u0026ldquo;assumed\u0026rdquo; states that cannot be ascertained by observation.\n Proposal #  Goals #   The status of the operator should be reported as a subsection in the Status of the resource itself. See #1504. It should be clear to users where to look to determine what the current status of their resource is. The status should include information in both normal and failure cases. When a failure occurs, it should be clear to the user that a failure has happened and what the cause of the failure was. When a failure occurs, there should be an error or code that is programmatically consumable (for automation, etc). When a failure occurs, there should be an error or text that is human consumable and ideally more informative than the programmatically consumable error code. When a failure occurs, it should be clear to the user whether that failure is transient and the operator can self-recover.  Details #  All resource\u0026rsquo;s Status\u0026rsquo;s will have a top level Conditions field which is a collection of type Condition. This collection supports extension (through conditions with different type names). Initially we will expose a single Ready condition across all resources, representing the high level availability of the resource and its readiness for use.\nThe structure of the Condition is based on the recommended shape of conditions KEP as well as the Cluster API conditions proposal and is as follows:\n// ConditionSeverity expresses the severity of a Condition. type ConditionSeverity string const ( // ConditionSeverityError specifies that a failure of a condition type  // should be viewed as an error. Errors are fatal to reconciliation and  // mean that the user must take some action to resolve  // the problem before reconciliation will be attempted again.  ConditionSeverityError ConditionSeverity = \u0026#34;Error\u0026#34; // ConditionSeverityWarning specifies that a failure of a condition type  // should be viewed as a warning. Warnings are informational. The operator  // may be able to retry through the warning without any action from the user, but  // in some cases user action to resolve the warning will be required.  ConditionSeverityWarning ConditionSeverity = \u0026#34;Warning\u0026#34; // ConditionSeverityInfo specifies that a failure of a condition type  // should be viewed as purely informational. Things are working.  // This is the happy path.  ConditionSeverityInfo ConditionSeverity = \u0026#34;Info\u0026#34; // ConditionSeverityNone specifies that there is no condition severity.  // For conditions which have positive polarity (Status == True is their normal/healthy state), this will set when Status == True  // For conditions which have negative polarity (Status == False is their normal/healthy state), this will be set when Status == False.  // Conditions in Status == Unknown always have a severity of None as well.  // This is the default state for conditions.  ConditionSeverityNone ConditionSeverity = \u0026#34;\u0026#34; ) type ConditionType string const ( ConditionTypeReady = \u0026#34;Ready\u0026#34; ) // Condition defines an extension to status (an observation) of a resource type Condition struct { // Type of condition.  // +kubebuilder:validation:Required  Type ConditionType `json:\u0026#34;type\u0026#34;` // Status of the condition, one of True, False, or Unknown.  // +kubebuilder:validation:Required  Status metav1.ConditionStatus `json:\u0026#34;status\u0026#34;` // Severity with which to treat failures of this type of condition.  // For conditions which have positive polarity (Status == True is their normal/healthy state), this will be omitted when Status == True  // For conditions which have negative polarity (Status == False is their normal/healthy state), this will be omitted when Status == False.  // This is omitted in all cases when Status == Unknown  // +kubebuilder:validation:Optional  Severity ConditionSeverity `json:\u0026#34;severity,omitempty\u0026#34;` // LastTransitionTime is the last time the condition changed.  // +kubebuilder:validation:Required  LastTransitionTime metav1.Time `json:\u0026#34;lastTransitionTime\u0026#34;` // Reason for the condition\u0026#39;s last transition.  // Reasons are upper CamelCase (PascalCase) with no spaces. A reason is always provided, this field will not be empty.  // +kubebuilder:validation:Required  Reason string `json:\u0026#34;reason\u0026#34;` // Message is a human readable message indicating details about the transition. This field may be empty.  // +kubebuilder:validation:Optional  Message string `json:\u0026#34;message,omitempty\u0026#34;` } The Ready condition #  The Condition definition above discusses support for arbitrary conditions. In practice at least for now, ASO will only expose a single Ready condition. The addition of a Severity field inspired by Cluster API allows different combinations of Severity and Reason to combine together and describe complex scenarios.\nThe Reason field #  Inside of the Ready condition we always include a Reason which provides a programmatically consumable reason that the resource is in the given state. Reason is derived from multiple sources depending on where in the resource lifecycle we are. The value in Reason may be set by the operator itself, or be set to the result of an error or response code received from Azure.\nThe operator defines the following reasons:\n   Reason Severity Meaning     Reconciling Info A request has been submitted to Azure. The operator may be waiting for a response from Azure.   WaitingForOwner Warning The owner of this resource cannot be found in Kubernetes. Progress is blocked until the owner is created.   Deleting Info The resource is being deleted.   Succeeded None (\u0026quot;\u0026quot;/empty) The spec has successfully been applied. No additional changes are being attempted at this time.    There are no failure conditions (Severity = Error) specified here because currently all fatal errors come directly from Azure. When an error response is received from Azure, the Code from Azure is set as the Reason, and the Message from Azure is set as the Message.\nSeverity meaning in the context of the Ready condition #   Error means that we were unable to reconcile the resource successfully. A Ready condition with Status=False, Reason=\u0026lt;something\u0026gt;, Severity=Error is no longer attempting to be reconciled. The user must make an update to the resource to fix one or more configuration problems. Warning means that something is wrong, but we haven\u0026rsquo;t given up. The resource will continue to be reconciled until we can progress past the cause of the Warning. Users should examine the Warning as some may be due to the operator waiting for action from them, as in the case of Reason=WaitingForOwner, Severity=Warning. Others may be due to transient unavailability in Azure, as in the case of Reason=InternalServerError, Severity=Warning. Info means that everything is proceeded as expected. This is the \u0026ldquo;happy path\u0026rdquo;.  Reference #   Article on conditions and status reporting in Kubernetes. Note that this article is a bit old as Cluster API uses conditions now, but it gives a great overview of the topic. Recommended shape of Conditions Update condition guidance - and the actual guidance itself is here More condition recommendations  Open questions #   There are some small differences between this proposal and what CAPI is doing - are we ok with these differences?  CAPI says Reason is optional, but we\u0026rsquo;re making it required.    Open questions answered #  Open questions which have since been answered are below.\nQuestion: Will this work with kstatus, which recommends negative polarity conditions? Answer: Yes, kstatus supports the Ready condition as well, with a few caveats. Anecdotally, we feel that positive polarity conditions (like Ready) are more clear. As mentioned above, there are many operators that follow this Ready pattern including Crossplane and CAPI. If need-be, we can work around the major problem with kstatus and Ready by providing a webhook that automatically includes it on all resource creations.\nQuestion: The KEP for Condition says that LastTransitionTime should be updated any time the condition changes. The CAPI proposal says it should change when Status changes, but the actual CAPI implementation changes it any time the Condition changes.Which behavior do we want? Answer: We will follow the KEPs definition (and CAPI\u0026rsquo;s actual implementation) and update LastTransitionTime any time a Condition changes, even if that change is from Status=False to Status=False.\nAn aside on ARM templates #  This isn\u0026rsquo;t related to the core topic, but it might be useful to understand because it has a big impact on where failure is possible when communicating with ARM.\nYou can read some guidelines about how ARM templates work here.\nThe process basically boils down to the following and is documented here:\n ARM: Validate basic template syntax. Return an error for invalid JSON, invalid resource ID structure, etc. ARM: Send the resource to the individual resource provider (RP). RP: Return a response for the resource. This can be:  201 or 200 HTTP status code and a response body. At this point the resource should exist, but with possibly with a non-terminal provisioningState. 202 HTTP status code. At this point the resource should not exist, and will not exist until the long-running operation completes successfully.   ARM: Poll long running operation until it completes (if needed). ARM: Deployment status will be the status of the resource at the end of the long running operation, which should be in one of the 3 terminal states Succeeded, Failed, or Canceled  Possible outcomes:\n   Status of deployment State of resource     4xx HTTP error code when creating deployment Resource does not exist   Deployment accepted, but fails after polling Resource may or may not exist (depends on the RP)   Deployment succeeds Resource has Failed provisioningState - The spec doesn\u0026rsquo;t say this is impossible but it\u0026rsquo;s probably very rare   Deployment succeeds Resource has Succeeded provisioningState    Note: When a deployment is accepted, the underlying resource may or may not have been created. This means we have to handle cases where the deployment failed but the resource was created anyway alongside cases where the deployment failed and no resource was created.\n"},{"id":12,"href":"/azure-service-operator/design/type-references-and-ownership/","title":"Type References and Ownership","section":"Design \u0026 Specifications","content":"Type references and ownership #  Related reading #   Kubernetes garbage collection. Kubernetes RBAC  Related Projects #   ASO: Azure Service Operator k8s-infra: The handcrafted precurser to the code generation tool being designed.  Goals #   Provide a way for customers to express relationships between Azure resources in an idiomatic Kubernetes way. Provide automatic ownership and garbage collection (using Kubernetes garbage collection) where appropriate (e.g. ResourceGroup as an owner of all the resources inside of it)  Ideally ResourceGroup is handled the same as other owners and isn\u0026rsquo;t special cased.   Define how Kubernetes interacts with Azure resources not created/managed by Kubernetes, for example resources which were created prior to the customer onboarding to the Azure Service Operator. References should be extensible to work across multiple Azure subscriptions, although initially we may not support that.  Non-goals #   Managing ownership for resources/resource hierarchies that were not created by the service operator. While this proposal allows references to point to external resources not managed by the service operator, the operator is not watching/monitoring the resource in question and as such cannot propagate deletes. Put another way: for the operator to manage ownership/object lifecycles, the entire resource hierarchy must exist within Kubernetes. If only part of the resource hierarchy is managed by the service operator, only those parts can have their lifecycles managed.  Different kinds of resource relationships in Azure #  Related/linked resources #  Two resources are related to one another (\u0026ldquo;has-a\u0026rdquo; relationship), but there is no ownership. Example: VMSS → Subnet (json schema).\nThis relationship is always one-way (a VMSS refers to a Subnet, but a Subnet does not refer to a VMSS).\nOwner and dependent #  Two resources have a relationship where one is owned by the other.\nExamples:\n a RouteTable owns many Routes (json schema) a BatchAccount owns many Pools (json schema) a ResourceGroup owns any resource  A relationship like those shown here tells us two things:\n Where to create/manage the dependent resource (this Route goes in that particular RouteTable, this RouteTable has that Route) That the dependent resource should be deleted when the parent resource is deleted. There are theoretically two cases here:  The dependent resource must be deleted before the parent can be deleted. Deletion of the parent automatically cascades to all dependent resources. Due to how Azure ARM resources express ownership (via id which is part of the URL, with dependent resources being a subdirectory under the owning resources URL) all ARM resources should fall into this case.    Note that sometimes an owning resource has its dependent resources embedded directly (for example: RouteTable has the property RouteTablePropertiesFormat). Most types do not embed the dependent resource directly in the owning resource. We will need to cater for both the embedded and non-embedded cases.\nWhat do these relationships look like in existing solutions? #  This section examines how other operator solutions have tackled these problems. We look at:\n ARM templates Azure Service Operator (ASO) k8s-infra  Related/Linked resources #  What does ARM do? #  These are just properties (often but not always called id) which refer to the fully qualified ARM ID of another resource. For example see a sample deployment template for a VMSS refering to an existing vnet.\n\u0026#34;properties\u0026#34;: { \u0026#34;subnet\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;[resourceId(parameters(\u0026#39;existingVnetResourceGroupName\u0026#39;), \u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;, parameters(\u0026#39;existingVnetName\u0026#39;), parameters(\u0026#39;existingSubNetName\u0026#39;))]\u0026#34; }, } What does ASO do? #  Similar to how ARM templates behave, ASO uses the decomposition of fully qualified resource id to reference another resource, as seen here for VMSS → VNet\ntype AzureVMScaleSetSpec struct { ... Location string `json:\u0026#34;location\u0026#34;` ResourceGroup string `json:\u0026#34;resourceGroup\u0026#34;` VirtualNetworkName string `json:\u0026#34;virtualNetworkName\u0026#34;` SubnetName string `json:\u0026#34;subnetName\u0026#34;` ... } These properties are combined into a fully qualified ARM ID like so:\nsubnetIDInput := helpers.MakeResourceID( client.SubscriptionID, resourceGroupName, \u0026#34;Microsoft.Network\u0026#34;, \u0026#34;virtualNetworks\u0026#34;, vnetName, \u0026#34;subnets\u0026#34;, subnetName, ) This produces a resource ID: /subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/Microsoft.Network/virtualNetworks/{vnetName}/subnets/{subnetName}.\nCurrently ASO does not support cross-subscription references (and some of the resources such as VMSS don\u0026rsquo;t allow cross-resource group references), but it in theory could by adding parameters.\nWhat does k8s-infra do? #  k8s-infra is a bit different in that resource references are in Kubernetes style (namespace + name) and not Azure style (resource-group + resource-name). All resource references are done using the special type KnownTypeReference which contains the fully qualified Kubernetes name for the resource.\nDependent Resources #  What does ARM do? #  ARM template deployments support two different ways of deploying dependent resources:\n Deploy the resources in the same ARM template with dependent resources embedded inside owning resources using the resources property.  The dependent resource need not specify which resource is its owner, because it is implied by embedded structure. The owning resource and the dependent resource must be created at the same time.   Deploy the resources separately.  The dependent resource must specify which resource is its owner by including in its name field both the owning resource name and the dependent resource name separated by a /. Each segment of the name corresponds to an owning resource. For example creating a Batch Pool foo in Batch account account would have name = account/foo. The dependent resource can be created after the owning resource has already been created, or can be created at the same time as the owning resource. If created at the same time, the dependsOn field must be used to inform ARM of the order to perform resource creation.    What does ASO do? #  Dependent resources in ASO have properties which map to the name/path to their owner. For example MySQLFirewallRuleSpec looks like this:\ntype MySQLFirewallRuleSpec struct { ResourceGroup string `json:\u0026#34;resourceGroup\u0026#34;` Server string `json:\u0026#34;server\u0026#34;` StartIPAddress string `json:\u0026#34;startIpAddress\u0026#34;` EndIPAddress string `json:\u0026#34;endIpAddress\u0026#34;` } The ResourceGroup and Server are references to the owners of this type.\nWhat does k8s-infra do? #  k8s-infra uses the same KnownTypeReference type mentioned above for ownership references too. There are two patterns for ownership in k8s-infra today.\nOne pattern is used for ResourceGroup, where top level resources have a link to the resource group they are in.\ntype VirtualMachineScaleSetSpec struct { // +kubebuilder:validation:Required  ResourceGroupRef *azcorev1.KnownTypeReference `json:\u0026#34;resourceGroupRef\u0026#34; group:\u0026#34;microsoft.resources.infra.azure.com\u0026#34; kind:\u0026#34;ResourceGroup\u0026#34;` ... } The other pattern is where the owning resource has links to the dependent resources it is expecting to have:\ntype RouteTableSpecProperties struct { DisableBGPRoutePropagation bool `json:\u0026#34;disableBgpRoutePropagation,omitempty\u0026#34;` RouteRefs []azcorev1.KnownTypeReference `json:\u0026#34;routeRefs,omitempty\u0026#34; group:\u0026#34;microsoft.network.infra.azure.com\u0026#34; kind:\u0026#34;Route\u0026#34; owned:\u0026#34;true\u0026#34;` } If the dependent resources aren\u0026rsquo;t there, the status of the owning resource reflects an error explaining that.\nProposal #  A note on names before we get started #  In Kubernetes each resource must have a unique name for its group-kind. For example, if we had a RouteTable CRD, each RouteTable object would need to have a unique name. In ARM, resources do not need to be uniquely named. There can be two RouteTable resources with the same name provided they are in different resource groups. The owner-dependent resource relationship impacts uniqueness in Azure in a way that it doesn\u0026rsquo;t in Kubernetes.\nProposed solution #  All Kubernetes resources will have two fields which are used in combination to build the Azure name: Metadata.Name and Spec.AzureName. When Spec.AzureName is empty, Metadata.Name is used as the resource name. When Spec.AzureName is provided, it takes precedence and is used when interacting with ARM, but the resource in Kubernetes is still called by its Metadata.Name.\nOverview #  We propose the following high-level solution:\n All references will be via Kubernetes group, kind, and name. If a resource not managed by Kubernetes must be referenced, that resource must be imported into Kubernetes as an Unmanaged resource. Dependent resources will refer to their parent via an Owner property. The Owner property will automatically detect group and kind, making specifying an owner as simple as providing the Kubernetes resource name. Dependent resources with the Owner property set will automatically have their ownerReferences configured so that Kubernetes garbage collection will delete the dependent resources when the owner is deleted. References to related resources will be automatically detected by the code generator and transformed into the correct reference type. At serialization time, the controller will transform the Kubernetes types (including related resource references and owner references) into the correct Azure resource definitions (including fully qualified ARM IDs).  More specific details about how this will be achieved are in the following sections.\nHow to represent references #  There are two kinds of references we need to represent: References to a resource whose type we know statically at compile time, and references to a resource whose type we do not know at compile time.\nWe could use the same type for both kinds of references, but that has the downside of allowing a situation where we know the group and version statically at compile time, but the customer has also provided it and it doesn\u0026rsquo;t match. Two types allows us to clearly express what we\u0026rsquo;re expecting for each reference. The resulting YAMLs look basically the same to the customer, and the required-ness of the fields will give push-back when customers need to specify a group or kind and have not.\n// KnownResourceReference is a resource reference to a known type. type KnownResourceReference struct { // This is the name of the Kubernetes resource to reference. \tName string `json:\u0026#34;name\u0026#34;` // References across namespaces are not supported.  // Note that ownership across namespaces in Kubernetes is not allowed, but technically resource \t// references are. There are RBAC considerations here though so probably easier to just start by \t// disallowing cross-namespace references for now } type ResourceReference struct { // The group of the referenced resource.  Group string `json:\u0026#34;group\u0026#34;` // The kind of the referenced resource.  Kind string `json:\u0026#34;kind\u0026#34;` // The name of the referenced resource.  Name string `json:\u0026#34;name\u0026#34;` // Note: Version is not required here because references are all about linking one Kubernetes  // resource to another, and Kubernetes resources are uniquely identified by group, kind, (optionally namespace) and  // name - the versions are just giving a different view on the same resource } How to represent ownership and dependent resources #  We will use the same KnownResourceReference type as an additional Owner field on dependent resource specifications.\nWhen we determine that a resource is a dependent resource of another resource kind, we will code-generate an Owner property in the dependent resource Spec. This will also include an annotation about the expected type of the resource (group and kind) so that the customer doesn\u0026rsquo;t have to specify that in the YAML.\ntype SubnetSpec struct { Owner KnownResourceReference `json:\u0026#34;owner\u0026#34; group:\u0026#34;microsoft.network.infra.azure.com\u0026#34; kind:\u0026#34;VirtualNetwork\u0026#34;` ... } When users submit a dependent object we will validate that the provided owner reference is present. This can be accomplished by making the property required in the CRD.\nA YAML snippet showing how this will look from the customer\u0026rsquo;s perspective:\n... spec: owner: name: my-vnet ... One major advantage of this approach is that the customer cannot really get the owning type wrong, because we\u0026rsquo;ve autogenerated the expected group/kind information all names they supply must point to the right kind of resource.\nHow to represent a resource generically #  In addition to representing references generically, we will need the ability to reference ARM resources generically, so that the generic controller can act on them without needing to cast to their specific type.\n// TODO: There may be more in this interface, or it may get rolled into MetaObject depending on yet to be determined implementation details type ArmResource interface { // Name returns the ARM resource name \tName() string // Owner returns the ResourceReference so that we can extract the Group/Kind for easier lookups  Owner() *ResourceReference } How to identify resource relationships #  For related (not owned) resources we must find each field that represents a resource reference and transform its type to ResourceReference. There is no specific marker which means: \u0026ldquo;This field is a reference\u0026rdquo; - most are called id but that\u0026rsquo;s not a guarantee. For example on the VirtualMachineScaleSetIPConfigurationProperties the subnet field is of custom type ApiEntityReference, which has an id field where you put the ARM ID for a subnet. This may require some manual work. One thing we can investigate doing long term is see if there\u0026rsquo;s a way to get teams to annotate \u0026ldquo;links\u0026rdquo; in their Swagger somehow.\nFor dependent resources we must identify all of the owner to dependent relationships between resources. As discussed in what ARM does, this can be done using the resources property in the ARM deployment templates. These are much easier to automatically detect than related resources as the dependent types are called out in the resources property explicitly.\nHow to choose the right reference type (ResourceReference vs KnownResourceReference) at generation time #  Because we are code-generating all of the Owner fields based on the resources property in the JSON schema, and each ARM resource can be owned by at most 1 other resource, we can always supply the annotations for group and kind automatically for the Owner field. This is not the case for abitrary references (id\u0026rsquo;s) to other resources. We do not actually know programmatically what type those references are. In some cases it may actually be allowed to point to multiple different types (for example: custom image vs shared image gallery).\nIn the KnownResourceReference case, we know the type we\u0026rsquo;re looking for and can fail fast if the customer specifies the wrong type. In the ResourceReference case, we cannot know the type we\u0026rsquo;re looking for, so we must accept what the customer has provided and ensure that we have good error messages if they have provided a link to an invalid resource (usually the error from Azure should suffice).\nResourceLifeCycle and unmanaged resources #  In order to keep references Kubernetes-native, allow a \u0026ldquo;single pane of glass\u0026rdquo; for customers looking at their Azure resources through Kubernetes, and allow references to resources that were created before the customer onboarded to the operator, we introduce a new mode to each resource: ResourceLifeCycle.\nResourceLifeCycle can be either Managed or Unmanaged.\nResourceLifeCycle is not specified by the customer explicitly in the Spec, instead it is inferred based on how the resource was created in Kubernetes. If a resource is created as just a reference (id, name, no spec details) then it is Unmanaged. If a resource is created with a populated spec, then the resource is Managed.\nRouteTable + Routes issue (multiple routes of the same name are allowed) #  Options for this: Note that all of these options share this restriction: Each resource must be imported, e.g. to import a VNET you may need to import the resource group the VNET is in, and then the VNET (with an Owner reference pointing to the imported ResourceGroup).\nOption 1: Users must create a valid resource with the same name as the resource they want to track. If this resource\u0026rsquo;s spec differs from what is in Azure, an error is logged but we never actually apply any state to Azure (i.e. we don\u0026rsquo;t try to sync to the spec). A tag in the metadata must be added to inform the operator not to sync.\nAdvantages\n Swapping from unmanaged to managed is super easy, just remove the tag blocking the reconciliation loop.  Disadvantages\n There is possibly a significant amount of extra effort required to re-specify a resource whose shape we really just want to \u0026ldquo;import\u0026rdquo; from ARM. Worse for large trees of objects or deeply nested objects. If the tag is forgotten (or has a typo) we will try to manage a resource which we shouldn\u0026rsquo;t be managing. This could be very problematic depending on how different the specification is from what exists in Azure. The existence of a spec may suggest we are actually seeking towards it \u0026ndash; which we are not. ASO does have a similar feature though so maybe not that big of a problem.  Option 2: Users create an entity with just the \u0026ldquo;identifying fields\u0026rdquo; set: Metadata.Name, Owner, and optionally Spec.AzureName. When an entity is created like this, the controller knows to treat it specially (optionally may also add a tag automatically?). These entities will only be watched by the controller, no mutating update will be sent to ARM.\nAdvantages\n Relatively easy to import even complex object hierarchies.  Disadvantages\n This screws up the \u0026ldquo;required-ness\u0026rdquo; of non-identifying fields in a spec. For example: a Virtual Network requires a Properties VirtualNetworkProperties field to be set, but since we have to allow that field to be nil when importing a Virtual Network we can\u0026rsquo;t set the Properties field with a required annotation for Kubebuilder.  Option 3: Same as option 2, but use anyOf to specify two valid structures:\nspec: type: object properties: owner: properties: name: type: string azureName: type: string foo: type: integer anyOf: - required: [\u0026#34;owner\u0026#34;] - required: [\u0026#34;owner\u0026#34;, \u0026#34;foo\u0026#34;] Note that it has to be anyOf because oneOf disallows multiple matches, and owner + foo matches both sets in the example above.\nAdvantages\n Represents what we want and maintains better automatic validation.  Disadvantages\n Kubebuilder doesn\u0026rsquo;t support generating this, so we would have to come up with another way to do it, or possibly upstream changes to Kubebuilder to support it.  Option 4: Other ideas\u0026hellip; Do away with Kubebuilder validation entirely and use our own (including our own validating webhooks). Use Kustomize and our own code-generator/parser to generate amendments to Kubebuilder\u0026rsquo;s generated CRDs to get the anyOf shape we want above.\nHow to transform Kubernetes objects to ARM objects (and back) #  In the case of resource ownership, the proposed Owner property exists on dependent resources in the CRD but must not go to Azure as Azure doesn\u0026rsquo;t understand it. In the case of a generic resource reference, the ResourceReference in the CRD must become an id (with fully-qualified ARM ID) when serialized to ARM. In both cases, we need two representations of the entity: one to Kubernetes as the CRD, and one to Azure. These two types are structurally similar but not identical. We cannot just override JSON serialization to solve this problem due to the fact that there are actually two distinct JSON representations we need.\nThe proposed solution is that the code generator intelligently generates 2 types for cases where we know the CRD shape differs from ARM. We will add an interface which types can optionally implement which allows them to transform themselves to another type prior to serialization to/from ARM. This is also a useful hook for any manual customization for serialization we may need.\nThe interface will look something like this:\ntype ARMTransformer interface { ToArm(owningName string) (interface{}, error) FromArm(owner KnownResourceReference, input interface{}) error } Here\u0026rsquo;s an example of how it will be implemented:\nfunc CreateArmResourceNameForDeployment(owningName string, name string) string { result := owningName + \u0026#34;/\u0026#34; + name return result } // +kubebuilder:object:root=true // +kubebuilder:storageversion type VirtualNetworksSubnets struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec VirtualNetworksSubnetsSpec `json:\u0026#34;spec,omitempty\u0026#34;` } var _ ArmResource = \u0026amp;VirtualNetworksSubnets{} func (resource *VirtualNetworksSubnets) Owner() *ResourceReference { r := reflect.TypeOf(resource.Spec) ownerField, found := r.FieldByName(\u0026#34;Owner\u0026#34;) if !found { return nil } group := ownerField.Tag.Get(\u0026#34;group\u0026#34;) kind := ownerField.Tag.Get(\u0026#34;kind\u0026#34;) return \u0026amp;ResourceReference { group: group, kind: kind, name: resource.Spec.Owner.Name } } func (resource *VirtualNetworksSubnets) Name() string { return resource.Spec.Name } type VirtualNetworksSubnetsSpec struct { // +kubebuilder:validation:Required \tApiVersion VirtualNetworksSubnetsSpecApiVersion `json:\u0026#34;apiVersion\u0026#34;` // +kubebuilder:validation:Required \tName string `json:\u0026#34;name\u0026#34;` // +kubebuilder:validation:Required \tOwner genruntime.KnownResourceReference `json:\u0026#34;owner\u0026#34; group:\u0026#34;microsoft.network\u0026#34; kind:\u0026#34;VirtualNetworks\u0026#34;` // +kubebuilder:validation:Required \t//Properties: Properties of the subnet. \tProperties SubnetPropertiesFormat `json:\u0026#34;properties\u0026#34;` // +kubebuilder:validation:Required \tType VirtualNetworksSubnetsSpecType `json:\u0026#34;type\u0026#34;` } // No KubeBuilder comments required here because not ever used to generate CRD type VirtualNetworksSubnetsSpecArm struct { ApiVersion VirtualNetworksSubnetsSpecApiVersion `json:\u0026#34;apiVersion\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` //Properties: Properties of the subnet. \tProperties SubnetPropertiesFormat `json:\u0026#34;properties\u0026#34;` Type VirtualNetworksSubnetsSpecType `json:\u0026#34;type\u0026#34;` } // This interface implementation would be autogenerated for ARM resources with references var _ genruntime.ArmTransformer = \u0026amp;VirtualNetworksSubnetsSpec{} func (transformer *VirtualNetworksSubnetsSpec) ToArm(owningName string) (interface{}, error) { result = VirtualNetworksSubnetsSpecArm{} result.ApiVersion = transformer.ApiVersion result.Name = CreateArmResourceNameForDeployment(owningName, transformer.Name) result.Properties = transformer.Properties result.Type = transformer.Type return result, nil } func (transformer *VirtualNetworksSubnetsSpec) FromArm(owner genruntime.KnownResourceReference, input interface{}) error { typedInput, ok := input.(VirtualNetworksSubnetsSpecArm) if !ok { return fmt.Errorf(\u0026#34;unexepected type supplied for FromArm function. Expected VirtualNetworksSubnetsSpecArm, got %T\u0026#34;, input) } transformer.ApiVersion = typedInput.ApiVersion transformer.Name = ExtractKubernetesResourceNameFromArmName(typedInput.Name) transformer.Owner = owner transformer.Properties = typedInput.Properties transformer.Type = typedInput.Type return nil } Controller example #  Putting it all together, here\u0026rsquo;s what a generic controller reconciliation loop would look like using the interfaces discussed previously.\n// Example usage -- error handling elided for brevity func (gr *GenericReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { scheme := ... gvk := ... client := ... // Load the object from etcd  obj := scheme.New(gvk) resource := client.Get(req.NamespacedName, obj) // Get the owner details  armResource := obj.(ARMResource) ownerRef := armResource.Owner() // Perform a get from Azure to see current resource state  armId := helpers.GetArmId(resource) objFromAzure := scheme.new(gvk) // We need to provide the empty type to deserialize into  // Somehow construct a new object of type etcdObject  if armTransformer, ok := objFromAzure.(ARMTransformer); ok { result := armTransformer.ToArm(\u0026#34;\u0026#34;) // This just converts from an empty kube shape to an empty arm shape  armClient.GetIt(armId, result) armTransformer.FromArm(ownerRef, result) } // Perform a put to update resource state  // Walk the owner hierarchy (assuming owner has no owner here for simplicity) to build owner name  ownerGvk := ownerRef.ToGvk() owner := scheme.New(ownerGvk) ownerArmResource := owner.(ARMResource) ownerId := owner.Name() var toSerialize interface{} toSerialize = resource if armTransformer, ok := toSerialize.(ARMTransformer); ok { toSerialize = armTransformer.ToArm(ownerArmId) } json := json.Marshal(toSerialize) armClient.SendIt(json) } FAQ #  What happens when a dependent resource specifies an Owner that doesn\u0026rsquo;t exist? #  The dependent resource will be stuck in an unprovisioned state with an error stating that the owner doesn\u0026rsquo;t exist. If the owner is created, the dependent resource will then be created by the reconciliation loop automatically.\nWhat happens when a resource contains a link to another resource which doesn\u0026rsquo;t exist? #  The resource with the link will be stuck in an unprovisioned state with an error stating that the linked resource doesn\u0026rsquo;t exist. This behavior is the same as for a dependent resource with a non-existent owner.\nHow are the CRD entities going to be rendered as ARM deployments? #  There are a few different ways to perform ARM deployments as discussed in Dependent Resources. Due to the nature of Kubernetes CRDs, each resource is managed separately and has its own reconcilation loop. It doesn\u0026rsquo;t make sense to try to deploy a single ARM template with the entire resource graph. Each resource will be done in its own deployment (with a dependsOn specified if required).\nAren\u0026rsquo;t there going to be races in resource creation? #  Yes. If you have a complex hierarchy of resources (where resources have involved relationships between one another) and submit all of their YAMLs to the operator at the same time it is likely that some requests when sent to ARM will fail because of missing dependencies. Those resources that failed to deploy initially will be in an unprovisioned state in Kubernetes, and eventually all the resources will be created through multiple iterations of the reconciliation loop.\nAren\u0026rsquo;t there going to be races in resource deletion? #  Yes. Owner as discussed in this specification is informing Kubernetes how Azure behaves. The fact that a ResourceGroup is the owner of a VirtualMachineScaleSet means that when the ResourceGroup is deleted in Azure, the VirtualMachineScaleSet will be too.\nThis means that practically speaking, we don\u0026rsquo;t need Kubernetes garbage collection to perform deletion of resources in Azure. Azure is already going to do that automatically. We need Kubernetes garbage collection to easily maintain sync with Azure.\nAs far as implementation goes this just means that when we are performing deletes in the generic controller and the resource is already deleted in Azure we just swallow that error and allow the Kubernetes object to be deleted.\nWhat exactly happens when a resource with an Owner is created? #  Once the resource has been accepted by the various admissions controllers and has been cofirmed to match the structural schema defined in the CRD, the generic controller will attempt to look up the owning resource in etcd (or in ARM if it\u0026rsquo;s an AzureReference).\nIf the generic controller finds the owning resource, it updates the ownerReference in the object metadata to include the uid of the owning resource and then submits an ARM template to ARM using the name of the owner and the name of the resource to build the name specified in the ARM template. It will include the name of the owner in the dependsOn field.\nWhat happens if an owning resource is deleted and immediately recreated? #  Kubernetes garbage collection is based on object uid\u0026rsquo;s. As discussed above, we bind to that uid on dependent resource creation. If a resource is deleted and then recreated Kubernetes will still understand that the new resource is fundamentally different than the old resource and garbage collection will happen as expected. The result will be that there is a new owning resource but all of its dependent resources were deleted (in Azure and in k8s).\nTODOs #   How can we allow customers to easily find all dependents for a particular owner (i.e. all subnets of a vnet) using kubectl? Cross subscription refs? Note that these are supported by a few Azure resources (VNET for example), but aren\u0026rsquo;t supported in most places.  Questions #  These are questions I am posing to the group - I don\u0026rsquo;t expect to have an answer without input from the group.\n What to do with awkward resources where the owner requires at least 1 dependent to also be created with it? David Justice pointed out this one Do we want to use the same type for ownership relationships and \u0026ldquo;related\u0026rdquo; relationships? Ownership has other angles such as how deletes propagate which in theory don\u0026rsquo;t apply for other kinds of relationships. Do we need to worry about letting customers choose between foreground cascading deletion and background cascading deletion or do we just pick one behavior which is best for our case?  The road not travelled #  Shape of Azure References #  We considered avoiding the complexity of ResourceLifecycle (Managed vs Unmanaged), instead allowing references to Azure resources directly by ARM ID.\nReferences would look like this:\ntype KnownResourceReference struct { Kubernetes KnownKubernetesReference `json:\u0026#34;kubernetes\u0026#34;` Azure string `json:\u0026#34;azure\u0026#34;` } type ResourceReference struct { Kubernetes KubernetesReference `json:\u0026#34;kubernetes\u0026#34;` Azure string `json:\u0026#34;azure\u0026#34;` } type KnownKubernetesReference struct { // This is the name of the Kubernetes resource to reference. \tName string `json:\u0026#34;name\u0026#34;` // References across namespaces are not supported. \t// Note that ownership across namespaces in Kubernetes is not allowed, but technically resource \t// references are. There are RBAC considerations here though so probably easier to just start by \t// disallowing cross-namespace references for now } type KubernetesReference struct { // The group of the referenced resource. \tGroup string `json:\u0026#34;group\u0026#34;` // The kind of the referenced resource. \tKind string `json:\u0026#34;kind\u0026#34;` // The name of the referenced resource. \tName string `json:\u0026#34;name\u0026#34;` // Note: Version is not required here because references are all about linking one Kubernetes \t// resource to another, and Kubernetes resources are uniquely identified by group, kind, (optionally namespace) and \t// name - the versions are just giving a different view on the same resource } Advantages compared to what we chose #   Can track resources which are not tracked by Kubernetes. Doesn\u0026rsquo;t need to introduce ResourceLifeCycle. ResourceLifeCycle complicates the mental model of individual resources as now apply on a resource can fail due to ResourceLifeCycle being Unmanaged. Can support references to resource types which the operator doesn\u0026rsquo;t yet support. It\u0026rsquo;s likely that we can work around this in the chosen architecture if it becomes a big problem though.  Disadvantages compared to what we chose #   References are not always Kubernetes-native looking. The reference structure is a more complex nested type, which makes references (which are common) more complicated. Moving from a resource link being to Azure directly to that same resource being managed/tracked by Kubernetes requires sweeping updates across all types referencing the migrated resource. Doesn\u0026rsquo;t allow for a \u0026ldquo;single pane of glass\u0026rdquo; experience where customers can easily view all of their resources in a Kubernetes native way.  How to represent references #  Use fully qualified ARM ID (a single string) for all references #  Pros #   Super simple to implement, because it\u0026rsquo;s what ARM expects at the end of the day anyway.  Cons #   You can\u0026rsquo;t easily transplant your YAML between subscriptions/resource groups because those IDs are in the YAML - you need templates and variables so that you can easily move between different resource groups or Subscriptions. Customers can\u0026rsquo;t stay in Kubernetes-land, they have to move their mental model to an \u0026ldquo;Azure\u0026rdquo; model.  Use built-in OwnerReference for owner references (customer setting these directly) #  Pros #   Basically none - customers are not supposed to set this directly.  Cons #   OwnerReference requires the object UID, which cannot be known at template authoring time. OwnerReference only works for ownership relationships, not for references.  Where ownership references are specified #  Ownership is from owner to dependent #  Pros #   It makes getting a list of all resources under a particular owner very easy.  Cons #   Adding/deleting a new dependent resource requires an update to the owner. The owner can be in a failed state because dependent resources are missing. It feels like we\u0026rsquo;re repeating our intent here: On the one hand, we told the owner that it should have 3 dependents, while on the other hand we only created 2 of those 3. It feels like the state of the resources in kubernetes (i.e. how many dependents there actually are) is already expressing the intent for how many we want, so having that also on the owner seems duplicate.  "},{"id":13,"href":"/azure-service-operator/introduction/authentication/","title":"Authentication","section":"User’s Guide","content":"Authentication in Azure Service Operator v2 #  Azure Service Operator supports two different styles of authentication today.\n Service Principal aad-pod-identity authentication (managed identity)  Service Principal #  Prerequisites #   An existing Azure Service Principal.  To use Service Principal authentication, specify an aso-controller-settings secret with AZURE_CLIENT_ID and AZURE_CLIENT_SECRET set.\n AZURE_CLIENT_ID must be set to the Service Principal client ID. This will be a GUID. AZURE_CLIENT_SECRET must be set to the Service Principal client secret.  For more information about Service Principals, see creating an Azure Service Principal using the Azure CLI. The AZURE_CLIENT_ID is sometimes also called the App ID. The AZURE_CLIENT_SECRET is the \u0026ldquo;password\u0026rdquo; returned by the command in the previously linked documentation.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aso-controller-settings namespace: azureserviceoperator-system stringData: AZURE_SUBSCRIPTION_ID: \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$AZURE_CLIENT_ID\u0026#34; AZURE_CLIENT_SECRET: \u0026#34;$AZURE_CLIENT_SECRET\u0026#34; EOF Managed Identity (aad-pod-identity) #  Prerequisites #   An existing Azure Managed Identity. aad-pod-identity installed into your cluster. If you are running ASO on an Azure Kubernetes Service (AKS) cluster, you can instead use the integrated aad-pod-identity.  First, set the following environment variables:\nexport IDENTITY_RESOURCE_GROUP=\u0026#34;myrg\u0026#34; # The resource group containing the managed identity. export IDENTITY_NAME=\u0026#34;myidentity\u0026#34; # The name of the identity. export AZURE_SUBSCRIPTION_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure Subscription ID the identity is in. export AZURE_TENANT_ID=\u0026#34;00000000-0000-0000-0000-00000000000\u0026#34; # The Azure AAD Tenant the identity/subscription is associated with. Use the az cli to get some more details about the identity to use:\nexport IDENTITY_CLIENT_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query clientId -otsv)\u0026#34; export IDENTITY_RESOURCE_ID=\u0026#34;$(az identity show -g ${IDENTITY_RESOURCE_GROUP} -n ${IDENTITY_NAME} --query id -otsv)\u0026#34; Deploy an AzureIdentity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentity metadata: name: aso-identity namespace: azureserviceoperator-system spec: type: 0 resourceID: ${IDENTITY_RESOURCE_ID} clientID: ${IDENTITY_CLIENT_ID} EOF Deploy an AzureIdentityBinding to bind this identity to the Azure Service Operator manager pod:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: \u0026#34;aadpodidentity.k8s.io/v1\u0026#34; kind: AzureIdentityBinding metadata: name: aso-identity-binding namespace: azureserviceoperator-system spec: azureIdentity: aso-identity selector: aso-manager-binding EOF Deploy the aso-controller-settings secret, configured to use the identity:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: aso-controller-settings namespace: azureserviceoperator-system stringData: AZURE_SUBSCRIPTION_ID: \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; AZURE_TENANT_ID: \u0026#34;$AZURE_TENANT_ID\u0026#34; AZURE_CLIENT_ID: \u0026#34;$IDENTITY_CLIENT_ID\u0026#34; EOF "},{"id":14,"href":"/azure-service-operator/introduction/resources/","title":"Resources","section":"User’s Guide","content":"microsoft.authorization #  ARM version 2020-08-01-preview #   RoleAssignment (sample)  Use CRD version v1alpha1api20200801preview\nmicrosoft.batch #  ARM version 2021-01-01 #   BatchAccount (sample)  Use CRD version v1alpha1api20210101\nmicrosoft.compute #  ARM version 2020-09-30 #   Disk (sample)  Use CRD version v1alpha1api20200930\nARM version 2020-12-01 #   VirtualMachine (sample) VirtualMachineScaleSet (sample)  Use CRD version v1alpha1api20201201\nmicrosoft.containerservice #  ARM version 2021-05-01 #   ManagedCluster (sample) ManagedClustersAgentPool (sample)  Use CRD version v1alpha1api20210501\nmicrosoft.dbforpostgresql #  ARM version 2021-06-01 #   FlexibleServer (sample) FlexibleServersConfiguration (sample) FlexibleServersDatabase (sample) FlexibleServersFirewallRule (sample)  Use CRD version v1alpha1api20210601\nmicrosoft.documentdb #  ARM version 2021-05-15 #   DatabaseAccount (sample) MongodbDatabase (sample) MongodbDatabaseCollection (sample) MongodbDatabaseCollectionThroughputSetting (sample) MongodbDatabaseThroughputSetting (sample) SqlDatabase (sample) SqlDatabaseContainer (sample) SqlDatabaseContainerStoredProcedure (sample) SqlDatabaseContainerThroughputSetting (sample) SqlDatabaseContainerTrigger (sample) SqlDatabaseContainerUserDefinedFunction (sample) SqlDatabaseThroughputSetting (sample)  Use CRD version v1alpha1api20210515\nmicrosoft.eventgrid #  ARM version 2020-06-01 #   Topic (sample)  Use CRD version v1alpha1api20200601\nmicrosoft.eventhub #  ARM version 2021-11-01 #   Namespace (sample) NamespacesAuthorizationRule (sample) NamespacesEventhub (sample) NamespacesEventhubsAuthorizationRule (sample) NamespacesEventhubsConsumerGroup (sample)  Use CRD version v1alpha1api20211101\nmicrosoft.managedidentity #  ARM version 2018-11-30 #   UserAssignedIdentity (sample)  Use CRD version v1alpha1api20181130\nmicrosoft.network #  ARM version 2020-11-01 #   LoadBalancer (sample) NetworkInterface (sample) NetworkSecurityGroup (sample) NetworkSecurityGroupsSecurityRule (sample) PublicIPAddress (sample) VirtualNetwork (sample) VirtualNetworkGateway (sample) VirtualNetworksSubnet (sample) VirtualNetworksVirtualNetworkPeering (sample)  Use CRD version v1alpha1api20201101\nmicrosoft.servicebus #  ARM version 2021-01-01-preview #   Namespace (sample) NamespacesQueue (sample) NamespacesTopic (sample)  Use CRD version v1alpha1api20210101preview\nmicrosoft.signalrservice #  ARM version 2021-10-01 #   SignalR (sample)  Use CRD version v1alpha1api20211001\nmicrosoft.storage #  ARM version 2021-04-01 #   StorageAccount (sample) StorageAccountsBlobService (sample) StorageAccountsBlobServicesContainer (sample)  Use CRD version v1alpha1api20210401\n"},{"id":15,"href":"/azure-service-operator/design/api-versions/","title":"API Versions","section":"Design \u0026 Specifications","content":"API Versions #  Specification for how we will ensure the ARM API version we use for interaction with ARM matches the version originally requested by a user when they created the resource in their Kubernetes cluster.\nWhy do we need this? #  Sometimes, in addition to structural changes, there are behaviour changes between ARM API versions. It\u0026rsquo;s therefore important that we use the requested API version when interacting with ARM to ensure that we get the behaviour requested.\nExample #  Revisting the CRM example from the Versioning specification, consider what happens if we have two available versions of the resource Person, lets call them v1 and v2. In v2 the new properties PostalAddress and ResidentialAddress are mandatory, requiring that everyone have a both a mailing address and a home.\nIf we have a valid v1 Person, trying to submit that through the v2 ARM API will fail because it\u0026rsquo;s missing these addresses.\nProposed Solution #  We need to preserve the original API Version of each resource, and use that to create an appropriate resource for ARM.\nAPI Preservation #  When generating storage variants, we\u0026rsquo;ll inject a new OriginalVersion property of type string into the Spec of each resource, providing a place to capture the API version that was originally used to create the resource.\nTo populate the OriginalVersion property on each storage spec, we\u0026rsquo;ll inject an OriginalVersion() method (returning string) into the API variant of each spec.\nAPI version shown on the left, corresponding Storage version shown on the right.\nFor each API spec, generated AssignPropertiesTo*() method will read the value of OriginalVersion() and write it to the OriginalVersion property on the storage variant. The AssignPropertiesFrom*() method will ignore OriginalVersion.\nFor each Storage spec, the generated AssignPropertiesTo*() and AssignPropertiesFrom*() methods will copy the OriginalVersion property between versions, preserving the original value.\nAPI Recovery #  Into each storage resource variant, we\u0026rsquo;ll inject a function OriginalGVK(): GroupVersionKind which will use OriginalVersion to create the GVK required.\nSequenceDiagram TBC.\n"},{"id":16,"href":"/azure-service-operator/design/versioning/case-studies/chained-storage-versions/","title":"Chained Storage Versions","section":"Case Studies","content":"Case Study - Chained Storage Versions #  This case study explores the alternative solution of using a chained storage versions. We update the storage schema of each resource each release of the service operator. We\u0026rsquo;ll keep the storage version up to date with the latest GA release of each resource. Older storage versions are retained, both as intermediate steps in the hub-and-spoke conversions, and to allow upgrades.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v20110101storage type Person struct { PropertyBag FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Every property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we currently have only single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nThe PropertyBag type provides storage for other properties, plus helper methods. It is always included in storage versions, but in this case will be unused. The method Hub() marks this version as the storage schema.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import storage \u0026#34;v20110101storage\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest storage.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } Conversion is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nThese methods will be automatically generated in order to handle the majority of the required conversions. Since they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map is simple and straightforward.\nVersion 2012-02-02 - No Change #  In this release of the CRM service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  The existing conversion between the v20110101 API version and v20110101storage version is retained, preserving in-place a conversion that\u0026rsquo;s already known to be reliable.\nThe new API version 20120202 has a matching storage version v20120202storage which becomes the authoratative storage version for the CRD. This conversion is identical to the earlier version.\nAn additional bidirectional conversion between v20110101storage and v20120202storage is also generated. Since both versions have the same structure, this is also trivial.\nVersion Map #  Our version map diagram is becoming useful for seeing the relationship between versions:\nObserve that the prior storage version is still shown, with a bidirectional conversion with the current storage version. Existing users who upgrade their service operator will have their storage upgraded using this conversion. The conversion between storage versions will be generated with the same approach, and with the same structure, as all our other conversions.\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The new storage version, based on this version, is what you\u0026rsquo;d expect:\npackage v20130303storage type Person struct { PropertyBag Id *Guid FirstName *string MiddleName *string // *** New storage ***  LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order. This is to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are unchanged, as they still convert to the same storage versions.\nA new bidirectional conversion between v20120202storage and v20130303storage versions is introduced. When down-converting to v20120202storage, the MiddleName property is stashed in the property bag; when up-converting to v20130303storage, the PropertyBag is checked to see if it contains MiddleName:\npackage v20120202storage import vnext \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the storage Hub version. func (person *Person) ConvertToStorage(dest vnext.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName if middleName, ok := PropertyBag.ReadString(\u0026#34;MiddleName\u0026#34;); ok { dest.MiddleName = middleName // *** New property copied too ***  } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(vnext storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.WriteString(\u0026#34;MiddleName\u0026#34;, source.MiddleName) return nil } Version Map #  A graph of our conversions now starts to show the chaining between storage versions that gives the name to this approach. Bidirectional conversions to and from earlier versions of storage allow conversion between any pairs of API versions.\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // ** Only Id is unchanged ***  FullName string FamilyName string KnownAs string } This is a preview version, but it still gets a dedicated storage version, v20140404previewStorage. The official hub version is left unchanged as v20130303storage.\nStorage Conversion #  The new properties don\u0026rsquo;t exist on prior storage versions, so the generated ConvertToStorage() and ConvertFromStorage() methods used to convert between v20130303storage and v20140404previewStorage must use the PropertyBag to carry the properties:\npackage v20140404previewStorage import vprior \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vprior.Person) error { dest.Id = person.Id // *** Store in the property bag ***  dest.WriteString(\u0026#34;FamilyName\u0026#34;, person.FamilyName) dest.WriteString(\u0026#34;FullName\u0026#34;, person.FullName) dest.WriteString(\u0026#34;KnownAs\u0026#34;, person.KnownAs) return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vprior.Person) error { person.Id = source.Id // *** Read from the property bag ***  if familyName, ok := source.ReadString(\u0026#34;FamilyName\u0026#34;);ok { person.FamilyName = familyName } if fullName, ok := source.ReadString(\u0026#34;FullName\u0026#34;); ok { person.FullName = fullName } if knownAs, ok := source.ReadString(\u0026#34;KnownAs\u0026#34;); ok { person.KnownAs = knownAs } return nil } In the example above, we show first copying all the directly supported properties, then using the property bag. We may not separate these steps in the generated code.\nThese methods are always generated on the storage versions furthest from the hub version, converting towards that version. In the usual case we\u0026rsquo;ll use the import name vnext (or equivalent) but in this case, given we have a preview version, we\u0026rsquo;ll use vprior to emphasize the direction of conversion.\nThis provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of a Person written by the preview release will have no values for FirstName, LastName, and MiddleName. Similarly, an older version won\u0026rsquo;t have FamilyName, FullName nor KnownAs.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding the semantic changes between versions.\nTo allow injection of manual conversion steps, interfaces will be generated as follows:\npackage v20130303storage // AssignableToPersonV20130303 provides methods to augment conversion to storage type AssignableToPersonV20130303 interface { AssignToV20130303(person Person) error } // AssignableFromPersonV20130303 provides methods to augment conversion from storage type AssignableFromPersonV20130303 interface { AssignFromV20130303(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment the generated conversion.\n Outstanding Issue: The interfaces and methods shown above include the version number of the target in order to disambiguate between versions. This is necessitated by having multiple storage versions in flight at the same time, and needing to avoid name collisions. Contrast this with the rolling storage version case study where there\u0026rsquo;s only one active storage version at a time.\nIs there a way we could structure this approach to avoid the need for version numbers in method names?\n The generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPersonV20130303); ok { assignable.AssignToV20130303(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPersonV20130303); ok { assignable.AssignFromV20130303(source) } return nil } Version Map #  Preview releases, by definition, include unstable changes that may differ once the feature reaches general availability.\nWe don\u0026rsquo;t want to make changes to our storage versions based on these speculative changes, so we handle persistence of the preview release with the existing storage version, by way of a down-conversion to v20130303storage:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } As usual, a custom storage version is generated:\npackage v20140404storage type Person struct { PropertyBag AlphaKey *string FamilyName *string LegalName *string Id *Guid KnownAs *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods between the API version v20140404 and the storage version v20140404storage are trivial and not shown.\nFor conversions between storage versions, the preview storage version is not considered - it\u0026rsquo;s out of the main line of processing. Instead, we have a bidirectional conversion between v20130303storage and v20140404storage. As usual, the conversion is implemented further away from the (new) hub version, on v20130303storage.\nWith a large difference in structure between the two versions, the PropertyBag gets a workout:\npackage v20130303storage import vnext \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vnext.Person) error { dest.Id = person.Id dest.WriteString(\u0026#34;FirstName\u0026#34;, person.FirstName) dest.WriteString(\u0026#34;LastName\u0026#34;, person.LastName) dest.WriteString(\u0026#34;MiddleName\u0026#34;, person.MiddleName) if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vnext.Person) error { person.Id = source.Id if firstName, ok := source.ReadString(\u0026#34;FirstName\u0026#34;); ok { person.FirstName = firstName } if middleName, ok := source.ReadString(\u0026#34;MiddleName\u0026#34;); ok { person.MiddleName = middleName } if lastName, ok := source.ReadString(\u0026#34;LastName\u0026#34;); ok { person.LastName = lastName } // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPersonV20140404); ok { assignable.AssignFromV20140404(source) } return nil } To interoperate between different versions of Person, we need to add manual conversions between the storage versions where the schema change occcurs.\nWhen we are converting from v20130303storage up to v20140404storage, we need to use FirstName, LastName and MiddleName to populate AlphaKey, FamilyName, KnownAs and LegalName.\nConversely, When we are converting from v20140404storage down to v20130303storage, we need to use AlphaKey, FamilyName, KnownAs and LegalName to populate FirstName, LastName and MiddleName.\nThese conversions occur in addition to use of the PropertyBag to store those same properties.\npackage v20130303storage import vnext \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignToV20140404(dest vnext.Person) error { if dest.KnownAs == \u0026#34;\u0026#34; { dest.KnownAs = person.FirstName } if dest.FamilyName == \u0026#34;\u0026#34; { dest.FamilyName = person.LastName } if dest.LegalName == \u0026#34;\u0026#34; { dest.LegalName = person.FirstName +\u0026#34; \u0026#34;+ person.MiddleName + \u0026#34; \u0026#34; + person.LastName } if dest.AlphaKey == \u0026#34;\u0026#34; { dest.AlphaKey = person.lastName } } func (person *Person) AssignFrom(source vNext.Person) error { if person.FirstName == \u0026#34;\u0026#34; { person.FirstName = source.KnownAs } if person.LastName == \u0026#34;\u0026#34; { person.LastName = source.FamilyName } if person.MiddleName == \u0026#34;\u0026#34; { person.MiddleName = // ... elided ...  } } For each property we need to consider that it might have already been populated with a more accurate value from the PropertyBag, so we only synthesize values when needed.\nVersion Map #  We can see in our version map that the preview release is still supported, but the associated storage version is not in the main chain of interconvertible versions.\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected, a matching storage version is also generated:\npackage v20150505storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string SortKey *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods that are generated between the v20140404storage and v20150505storage versions:\npackage v20140404 import vNext \u0026#34;v20150505storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vNext.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source vNext.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } While SortKey appears at the end of the list of assignments in the first method, the mirror assignment of AlphaKey appears at the start of the list in the second method. In both cases the properties are shown in alphabetical order.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nFor users staying up to date with releases of the service operator, this will likely have no effect - but users still using the original release (storage version v2011-01-01storage) will need to update to an intermediate release before adopting this version.\nAn alternative approach would be to always support conversion from every storage version, even if the related API version has been dropped:\nThis would allow users to upgrade from almost any older version of the service operator. (\u0026ldquo;Almost\u0026rdquo; because we would still have older versions drop off when they are retired by ARM.)\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions; 17 of these involved changes to letter case alone. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs that make up our storage version:\npackage v20160606storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string MailingAddress *Address // *** New ***  SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type of Person as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods between the API version v20160606 and the storage version v201606061 get generated in the expected way:\npackage v20160606 import storage \u0026#34;v20160606storage\u0026#34; // ConvertTo converts this Person to the Storage version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest storage.Address) error { dest.City = address.City dest.Street = address.Street if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(storage.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source storage.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version drop out, allowing users of the three prior versions of the service operator to upgrade cleanly:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage versions are identical to those used previously and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import storage \u0026#34;v20170707storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional field that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  How often does optionality change? #  At the time of writing, there are 100 version-to-version changes where fields became optional in the later version of the API, and 99 version-to-version changes where fields became required.\nVersion 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version is generated to match, with prior conversions using the property bag to store additional properties:\npackage v20180808storage type Address struct { PropertyBag City *string Country *string FullAddress *string PostCode *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nUsers still running earlier releases of the service operator that are using 2014-04-04 or earlier will need to install an intermediate release in order to upgrade to this one.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Latitude double Longitude double } The storage version gets generated in a straightforward way:\npackage v20190909storage type Location struct { PropertyBag City *string Country *string FullAddress *string Latitude *double Longitude *double PostCode *string } Storage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename of the type (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20180808storage // *** Updated storage version *** import vNext \u0026#34;v20190909storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest vNext.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;vNext.Location{} // ** New Type ***  err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest vNext.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":17,"href":"/azure-service-operator/design/versioning/case-studies/fixed-storage-version/","title":"Fixed Storage Version","section":"Case Studies","content":"Case Study - Fixed Storage Version #  This case study explores the alternative solution of using a fixed storage version where the schema of the storage version is modified to handle each additional release.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus on specific details, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v1 type Person struct { FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} By convention, a fixed storage version is noted as v1\nEvery property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we are only concerned with a single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import \u0026#34;v1\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*v1.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest v1.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*v1.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } These four methods will be automatically generated in order to handle much of the boilerplate required for conversion.\nConversion in each direction is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nSince they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map doesn\u0026rsquo;t look much like the traditional hub and spoke model, but this will change as we work through this case study:\nVersion 2012-02-02 - No Change #  In this release of the CRM service, despite changes elsewhere in the service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  Conversions between version v20120202 and the v1 storage version will be identical to those generated for the earlier v20110101 version.\nVersion Map #  Our hub and spoke diagram is becoming useful for seeing the relationship between versions:\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The storage version is updated with the addition of the new property:\npackage v1 type Person struct { FirstName *string Id *Guid LastName *string MiddleName *string // *** New *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import \u0026#34;v1\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are essentially unchanged.\nVersion Map #  A graph of our conversions now starts to show the expected hub and spoke structure:\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // *** Only Id is unchanged from the prior version ***  FullName string FamilyName string KnownAs string } The storage version gets modified to add these new properties:\npackage v1 type Person struct { FamilyName *string // *** New ***  FirstName *string FullName *string // *** New ***  Id *Guid KnownAs *string // *** New ***  LastName *string MiddleName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The new properties are copied to and from the storage version. Ensuring that all properties are optional makes it possible to leave the unused properties empty.\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.FullName = person.FullName dest.Id = person.Id dest.KnownAs = person.KnownAs return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FamilyName = source.FamilyName person.FullName = source.FullName person.Id = source.Id person.KnownAs = source.KnownAs return nil } This provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of Person written by the preview release will have no values for FirstName, LastName, and MiddleName.\nSimilarly, the storage version of Person written by an earlier release will have no values for KnownAs, FamilyName, or FullName.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding of the semantic changes between versions.\nTo allow injection of manual conversion steps, two interfaces will be generated as follows:\npackage v1 // AssignableToPerson is implemented on an API version // of `Person` to update the storage version type AssignableToPerson interface { AssignTo(person Person) error } // AssignableFromPerson is implemented on an API version // of `Person` to populate it from the storage version type AssignableFromPerson interface { AssignFrom(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment (not replace) the generated conversion.\nThe generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { // … elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { // … elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } Implementations of these interfaces are called after the generated boilerplate conversion has completed. This gives developers freedom to make any changes required as they won\u0026rsquo;t be overwritted by the effects of generated code.\nVersion Map #  The preview version just appears as another version in our hub and spoke diagram:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } The two new properties need to be added to the storage version to allow this to be stored:\npackage v1 type Person struct { Id *Guid AlphaKey *string // *** New ***  FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string // *** New ***  MiddleName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Issue: Property Bloat #  As our API evolves over time, our storage version is accumulating all the properties that have ever existed, bloating the storage version with obsolete properties that are seldom (if ever) used.\nFor example, only preview users would ever have used FullName as it became LegalName in the general release; most users will never use FullName.\nWe have a problem, however. We can\u0026rsquo;t remove the FullName property as that is a breaking change that will negatively impact users who have used the preview version. Both properties will need to be retained permanently.\nThis violates the pay for play principle - even users who adopt the operator after the 2014-04-04 release will have to deal with the complexity even though they\u0026rsquo;ve never used the preview version.\nStorage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods for the new version of Person are generated as expected, copying across values and invoking the AssignableToPerson and AssignableFromPerson interfaces if present:\npackage v20140404 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.AlphaKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } For older versions of Person, the conversion methods are essentially unchanged as the properties they require are still present on the storage version.\nTo interoperate between different versions of Person, we need to add some manual conversions.\nWhen a newer version of Person is written to storage, we need to also populate FirstName, LastName and MiddleName to allow older versions to be requested. Similarly, when an older version of Person is written, we need to populate AlphaKey, FamilyName, KnownAs and LegalName so that newer versions can be requested.\nTo avoid repetition of code across multiple implementations of AssignTo() and AssignFrom(), we write some helper methods on the storage version:\npackage v1 func (person *Person) PopulateFromFirstMiddleLastName(firstName string, middleName string, lastName string) { person.KnownAs = firstName person.FamilyName = lastName person.LegalName = firstName +\u0026#34; \u0026#34;+ middleName + \u0026#34; \u0026#34; + lastName person.AlphaKey = lastName } func (person *Person) PopulateLegacyProperties() { person.FirstName = person.KnownAs person.FamilyName = person.FamilyName person.MiddleName = // ... elided ... } These methods are manually authored, so the names are arbitary. However, we will provide some guidance for implementers to encouarage consistency as the operator is updated and improved.\nWith these methods available, implementing the interface AssignableToPerson becomes straightforward. For the 2011-01-01 release of Person:\npackage v20110101 import v1 func (person *Person) AssignTo(dest v1.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, \u0026#34;\u0026#34;, person.LastName) return nil } For the 2013-03-3 release that introduced MiddleName the code is very similar:\npackage v20130303 import v1 func (person *Person) AssignTo(dest v1.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, person.MiddleName, person.LastName) return nil } Version Map #  We can see in our version map that the preview release is still supported:\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected the storage version is also extended:\npackage v1 type Person struct { Id *Guid AlphaKey *string // ** Debris ***  FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string MiddleName *string SortKey *string // ** New *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Again, we see the issue of property bloat where the storage type needs to have both AlphaKey and SortKey for backward compatibility.\nStorage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods, as shown here for the 2014-04-04 version of Person:\npackage v20140404 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.AlphaKey // *** Store it the old way ***  dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } For forward compatibility, the ConvertToStorage() method for version 2014-04-04 populates both AlphaKey and SortKey.\nFor the 2015-05-05 release of Person, the methods are similar:\npackage v20150505 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.AlphaKey = person.SortKey // *** Back compatibility ***  dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.SortKey // *** New ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName person.SortKey = source.SortKey // *** New ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } Here we can see the 2015-05-05 version of ConvertToStorage() populates AlphaKey for backwards compatiblity.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nOf these 17 of these involved changes to letter case alone.\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with a mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs used in storage:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FirstName *string FullName *string KnownAs *string LastName *string LegalName *string MailingAddress *Address // *** New ***  MiddleName *string SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods get generated in the expected way:\npackage v20160606 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest v1.Address) error { dest.Street = address.Street dest.City = address.City if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source v1.Person) error { person.AlphaKey = source.SortKey // *** Rename still is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(source.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source v1.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version (2012-02-02) drop out:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage version is identical to that generated used previously (because all properties are marked as optional anyway) and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional property that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  Note that the 2013-03-03 version has now dropped out:\nHow often do optionality changes happen? #  At the time of writing, there are 100 version-to-version changes where properties became optional in the later version of the API, and 99 version-to-version changes where properties became required.\nIssue: Property Amnesia #  Our code generator only knows about properties defined in current versions of the API. Once an API version has been excluded (or if the JSON schema definition is no longer available), the generator completely forgets about older properties.\nThis means that we now have a significant issue - with the versions 2011-01-01, 2012-02-02 and 2013-03-03 no longer included, we can no longer fully generate the expected storage version automatically.\nThe properties FirstName, MiddleName and LastName no longer exist - they are only defined on the earliest versions of the API.\nAs a consequence, the storage version generated for this release will be this:\npackage v1 type Person struct { AlphaKey *string FamilyName *string FullName *string Id *Guid KnownAs *string LegalName *string MailingAddress *Address SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} References to FirstName, MiddleName and LastName will disappear across all the generated ConvertFromStorage() and ConvertToStorage() methods as well.\nThis is a breaking change. Existing long time users of the service operator, with CRDs serialized containing any of the properties FirstName, MiddleName and LastName will find that their clusters break when this version of the service operator is deployed.\nWithout explicit intervention in the operator, their only mitigation would be to revert back to the earlier version of the service operator and never upgrade it ever again.\nThe root cause of the problem is that the CRD has never been modified (upgraded) - once first deployed the resource is never modified, only loaded and reconciled.\nTo retain backward compatibility we would need to manually merge the generated code with prior versions to retain both the property definitions and the conversion support previously generated.\nThere are many problems with this approach:\n Violates the goal of avoiding changes to generated files. Merging these changes would be tedious and error prone. Changes would need to be re-merged for every subsequent release of the service operator. Manual conversions would need to be written for newer API versions. Over time, the number of classes requiring manual attention would grow.  Version 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version is updated with the additional properties:\npackage v1 type Address struct { City *string Country *string // *** New ***  FullAddress *string // *** New ***  PostCode *string // *** New ***  Street *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nDropping those releases triggers a reccurrance of the Property Amnesia issue discussed above - the FullName property (only included in the 2014-04-04preview release) has been forgotten.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Lattitude double Longitide double } The storage version for Location gets generated in a straightforward way:\npackage v1 type Location struct { City *string Country *string FullAddress *string PostCode *string } We also need to retain the Address struct - if we drop support for it, we will be breaking existing users as we will be unable to deserialize their resources.\ntype Address struct { City *string Country *string FullAddress *string PostCode *string Street *string } Issue: Type collision #  We run into a problem with the storage version of the Person type:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Address // *** Existing property ***  MailingAddress *Location // *** New property with the same name ***  SortKey *string } We can\u0026rsquo;t have two properties with the same name, either we create a single property that handles both types, or we must change the name of one of them.\nWe can\u0026rsquo;t change the name of the existing property, because that would break existing users who already have serialised resources. For similar reasons, we can\u0026rsquo;t change the type of the existing property.\nSo we need to change the name of the new property - and need to do so in a deterministic way to ensure that we generate the same code each time. One way would be to suffic the type name with the version:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Address MailingAddress_v20190909 *Location SortKey *string } Needing to do this is a wart, but one with a nasty sting in the tail:\nWhen the original MailingAddress property ages out of the system (see property amnesia, above), we\u0026rsquo;ll no longer have a collision, and the storage struct will be generated with this structure:\npackage v1 type Person struct { Id *Guid AlphaKey *string FamilyName *string FullName *string KnownAs *string LegalName *string MailingAddress *Location SortKey *string } This is a major breaking change.\nNot only will this break older users who have serialized resources using Address, but it will also break newer users who have serialized resources using Location.\nStorage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20170707 // *** Updated storage version *** import v1 // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest v1.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;storage.Location{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest v1.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":18,"href":"/azure-service-operator/design/versioning/case-studies/rolling-storage-versions/","title":"Rolling Storage Versions","section":"Case Studies","content":"Case Study - Rolling Storage Versions #  This case study explores the recommended solution of using a rolling storage version where we update the storage schema of each resource each release of the service operator. We\u0026rsquo;ll keep the storage version up to date with the latest GA release of each resource.\nFor the purposes of discussion, we\u0026rsquo;ll be following the version by version evolution of a theoretical ARM service that provides customer resource management (CRM) services. Synthetic examples are used to allow focus on specific scenarios one by one, providing motivation for specific features.\nExamples shown are deliberately simplified in order to focus, and therefore minutiae should be considered motivational, not binding. Reference the formal specification for precise details.\nVersion 2011-01-01 - Initial Release #  The initial release of the CRM includes a simple definition to capture information about a particular person:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } We\u0026rsquo;re not reusing the API version directly as our storage version. Instead, we define a separate (independent) type with a similar structure:\npackage v20110101storage type Person struct { PropertyBag FirstName *string Id *Guid LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Every property is marked as optional. Optionality doesn\u0026rsquo;t matter at this point, as we currently have only single version of the API. However, as we\u0026rsquo;ll see with later versions, forward and backward compatibility issues would arise if they were not optional.\nThe PropertyBag type provides storage for other properties, plus helper methods. It is always included in storage versions, but in this case will be unused. The method Hub() marks this version as the storage schema.\nStorage Conversion #  We need to implement the Convertible interface to allow conversion to and from the storage version:\npackage v20110101 import storage \u0026#34;v20110101storage\u0026#34; // ConvertTo converts this Person to the Hub storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertToStorage converts this Person to a storage version func (person *Person) ConvertToStorage(dest storage.Person) error { // Copy simple properties across  dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName return nil } // ConvertFrom converts from the Hub storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } // ConvertFrom converts from a storage version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // Copy simple properties across  person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName return nil } Conversion is separated into two methods (e.g. ConvertFrom() and ConvertFromStorage()) to allow for reuse of the ConvertFromStorage() methods for conversion of nested complex properties, as we\u0026rsquo;ll see later on.\nThese methods will be automatically generated in order to handle the majority of the required conversions. Since they never change, the ConvertTo() and ConvertFrom() methods are omitted from the following discussion.\nVersion Map #  With only two classes, our version map doesn\u0026rsquo;t look much like the traditional hub and spoke model, but this will change as we work through this case study:\nVersion 2012-02-02 - No Change #  In this release of the CRM service, there are no changes made to the structure of Person:\npackage v20120202 type Person struct { Id Guid FirstName string LastName string } Storage Conversion #  Conversions with the upgraded storage version will need to be trivially modified by changing the import statements for the referenced types.\nVersion Map #  Our hub and spoke diagram is becoming useful for seeing the relationship between versions:\nObserve that the prior storage version is still shown, with a one way conversion to the current storage version. Existing users who upgrade their service operator will have their storage upgraded using this conversion. The conversion between storage versions will be generated with the same approach, and with the same structure, as all our other conversions.\nVersion 2013-03-03 - New Property #  In response to customer feedback, this release of the CRM adds a new property to Person to allow a persons middle name to be stored:\npackage v20130303 type Person struct { Id Guid FirstName string MiddleName string // *** New ***  LastName string } The new storage version, based on this version, updates accordingly:\npackage v20130303storage type Person struct { PropertyBag Id *Guid FirstName *string MiddleName *string // *** New storage ***  LastName *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversions #  Conversions to and from earlier versions of Person are unchanged, as those versions do not support MiddleName. For the new version of Person, the new property will be included in the generated methods:\npackage v20130303 import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FirstName = person.FirstName dest.Id = person.Id dest.LastName = person.LastName dest.MiddleName = person.MiddleName // *** New property copied too ***  return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.FirstName = source.FirstName person.Id = source.Id person.LastName = source.LastName person.MiddleName = source.MiddleName // *** New property copied too ***  return nil } The new property is shown at the end of the list not because it is new, but because values are copied across in alphabetical order. This is to guarantee that code generation is deterministic and generates the same result each time.\nConversion methods for earlier API versions of Person are essentially unchanged. The import statement at the top of the file will be updated to the new storage version; no other changes are necessary.\nVersion Map #  A graph of our conversions now starts to show the expected hub and spoke structure, with conversions from earlier versions of storage allowing easy upgrades for existing users of the service operator.\nHow often are new properties added? #  At the time of writing, there were 381 version-to-version changes where the only change between versions was solely the addition of new properties. Of those, 249 were adding just a single property, and 71 added two properties.\nVersion 2014-04-04 Preview - Schema Change #  To allow the CRM to better support cultures that have differing ideas about how names are written, a preview release of the service modifies the schema considerably:\npackage v20140404preview type Person struct { Id Guid // ** Only Id is unchanged ***  FullName string FamilyName string KnownAs string } This is a preview version, so the storage version is left unchanged (see below).\nStorage Conversion #  The new properties don\u0026rsquo;t exist on the storage version of Person, so the generated ConvertToStorage() and ConvertFromStorage() methods use the PropertyBag to carry the properties:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.Id = person.Id // *** Store in the property bag ***  dest.WriteString(\u0026#34;FamilyName\u0026#34;, person.FamilyName) dest.WriteString(\u0026#34;FullName\u0026#34;, person.FullName) dest.WriteString(\u0026#34;KnownAs\u0026#34;, person.KnownAs) return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.Id = source.Id // *** Read from the property bag ***  person.FamilyName = source.ReadString(\u0026#34;FamilyName\u0026#34;) person.FullName = source.ReadString(\u0026#34;FullName\u0026#34;) person.KnownAs = source.ReadString(\u0026#34;KnownAs\u0026#34;) return nil } In the example above, we show first copying all the directly supported properties, then using the property bag. We may not separate these steps in the generated code.\nThis provides round-trip support for the preview release, but does not provide backward compatibility with prior official releases.\nThe storage version of Person written by the preview release will have no values for FirstName, LastName, and MiddleName.\nThese kinds of cross-version conversions cannot be automatically generated as they require more understanding of the semantic changes between versions.\nTo allow injection of manual conversion steps, interfaces will be generated as follows:\npackage v20130303storage // AssignableToPerson provides methods to augment conversion to storage type AssignableToPerson interface { AssignTo(person Person) error } // AssignableFromPerson provides methods to augment conversion from storage type AssignableFromPerson interface { AssignFrom(person Person) error } This interface can be optionally implemented by API versions (spoke types) to augment the generated conversion.\nThe generated ConvertToStorage() and ConvertFromStorage() methods will test for the presence of this interface and will call it if available:\npackage v20140404preview import storage \u0026#34;v20130303storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableTo); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { // … property copying and property bag use elided …  // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFrom); ok { assignable.AssignFrom(source) } return nil } Version Map #  Preview releases, by definition, include unstable changes that may differ once the feature reaches general availability.\nWe don\u0026rsquo;t want to make changes to our storage versions based on these speculative changes, so we handle persistence of the preview release with the existing storage version:\nVersion 2014-04-04 - Schema Change #  Based on feedback generated by the preview release, the CRM schema changes have gone ahead with a few minor changes:\npackage v20140404 type Person struct { Id Guid LegalName string // *** Was FullName in preview ***  FamilyName string KnownAs string AlphaKey string // *** Added after preview *** } No longer being a preview release, the storage version is also regenerated:\npackage v20140404storage type Person struct { PropertyBag AlphaKey *string FamilyName *string LegalName *string Id *Guid KnownAs *string } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The ConvertToStorage() and ConvertFromStorage() methods for the new version of Person are generated as expected, copying across values and invoking the AssignableToPerson and AssignableFromPerson interfaces if present:\npackage v20140404 import storage \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.AlphaKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.AlphaKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Check for the interface and use it if found ***  if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } The changes to the structure of Person mean that our prior conversion methods need to change too. For properties that are no longer present on the storage version, they now need to use the PropertyBag to stash the required values.\nFor example, the 2011-01-01 version of Person now has these conversion methods:\npackage v20110101 import storage \u0026#34;v20140404storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.Id = person.Id dest.WriteString(\u0026#34;FirstName\u0026#34;, person.FirstName) dest.WriteString(\u0026#34;LastName\u0026#34;, person.LastName) if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.Id = source.Id person.FirstName = source.ReadString(\u0026#34;FirstName\u0026#34;) person.LastName = source.ReadString(\u0026#34;LastName\u0026#34;) if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } To interoperate between different versions of Person, we need to add manual conversions.\nWhen a newer version of Person is written to storage, we need to also populate FirstName, LastName and MiddleName within the PropertyBag to allow older versions to be requested.\nWhen an older version of Person is written, we need to populate AlphaKey, FamilyName, KnownAs and LegalName so that newer versions can be requested.\nTo avoid repetition of code across multiple implementations of AssignTo() and AssignFrom(), we might write some helper methods on the storage version:\npackage v20140404storage func (person *Person) PopulateFromFirstMiddleLastName(firstName string, middleName string, lastName string) { person.KnownAs = firstName person.FamilyName = lastName person.LegalName = firstName +\u0026#34; \u0026#34;+ middleName + \u0026#34; \u0026#34; + lastName person.AlphaKey = lastName } func (person *Person) PopulateLegacyFields() { person.WriteString(\u0026#34;FirstName\u0026#34;, person.KnownAs) person.WriteString(\u0026#34;LastName\u0026#34;, person.FamilyName) person.WriteString(\u0026#34;MiddleName\u0026#34;, ... elided ...) } With these methods available, implementing the interface AssignableToPerson becomes straightforward. For the 2011-01-01 release of Person:\npackage v20110101 import storage \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignTo(dest storage.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, \u0026#34;\u0026#34;, person.LastName) } For the 2013-03-03 release that introduced MiddleName the code is very similar:\npackage v20130303 import storage \u0026#34;v20140404storage\u0026#34; func (person *Person) AssignTo(dest storage.Person) error { dest.PopulateFromFirstMiddleLastName( person.FirstName, person.MiddleName, person.LastName) return nil } Version Map #  We can see in our version map that the preview release is still supported, but is now backed by the GA release of the version:\nVersion 2015-05-05 - Property Rename #  The term AlphaKey was found to be confusing to users, so in this release of the API it is renamed to SortKey. This better reflects its purpose of sorting names together (e.g. so that the family name McDonald gets sorted as though spelt MacDonald).\npackage v20150505 type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string // *** Used to be AlphaKey *** } As expected the storage version is also regenerated:\npackage v20150505storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string SortKey *string // *** Used to be AlphaKey *** } // Hub marks this type as a conversion hub. func (*Person) Hub() {} Storage Conversion #  By documenting the renames in the configuration of our code generator, this rename will be automatically handled within the ConvertTo() and ConvertFrom() methods, as shown here for the 2014-04-04 version of Person:\npackage v20140404 import storage \u0026#34;v20150505storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Rename is automatically handled ***  if assignable, ok := person.(AssignableToPerson); ok { assignable.AssignTo(dest) } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey // *** Rename is automatically handled ***  person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName if assignable, ok := person.(AssignableFromPerson); ok { assignable.AssignFrom(source) } return nil } While SortKey appears at the end of the list of assignments in the first method, the mirror assignment of AlphaKey appears at the start of the list in the second method.\nIssue: Instability of manual conversions #  The earlier manually authored conversions for AlphaKey will also need to be modified. While this change looks simple, it\u0026rsquo;s a symptom of an underlying problem: with each release, the map of required conversions is completely new (no reuse of older conversions.\nThis both requires the introduction of additional conversions to support older versions (as has happened here) and the modification of existing conversions.\nTo illustrate, consider the manual code (AssignTo() and AssignFrom()) that was written to augment conversion between v20110101.Person and v20140404storage.Person.\nNow that we\u0026rsquo;ve moved to a new release, there is no direct conversion between those two versions (see the version map below) - so the manual conversion just drops off and is ignored. If this is not detected, we may end up corrupting resource definitions as they are converted.\nIn many cases, updating manual conversion code will only require changing imported package references, but this does introduce risk as it involves modifying the code, even if trivially.\nThere will certainly also be cases where the conversion is much harder to convert.\nWe also have the issue seen above where introduction of a change requires additional conversions to be written for older versions.\nVersion Map #  Here we see our horizon policy coming into effect, with support for version 2011-01-01 being dropped in this release:\nFor users staying up to date with releases of the service operator, this will likely have no effect - but users still using the original release (storage version v2011-01-01storage) will need to update to an intermediate release before adopting this version.\nAn alternative approach would be to always support conversion from every storage version, even if the related API version has been dropped. This would allow users to upgrade from any older version of the service operator.\nHow often do property renames happen? #  At the time of writing, there were nearly 60 cases of properties being renamed between versions; 17 of these involved changes to letter case alone. (Count is somewhat inexact because renaming was manually inferred from the similarity of names.)\nVersion 2016-06-06 - Complex Properties #  With some customers expressing a desire to send physical mail to their customers, this release extends the API with mailing address for each person.\npackage v20160606 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress Address } We now have two structs that make up our storage version:\npackage v20160606storage type Person struct { PropertyBag Id *Guid LegalName *string FamilyName *string KnownAs *string MailingAddress *Address // *** New ***  SortKey *string } type Address struct { PropertyBag City *string Street *string } // Hub marks this type of Person as a conversion hub. func (*Person) Hub() {} Storage Conversion #  The required ConvertToStorage() and ConvertFromStorage() methods get generated in the expected way:\npackage v20160606 import storage \u0026#34;v20160606storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName dest.SortKey = person.AlphaKey // *** Copy the mailing address over too ***  address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version func (address *Address) ConvertToStorage(dest storage.Address) error { dest.City = address.City dest.Street = address.Street if assignable, ok := person.(AssignableToAddress); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertFrom converts from the Hub version to this version. func (person *Person) ConvertFromStorage(source storage.Person) error { person.AlphaKey = source.SortKey person.FamilyName = source.FamilyName person.Id = source.Id person.KnownAs = source.KnownAs person.LegalName = source.LegalName // *** Copy the mailing address over too ***  if storage.MailingAddress != nil { address := \u0026amp;Address{} err := address.ConvertFromStorage(storage.Address) person.MailingAddress = address } if assignable, ok := person.(AssignableFromPerson); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } // ConvertFromStorage converts from the hub storage version to this version func (address *Address) ConvertFromStorage(source storage.Address) error { address.Street = source.Street address.City = source.City if assignable, ok := person.(AssignableFromAddress); ok { err := assignable.AssignFrom(source) if err != nill { return err } } return nil } We\u0026rsquo;re recursively applying the same conversion pattern to Address as we have already been using for Person. This scales to any level of nesting without the code becoming unweildy.\nVersion Map #  Again we see the oldest version drop out, allowing users of the three prior versions of the service operator to upgrade cleanly:\nVersion 2017-07-07 - Optionality changes #  In the 2016-06-06 version of the API, the MailingAddress property was mandatory. Since not everyone has a mailing address (some people receive no physical mail), this is now being made optional.\nThe change to the API declarations is simple:\npackage v20170707 type Address struct { Street string City string } type Person struct { Id Guid LegalName string FamilyName string KnownAs string SortKey string MailingAddress *Address // *** Was mandatory, now optional *** } Storage Conversion #  The storage versions are identical to those used previously and are not shown here.\nWhat does change is the ConvertToStorage() method, which now needs to handle the case where the MailingAddress has not been included:\npackage v20170707 import storage \u0026#34;v20170707storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { dest.SortKey = person.AlphaKey dest.FamilyName = person.FamilyName dest.Id = person.Id dest.KnownAs = person.KnownAs dest.LegalName = person.LegalName // *** Need to check whether we have a mailing address to copy ***  if person.MailingAddress != nil { address := \u0026amp;storage.Address{} err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we instead had an optional field that became required in a later version of the API, the generated code for ConvertToStorage() would become simpler as the check for nil would not be needed.\nVersion Map #  How often does optionality change? #  At the time of writing, there are 100 version-to-version changes where fields became optional in the later version of the API, and 99 version-to-version changes where fields became required.\nVersion 2018-08-08 - Extending nested properties #  Defining an address simply as Street and City has been found to be overly simplistic, so this release makes changes to allow a more flexible approach.\npackage v20180808 type Address struct { // FullAddress shows the entire address as should be used on postage  FullAddress string City string Country string PostCode string } As before, the storage version changes to match, with prior conversions using the property bag to store additional properties:\npackage v20180808storage type Address struct { PropertyBag City *string Country *string FullAddress *string PostCode *string } These changes are entirely similar to those previously covered in version 2014-04-04, above.\nVersion Map #  In this release, we see that support for both 2014-04-04 and the preview version 2014-04-04preview has been dropped:\nUsers still running earlier releases of the service operator that are using 2014-04-04 or earlier will need to install an intermediate release in order to upgrade to this one.\nVersion 2019-09-09 - Changing types #  Realizing that some people get deliveries to places that don\u0026rsquo;t appear in any formal database of addresses, in this release the name of the type changes to Location and location coordinates are added:\npackage v20190909 type Location struct { FullAddress string City string Country string PostCode string Latitude double Longitude double } The storage version gets changed in a straightforward way:\npackage v20190909storage type Location struct { PropertyBag City *string Country *string FullAddress *string Latitude *double Longitude *double PostCode *string } Storage Conversion #  The conversion methods need to change as well. If we configure metadata detailing the rename (as we did for properties in version 2015-05-05), we can generate the required conversions automatically:\npackage v20170707 // *** Updated storage version *** import storage \u0026#34;v20190909storage\u0026#34; // ConvertTo converts this Person to the Hub version. func (person *Person) ConvertToStorage(dest storage.Person) error { // ... elided properties ...  if person.MailingAddress != nil { address := \u0026amp;storage.Location{} // ** New Type ***  err := person.MailingAddress.ConvertToStorage(address) if err != nil { return err } dest.MailingAddress = address } if assignable, ok := person.(AssignableToPerson); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } // ConvertToStorage converts this Address to the hub storage version // ** Different parameter type for dest *** func (address *Address) ConvertToStorage(dest storage.Location) error { dest.Street = address.Street dest.City = address.City // *** Interface has been renamed too **  if assignable, ok := person.(AssignableToLocation); ok { err := assignable.AssignTo(dest) if err != nill { return err } } return nil } If we don\u0026rsquo;t include metadata to capture type renames, the conversion can be manually injected by implementing the AssignableToLocation interface.\nVersion Map #  How often do properties change their type? #  At the time of writing, there are 160 version-to-version changes where the type of the property changes. This count excludes cases where an optional property become mandatory, or vice versa.\n"},{"id":19,"href":"/azure-service-operator/introduction/tutorial-cosmosdb/","title":"Tutorial: CosmosDB to-do List","section":"User’s Guide","content":"Tutorial: CosmosDB to-do List #  Follow the guided example to create a to-do list application backed by CosmosDB. The CosmosDB is hosted in Azure but created easily via kubectl and Azure Service Operator!\n"},{"id":20,"href":"/azure-service-operator/introduction/tutorial-postgresql/","title":"Tutorial: PostgreSQL Votes","section":"User’s Guide","content":"Tutorial: PostgreSQL Votes #  Follow the guided example to create a simple voting application backed by PostgreSQL. The PostgreSQL Server and Database are hosted in Azure but created easily via kubectl and Azure Service Operator!\n"},{"id":21,"href":"/azure-service-operator/design/versioning/","title":"Versioning","section":"Design \u0026 Specifications","content":"Versioning #  Specification for how storage versioning will operate for code generated CRD definitions.\nWe\u0026rsquo;re generating a large number of CRD definitions based on the JSON schema definitions published for Azure Resource Manager use.\nGoals #  Principle of Least Surprise: The goal of the service operator is to allow users to consume Azure resources without having to leave the tooling they are familiar with. We therefore want to do things in the idiomatic Kubernetes fashion, so that they don\u0026rsquo;t experience any nasty surprises.\nAuto-generated conversions: As far as practical, we want to autogenerate the schema for storage use along with the conversions to and from the actual ARM API versions. Hand coding all the required conversions doesn\u0026rsquo;t scale across all the different Azure sevices, especially with the ongoing rate of change.\nAllow for hand coded conversions: While we expect to use code generation to handle the vast majority of needed conversions, we anticipate that some breaking API changes will require part of the conversion to be hand coded. We need to make it simple for these conversions to be introduced while still autogenerating the majority of the conversion. We also want to minimize the need for these conversions to be revisited and maintained over time.\nNo modification of generated files. Manual modification of generated files is a known antipattern that greatly increases the complexity and burden of updates. If some files have been manually changed, every difference showing after code generation needs to be manually reviewed before being committed. This is tedious and error prone because the vast majority of auto generated changes will be perfectly fine. Worse, this process would need to be repeated every time we want to update the operator.\nCompliance with Kubernetes versioning. To quote Kubebuilder\u0026rsquo;s documentation:\n In Kubernetes, all versions must be safely round-tripable through each other. This means that if we convert from version 1 to version 2, and then back to version 1, we must not lose information. Thus, any change we make to our API must be compatible with whatever we supported in v1, and also need to make sure anything we add in v2 is supported in v1.\n Consistency of experience: Early adopters should have a similar experience with the latest release of the service operator as new users who are adopting it for the first time. We don\u0026rsquo;t want early adopters to be penalized for their enthusiasm.\nNon-Goals #  Coverage of every case by code generation: While it\u0026rsquo;s likely that very high coverage will be achievable with code generation, we don\u0026rsquo;t believe that it will be practical to handle every possible situation automatically. It\u0026rsquo;s therefore necessary for the solution to have some form of extensibility allowing for the injection of hand written code.\nOther Constraints #  Unlike the typical situation with a hand written service operator, we don\u0026rsquo;t have complete control over the schema we are publishing for custom resources - we\u0026rsquo;re deriving the CRD schema from the ARM JSON schema published online. This somewhat paints us into a corner where some issues that would be easily avoided with a hand-coded schema have to be faced head on.\nCase Studies #  There are three case studies that accompany this specification, each one walking through one possible solution and showing how it will perform as a synthetic ARM style API evolves over time.\nThe Chained Versions case study shows how the preferred solution adapts to changes as the API is modified.\nThe Rolling Versions case study shows an alternative that works well but falls down when hand coded conversions are introduced between versions.\nThe Fixed Version case study shows how a popular alternative would fare, calling out some specific problems that will occur.\nTL;DR: Using a fixed storage version appears simpler at first, and works well as long as the changes from version to version are simple. However, when the changes become complex (as they are bound to do over time), this approach starts to break down. While there is up front complexity to address with chained storage versions, the approach doesn\u0026rsquo;t break down over time and we can generate useful automated tests for verification. The rolling storage version approach is viable, but requires additional ongoing maintenance when manual conversions are introduced between versions.\nExamples shown in this document are drawn from the case studies.\nProposed Solution #  In summary:\n  For each supported version of an Azure Resource Type, we will define a synthetic storage type type that will be used for serialization of that versions of the API. The latest non-preview version will be tagged as the canonical storage type for Kubernetes.\n  Automatically generated conversions will allow for lossless conversions between the externally exposed API versions of resources and the related storage versions. Additional conversions will be generated to allow upgrade or downgrade between adjacent storage versions.\n  External metadata that we bundle with the code generator will document common changes that occur over time (including property and type name changes), extending the coverage of our automatically generated conversions.\n  For cases where automatically generated conversion is not sufficient, standard extension points for each resource type will allow hand-coded conversion steps to be injected into the process at key points.\n  Each of these four points is expanded upon in detail below.\nDefining Storage Versions #  We\u0026rsquo;ll base the schema of the storage versions on the corresponding API version, with the following modifications:\nAll properties will be defined as optional allowing for back compatibility with prior versions of the API that might not have included specific properties.\nInclusion of a property bag to provide for storage for properties present in other versions of the API that are not present in this version.\nIf a resource type is dropped from later releases of the ARM API, we will still generate a storage type based on the latest available release of that type. We need to do this in order to maintain backward compatibility with existing installations of the service operator.\nUsing a purpose designed types for storage avoids a number of version-to-version compatibility issues that can arise if the API version itself is used directly for storage.\nTo illustrate, if the API version defined the following Person type:\npackage v20110101 type Person struct { Id Guid FirstName string LastName string } Then the generated storage (hub) version will be:\npackage v20110101storage type Person struct { PropertyBag Id *Guid FirstName *string LastName *string } Using the latest version of the API as the basis for our storage version gives us maximum compatibility for the usual case, where a user defines their custom resource using the latest available version.\nIf a type has been dropped from the ARM API, we will still generate a storage schema for it based on the last ARM API version where it existed; this helps to ensure backward compatibility with existing service operator deployments. For example, if Person was dropped in favour of Party type (that can capture companies and organizations as well), we will still continue to generate a storage version of Person to allow deserialization by existing service operator installations as a part of their upgrade process.\nSequestering additional properties away within a property bag in the storage schema is more robust than using separate annotations as they are less subject to arbitrary modification by users. This allows us to largely avoid situations where well meaning (but uninformed) consumers of the service operator innocently make changes that result in the operator becoming failing. We particularly want to avoid this failure mode because recovery will be difficult - restoration of the modified/deleted information may be impractical or impossible.\nThe latest non-preview storage version will be selected as the definitive storage version (or hub version) for use by the controller.\nGenerated conversion methods #  Each of the structs generated for ARM API will have the normal ConvertTo() and ConvertFrom() methods generated automatically, implementing the required Convertible interface:\n// ConvertTo converts this Person to the matching storage version. func (person *Person) ConvertTo(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertToStorage(p) } // ConvertFrom converts from the matching storage version func (person *Person) ConvertFrom(raw conversion.Hub) error { p := raw.(*storage.Person) return ConvertFromStorage(p) } As shown, these methods will delegate to two strongly typed helper methods (ConvertToStorage() and ConvertFromStorage()) that are generated to handle the process of copying information across between instances.\nThe ConvertToStorage() method is responsible for copying all of the properties from the API type onto the storage type. The ConvertFromStorage() method is its mirror, responsible for populating all of the properties on the API type from the storage type.\nEach property defined in the API type is considered in turn, and will require different handling based on its type and whether a suitable match is found on the storage type:\nFor properties with a primitive type a matching property must have the same name and the identical type. If found, a simple assignment will copy the value over. If not found, the value will be stashed-in/recalled-from the property bag present on the storage type.\n Primitive types are string, int, float64, and bool Name comparisons are case-insensitive  For properties with an enumeration type a matching property must have the same property name and a type matching the underlying type of the enumeration. If found, a simple assignment will copy the value over with a suitable type cast. If not found, the value will be stashed-in/recalled-from the property bag present on the storage type using the underlying type of the enumeration.\n Name comparisons are case-insensitive for both property names and enumeration type names Enumeration types are generated independently for each version, so they will never be identical types  For properties with a custom type a matching property must have the same name and a custom type with same type name. If found, a new instance will be created and the appropriate ConvertToStorage() or ConvertFromStorage() method for the custom type will be used. If not found, JSON serialization will be used with the property bag for storage.\n Name comparisons are case-insensitive for both property names and custom type names Custom types are generated independently for each version, so they will never be identical types   TODO: Show an example that includes all the cases\n External Metadata for common changes #  We\u0026rsquo;ll capture common changes between versions in metadata (likely a YAML file) that we bundle with the code generator, allowing it to handle a wider range of scenarios.\nIf a property is renamed in a particular API version, conversion from the prior API version to that point of change will instead match based on the new name of the property on the storage type.\nThere are more than 40 cases of properties being renamed across versions of the ARM API.\n TODO: Show an example\n If a type has been renamed in a particular API version, conversion from the API version prior to that point of change will instead match based on the new type of the property on the storage type.\nThere are 160 cases of properties changing type across versions of the ARM API. Many of these can be handled automatically by capturing type renames in metadata.\n TODO: Show an example\n  Outstanding Issue: Are there other kinds of common change we want to support?\nAre there other cases of changes between versions that we may be able to handle automatically. Can we find examples? Do we want to support these cases?\n Standard extension points #  Code generation will include interfaces to allow easy injection of manual conversion steps.\nFor each storage type, two interfaces will be generated, one for each direction of conversion.\nAn AssignableTo* interface for conversion to the storage type will be available for conversions that write to an instance of the storage type.\ntype AssignableToPerson interface { AssignToPerson(person Person) error } Similarly, an AssignableFrom* interface for conversion from the storage type will be available for conversions that read from an instance of the storage type:\ntype AssignableFromPerson interface { AssignFromPerson(person Person) error } If a type (whether API or storage) implements one (or both) of these interfaces, they will be automatically invoked after the standard conversion code has completed, creating an opportunity to augment the standard conversion process.\nTesting #  It\u0026rsquo;s vital that we are able to correctly convert between versions. We will therefore generate a set of unit tests to help ensure that the conversions work correctly. Coverage won\u0026rsquo;t be perfect (as there are conversion steps we can\u0026rsquo;t automatically verify) but these tests will help ensure correctness.\nRound Trip Testing #  We will generate a unit test to ensure that every spoke version can round trip to the hub version and back again to the same version with no loss of information. This will help to ensure a base level of compliance, that information is not lost through serialization.\nThis test targets the following failure modes:\n Edge cases not correctly handled by the generated conversion code. Manually implemented conversions that don\u0026rsquo;t handle some cases correctly.  Each test will work as follows:\n Create an instance of the required type and API version  This will likely be done by using one of the available fuzzing libraries for Go testing   Convert this to the current storage version Convert back from the storage version to a new instance of the original type and API version Verify that all properties are equal  string, int, bool much match exactly Float64 match within tolerance Complex types are recursively matched using the same rules    Relibility Testing #  We will generate unit tests to ensure that every spoke version can be converted to every other spoke version via the hub version without crashing. We lack the semantic context to verify that the conversion is correct, but we can at least verify that it doesn\u0026rsquo;t crash.\nThis test targets the following failure modes:\n Conversions that fail when information is missing (as may happen when converting from earlier versions)  Golden Tests #  For API (spoke) types where the optional interfaces AssignableTo...() and AssignableFrom...() have been implemented, we\u0026rsquo;ll generate golden tests to verify that they are generating the expected results.\nThis test targets the following failure modes:\n Manually implemented conversions that don\u0026rsquo;t handle all the expected edge cases. Manually implemented conversions that fail when given newer (or older) starting versions than expected.  These tests will be particularly useful when a new version of the ARM API is released for a given service as they will help to catch any new changes that now require support.\nWe\u0026rsquo;ll generate two golden tests for each type in each API type, one to test verify conversion to the latest version, and one to test conversion from the latest version.\nTesting conversion to the latest version will check that an instance of a older version of the API can be up-converted to the latest version:\nThe test will involve these steps:\n Create an exemplar instance of the older API type Convert it to the storage type using ConvertToStorage() Convert it to the latest API type using ConvertFromStorage() Check that it matches the golden file from a previous run  Testing will only occur if one (or both) types implements one of the optional interfaces. That is, one or both of the following must be true:\n The older API type implements AssignableTo...() The latest API type implements AssignableFrom...()  If neither rule is satisfied, the test will silently null out.\nTesting conversion from the latest version will check that an instance of the latest version of the API can be down-converted to an older version.\n Create an exemplar instance of the latest API type Convert it to the storage type using ConvertToStorage() Convert it to the older API type using ConvertFromStorage() Check that it matches the golden file from a previous run  Testing will only occur if one (or both) types implements one of the optional interfaces. That is, one or both of the following must be true:\n The older API type implements AssignableFrom...() The latest API type implements AssignableTo...()  If neither rule is satisfied, the test will silently null out.\nConversion Flow #  To illustrate the operation of conversions, consider the following graph of related versions of Person:\nAPI versions are shown across the top, with the associated storage versions directly below. The arrows show the direction of references between the packages, with a package at the start of the arrow importing the package at the end. For example, package v3 imports v3storage and can access the types within.\nThe highlighted storage version v4storage is the currently nominated hub version - all conversions are to or from this type.\nDirect conversion to storage type #  The simplest case is a conversion directly between v4 and v4storage, which simply involves copying properties across:\nTwo step conversion to storage type #  There\u0026rsquo;s no direct conversion between a v3.Person and a v4storage.Person, so an intermediate step is required: we convert first to a v3storage.Person, and then to the final type:\nMultiple step conversion to storage type #  The approach generalizes - at each stage, an intermediate instance is created, one step closer to the current hub type, and the properties are copied across:\nTwo step conversion from storage type #  When converting in the other direction, the process is similar - we show here just the two step case to illustrate.\nAlternative Solutions #  AKA the road not travelled\nAlternative: Fixed storage version #  The \u0026ldquo;v1\u0026rdquo; storage version of each supported resource type will be created by merging all of the fields of all the distinct versions of the resource type, creating a superset type that includes every property declared across every version of the API.\nTo maintain backward compatibility as Azure APIs evolve over time, we will include properties across all versions of the API, even for versions we are not currently generating as output. This ensures that properties in use by older APIs are still present and available for forward conversion to newer APIs, even as those older APIs age out of use.\nThis approach has a number of issues that are called out in detail in the fixed storage version case study.\nProperty Bloat: As our API evolves over time, our storage version is accumulating all the properties that have ever existed, bloating the storage version with obsolete properties that are seldom (if ever) used. Even properties that only ever existed on a single preview release of an ARM API need to be correctly managed for the lifetime of the service operator.\nProperty Amnesia: Our code generator only knows about properties defined in current versions of the API. Once an API version has been excluded (or if the JSON schema definition is no longer available), the generator completely forgets about older properties. This would cause compatibility issues for established users who would find upgrading the service operator breaks their cluster.\nType Collision: Identically named properties with different types can\u0026rsquo;t be stored in the same property; mitigation is possible for a limited time, though eventually property amnesia will cause a breaking change.\nAlternative: Use the latest API version #  The supported storage version of each resource type will simply be the latest version of the API supported by ARM. Any additional information not supported on that version will be persisted via annotations on the resource.\nThis is a known antipattern that we should avoid.\nAnnotations are publicly visible on the cluster and can easily modified. This makes it spectacularly easy for a user to make an innocent change that would break the functionality of the operator.\nMetadata Design #  To support property and type renaming, we will include metadata describing the known changes. This will be added to the existing configuration file we already have that includes filtering information on the types we exclude/include in the output.\nAs identification of renames requires manual inspection, any case of a property not appearing in a later version of the API needs to be checked. We\u0026rsquo;ll therefore also capture metadata for property removal so that we know which properties have been assessed and which have not.\nOutstanding Issues #  Service Operator Upgrades #  There are a number of issues outstanding around upgrades of the service operator.\nTiming - when are upgrades triggered? Does this happen immediately after installation of a new version of the service operator, or does it happen at a later point? If so, what\u0026rsquo;s the trigger?\nAtomicity - are all the CRDs upgraded in one atomic operation that either succeeds or fails, or are they upgraded one at a time? Are CRDs upgraded serially, or in parallel?\nPerformance - for users who have a large number of CRDs (hundreds to thousands), what sort of upgrade performance will they see?\nRecovery - if an upgrade aborts part way through, or if a new version of the service operator proves to be unreliable, is it possible for users to roll back to a previous version, or must they roll forward to a fixed version?\nSee Also #    Hubs, spokes, and other wheel metaphors\n  Falsehoods programmers believe about addresses\n  https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/\n"}]